{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 18:40:41.713516 4726353344 file_utils.py:41] PyTorch version 1.5.0 available.\n",
      "I0806 18:40:45.314117 4726353344 file_utils.py:57] TensorFlow version 2.2.0-rc3 available.\n",
      "I0806 18:40:45.841892 4726353344 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "I0806 18:40:46.633439 4726353344 textcleaner.py:37] 'pattern' package not found; tag filters are not available for English\n",
      "W0806 18:40:46.723387 4726353344 deprecation.py:323] From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from dotenv import find_dotenv, load_dotenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '../src'))\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.getcwd(), '../src'))\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcontrib.optim import SWA\n",
    "from torch.optim import Adam, SGD \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CyclicLR, \\\n",
    "                                     CosineAnnealingWarmRestarts\n",
    "\n",
    "from consNLP.data import load_data, data_utils, fetch_dataset\n",
    "from consNLP.models import transformer_models, activations, layers, losses, scorers\n",
    "from consNLP.visualization import visualize\n",
    "from consNLP.trainer.trainer import BasicTrainer, PLTrainer, test_pl_trainer, QATrainer, PLTrainerQA\n",
    "from consNLP.trainer.trainer_utils import set_seed, _has_apex, _torch_lightning_available, _has_wandb, _torch_gpu_available, _num_gpus, _torch_tpu_available\n",
    "from consNLP.preprocessing.custom_tokenizer import BERTweetTokenizer\n",
    "\n",
    "if _has_apex:\n",
    "    #from torch.cuda import amp\n",
    "    from apex import amp\n",
    "\n",
    "if _torch_tpu_available:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "if _has_wandb:\n",
    "    import wandb\n",
    "    try:\n",
    "        load_dotenv(find_dotenv())\n",
    "        wandb.login(key=os.environ['WANDB_API_KEY'])\n",
    "    except:\n",
    "        _has_wandb = False\n",
    "\n",
    "if _torch_lightning_available:\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    from pytorch_lightning.loggers import WandbLogger\n",
    "    from pytorch_lightning.metrics.metric import NumpyMetric\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "\n",
    "import tokenizers\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 14:06:39.849577 4482461120 fetch_dataset.py:16] making final data set from raw data\n",
      "I0806 14:06:39.850909 4482461120 fetch_dataset.py:21] project directory ../\n",
      "I0806 14:06:39.851778 4482461120 fetch_dataset.py:30] output path ../data/raw\n",
      "I0806 14:06:49.323308 4482461120 fetch_dataset.py:95] download complete\n"
     ]
    }
   ],
   "source": [
    "fetch_dataset(project_dir='../',download_from_kaggle=True,\\\n",
    "              kaggle_competition='tweet-sentiment-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../data/raw/tweet-sentiment-extraction.zip\r\n",
      "  inflating: ../data/raw/sample_submission.csv  \r\n",
      "  inflating: ../data/raw/test.csv    \r\n",
      "  inflating: ../data/raw/train.csv   \r\n"
     ]
    }
   ],
   "source": [
    "!unzip ../data/raw/tweet-sentiment-extraction.zip -d ../data/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Logging: False, GPU: False, Pytorch Lightning: False, TPU: False, Apex: False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog='Torch trainer function',conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/train.csv', required=False,\n",
    "                    help='train data')\n",
    "parser.add_argument('--val_data', type=str, default='', required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--task_type', type=str, default='multiclass_sequence_classification', required=False,\n",
    "                    help='type of task')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "parser.add_argument('--bpe_vocab_path', type=str, default='../models/roberta-base/vocab.json', required=False,\n",
    "                    help='bytepairencoding vocab file path')\n",
    "parser.add_argument('--bpe_merges_path', type=str, default='../models/roberta-base/merges.txt', required=False,\n",
    "                    help='bytepairencoding merges file path')\n",
    "parser.add_argument('--berttweettokenizer_path', type=str, default='', required=False,\n",
    "                    help='BERTweet tokenizer path')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=128, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--epochs', type=int, default=2, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.00003, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--loss_function', type=str, default='qa_ce', required=False,\n",
    "                    help='loss function')\n",
    "parser.add_argument('--metric', type=str, default='qa_jaccard', required=False,\n",
    "                    help='scorer metric')\n",
    "\n",
    "parser.add_argument('--use_lightning_trainer', type=bool, default=False, required=False,\n",
    "                    help='if lightning trainer needs to be used')\n",
    "parser.add_argument('--use_torch_trainer', type=bool, default=True, required=False,\n",
    "                    help='if custom torch trainer needs to be used')\n",
    "parser.add_argument('--use_apex', type=bool, default=False, required=False,\n",
    "                    help='if apex needs to be used')\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, required=False,\n",
    "                    help='GPU mode')\n",
    "parser.add_argument('--use_TPU', type=bool, default=False, required=False,\n",
    "                    help='TPU mode')\n",
    "parser.add_argument('--num_gpus', type=int, default=0, required=False,\n",
    "                    help='Number of GPUs')\n",
    "parser.add_argument('--num_tpus', type=int, default=0, required=False,\n",
    "                    help='Number of TPUs')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=16, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/span_prediction/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=False, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print (\"Wandb Logging: {}, GPU: {}, Pytorch Lightning: {}, TPU: {}, Apex: {}\".format(\\\n",
    "            _has_wandb and args.wandb_logging, _torch_gpu_available,\\\n",
    "            _torch_lightning_available and args.use_lightning_trainer, _torch_tpu_available, _has_apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = False\n",
    "final_activation = None\n",
    "convert_output = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data.load_pandas_df(args.train_data,sep=',')\n",
    "df = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('')\n",
    "df['text'] = df.text.apply(lambda x: x.strip())\n",
    "df['selected_text'] = df.selected_text.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = args.model_save_path\n",
    "try:\n",
    "    os.makedirs(model_save_dir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(5)\n",
    "\n",
    "for train_index, val_index in kf.split(df.text, df.selected_text):\n",
    "    break\n",
    "    \n",
    "train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "val_df = df.iloc[val_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 4), (200, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 18:40:48.130651 4726353344 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0806 18:40:48.131536 4726353344 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0806 18:40:50.283564 4726353344 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /Users/victor/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0806 18:40:50.284409 4726353344 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /Users/victor/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "if args.berttweettokenizer_path:\n",
    "    tokenizer = BERTweetTokenizer(args.berttweettokenizer_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.transformer_model_pretrained_path)\n",
    "\n",
    "if not args.berttweettokenizer_path:\n",
    "    try:\n",
    "        bpetokenizer = tokenizers.ByteLevelBPETokenizer(args.bpe_vocab_path, \\\n",
    "                                        args.bpe_merges_path)\n",
    "    except:\n",
    "        bpetokenizer = None \n",
    "else:\n",
    "    bpetokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_utils.TransformerDataset(train_df.text, bpetokenizer=bpetokenizer, tokenizer=tokenizer, MAX_LEN=args.max_text_len, \\\n",
    "              target_text=train_df.selected_text, conditional_label=train_df.sentiment, conditional_all_labels=train_df.sentiment.unique())\n",
    "val_dataset = data_utils.TransformerDataset(val_df.text, bpetokenizer=bpetokenizer, tokenizer=tokenizer, MAX_LEN=args.max_text_len, \\\n",
    "              target_text=val_df.selected_text, conditional_label=val_df.sentiment, conditional_all_labels=train_df.sentiment.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, base_model, dropout=.3):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.base_model = base_model\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(base_model.config.hidden_size, 2)\n",
    "        torch.nn.init.normal_(self.out.weight, std=0.02)\n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        o2 = self.base_model(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        o2 = o2[0]\n",
    "        bo = self.drop(o2)\n",
    "        logits = self.out(bo)\n",
    "        \n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 18:40:51.511431 4726353344 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0806 18:40:51.512250 4726353344 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0806 18:40:52.793122 4726353344 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /Users/victor/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(args.transformer_config_path, output_hidden_states=True, output_attentions=True)\n",
    "basemodel = AutoModel.from_pretrained(args.transformer_model_pretrained_path,config=config)\n",
    "model = TransformerModel(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      train_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True\n",
    "    )\n",
    "\n",
    "    val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      val_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, sampler=train_sampler,\n",
    "        drop_last=True,num_workers=2)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size, sampler=val_sampler,\n",
    "        drop_last=False,num_workers=1)\n",
    "else:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128]) torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "for d in train_data_loader:\n",
    "    break\n",
    "\n",
    "start, end = model(d['ids'],d['mask'],d['token_type_ids'])\n",
    "print (start.shape, end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.6959, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = losses.get_loss(args.loss_function)\n",
    "print (loss(start, end, d['targets_start'], d['targets_end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 10,  63,  82,  82,  23,   7,  36,  30,  15, 120,  46,  10,  31,\n",
       "         69,  18,  17]),\n",
       " array([ 9,  4, 26, 11, 14,  4,  4,  7,  9,  4,  5,  9, 21, 10,  3,  9]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start.detach().cpu().numpy().argmax(-1), end.detach().cpu().numpy().argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8, 17,  3,  3,  3, 11,  3,  3,  7,  3, 15,  3,  6, 15,  3,  3]),\n",
       " tensor([10, 17, 15, 11, 32, 11,  3, 11, 10,  9, 18, 10,  7, 15, 18, 19]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['targets_start'], d['targets_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "metric = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "print (metric(d['targets_start'].detach().cpu().numpy(),d['targets_end'].detach().cpu().numpy(), \\\n",
    "              start.detach().cpu().numpy(), end.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pytorch Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "[LOG] Total number of parameters to learn 124647170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from previous checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current training Loss 2.579: 100%|██████████| 50/50 [04:46<00:00,  5.73s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 3.113: 100%|██████████| 50/50 [01:25<00:00,  1.71s/it]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 3.076: 100%|██████████| 13/13 [00:20<00:00,  1.58s/it]\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 2.515 Train metric = 0.593 Val loss = 2.842 Val metric = 0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current training Loss 3.539: 100%|██████████| 50/50 [04:51<00:00,  5.82s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 3.113: 100%|██████████| 50/50 [01:24<00:00,  1.69s/it]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 3.076: 100%|██████████| 13/13 [00:20<00:00,  1.60s/it]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 2.515 Train metric = 0.593 Val loss = 2.842 Val metric = 0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:20<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "if args.use_torch_trainer:\n",
    "    device = torch.device(\"cuda\" if _torch_gpu_available and args.use_gpu else \"cpu\")\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        device=xm.xla_device()\n",
    "\n",
    "    print (\"Device: {}\".format(device))\n",
    "    \n",
    "    if args.use_TPU and _torch_tpu_available and args.num_tpus > 1:\n",
    "        train_data_loader = torch_xla.distributed.parallel_loader.ParallelLoader(train_data_loader, [device])\n",
    "        train_data_loader = train_data_loader.per_device_loader(device)\n",
    "\n",
    "\n",
    "    trainer = QATrainer(model, train_data_loader, val_data_loader, device, args.transformer_model_pretrained_path, \\\n",
    "                               final_activation=final_activation, \\\n",
    "                               test_data_loader=val_data_loader)\n",
    "\n",
    "    param_optimizer = list(trainer.model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr*xm.xrt_world_size())\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr)\n",
    "\n",
    "    if args.use_apex and _has_apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape) \n",
    "    \n",
    "    def _mp_fn(rank, flags, trainer, epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        a = trainer.train(epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed)\n",
    "\n",
    "    FLAGS = {}\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS, trainer, args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus, \\\n",
    "                 1, 3, False, args.use_apex, False, args.seed), nprocs=8, start_method='fork')\n",
    "    else:\n",
    "        use_wandb = _has_wandb and args.wandb_logging\n",
    "        trainer.train(args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus,  \\\n",
    "                max_grad_norm=1, early_stopping_rounds=3, snapshot_ensemble=False, is_amp=args.use_apex, use_wandb=use_wandb, seed=args.seed)\n",
    "\n",
    "elif args.use_lightning_trainer and _torch_lightning_available:\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.PLMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "    \n",
    "    log_args = {'description': args.transformer_model_pretrained_path, 'loss': loss.__class__.__name__, 'epochs': args.epochs, 'learning_rate': args.lr}\n",
    "\n",
    "    if _has_wandb and not _torch_tpu_available and args.wandb_logging:\n",
    "        wandb.init(project=\"Project\",config=log_args)\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filepath=args.model_save_path,\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "               verbose=False,\n",
    "               mode='min'\n",
    "               )\n",
    "\n",
    "    if args.use_gpu and _torch_gpu_available:\n",
    "        print (\"using GPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    elif args.use_TPU and _torch_tpu_available:\n",
    "        print (\"using TPU\")\n",
    "        if _has_apex:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    else:\n",
    "        print (\"using CPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    pltrainer = PLTrainerQA(num_train_steps, model, scorer, loss, args.lr, \\\n",
    "                          final_activation=final_activation, seed=42)\n",
    "\n",
    "    #try:\n",
    "    #    print (\"Loaded model from previous checkpoint\")\n",
    "    #    pltrainer = PLTrainer.load_from_checkpoint(args.model_save_path)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    trainer.fit(pltrainer, train_data_loader, val_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_start1, test_output_start1 = trainer.test_output_start, trainer.test_output_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pytorch Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Logging: False, GPU: False, Pytorch Lightning: True, TPU: False, Apex: False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog='Torch trainer function',conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/train.csv', required=False,\n",
    "                    help='train data')\n",
    "parser.add_argument('--val_data', type=str, default='', required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--task_type', type=str, default='multiclass_sequence_classification', required=False,\n",
    "                    help='type of task')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "parser.add_argument('--bpe_vocab_path', type=str, default='../models/roberta-base/vocab.json', required=False,\n",
    "                    help='bytepairencoding vocab file path')\n",
    "parser.add_argument('--bpe_merges_path', type=str, default='../models/roberta-base/merges.txt', required=False,\n",
    "                    help='bytepairencoding merges file path')\n",
    "parser.add_argument('--berttweettokenizer_path', type=str, default='', required=False,\n",
    "                    help='BERTweet tokenizer path')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=128, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--epochs', type=int, default=2, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.00003, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--loss_function', type=str, default='qa_ce', required=False,\n",
    "                    help='loss function')\n",
    "parser.add_argument('--metric', type=str, default='qa_jaccard', required=False,\n",
    "                    help='scorer metric')\n",
    "\n",
    "parser.add_argument('--use_lightning_trainer', type=bool, default=True, required=False,\n",
    "                    help='if lightning trainer needs to be used')\n",
    "parser.add_argument('--use_torch_trainer', type=bool, default=False, required=False,\n",
    "                    help='if custom torch trainer needs to be used')\n",
    "parser.add_argument('--use_apex', type=bool, default=False, required=False,\n",
    "                    help='if apex needs to be used')\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, required=False,\n",
    "                    help='GPU mode')\n",
    "parser.add_argument('--use_TPU', type=bool, default=False, required=False,\n",
    "                    help='TPU mode')\n",
    "parser.add_argument('--num_gpus', type=int, default=0, required=False,\n",
    "                    help='Number of GPUs')\n",
    "parser.add_argument('--num_tpus', type=int, default=0, required=False,\n",
    "                    help='Number of TPUs')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=16, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/span_prediction/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=False, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print (\"Wandb Logging: {}, GPU: {}, Pytorch Lightning: {}, TPU: {}, Apex: {}\".format(\\\n",
    "            _has_wandb and args.wandb_logging, _torch_gpu_available,\\\n",
    "            _torch_lightning_available and args.use_lightning_trainer, _torch_tpu_available, _has_apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Checkpoint directory ../models/span_prediction/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "I0806 18:41:05.368399 4726353344 distributed.py:29] GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0806 18:41:05.370901 4726353344 distributed.py:29] TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | model  | TransformerModel | 124 M \n",
      "1 | metric | PLMetric         | 0     \n",
      "I0806 18:41:05.416452 4726353344 lightning.py:1495] \n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | model  | TransformerModel | 124 M \n",
      "1 | metric | PLMetric         | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU\n",
      "[LOG] Total number of parameters to learn 124647170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 9.738 val metric = 0.134 \n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c1171b296543d0a17b5f1bc9b75b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: val_loss reached 2.88305 (best 2.88305), saving model to ../models/span_prediction/epoch=0.ckpt as top 1\n",
      "I0806 18:46:47.367697 4726353344 model_checkpoint.py:346] \n",
      "Epoch 00000: val_loss reached 2.88305 (best 2.88305), saving model to ../models/span_prediction/epoch=0.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 2.883 val metric = 0.58 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 5.022 Train metric = 0.404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss reached 2.49742 (best 2.49742), saving model to ../models/span_prediction/epoch=1.ckpt as top 1\n",
      "I0806 18:52:17.967818 4726353344 model_checkpoint.py:346] \n",
      "Epoch 00001: val_loss reached 2.49742 (best 2.49742), saving model to ../models/span_prediction/epoch=1.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 2.497 val metric = 0.61 \n",
      "Train loss = 2.758 Train metric = 0.566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.use_torch_trainer:\n",
    "    device = torch.device(\"cuda\" if _torch_gpu_available and args.use_gpu else \"cpu\")\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        device=xm.xla_device()\n",
    "\n",
    "    print (\"Device: {}\".format(device))\n",
    "    \n",
    "    if args.use_TPU and _torch_tpu_available and args.num_tpus > 1:\n",
    "        train_data_loader = torch_xla.distributed.parallel_loader.ParallelLoader(train_data_loader, [device])\n",
    "        train_data_loader = train_data_loader.per_device_loader(device)\n",
    "\n",
    "\n",
    "    trainer = QATrainer(model, train_data_loader, val_data_loader, device, args.transformer_model_pretrained_path, \\\n",
    "                               final_activation=final_activation, \\\n",
    "                               test_data_loader=val_data_loader)\n",
    "\n",
    "    param_optimizer = list(trainer.model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr*xm.xrt_world_size())\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr)\n",
    "\n",
    "    if args.use_apex and _has_apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape) \n",
    "    \n",
    "    def _mp_fn(rank, flags, trainer, epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        a = trainer.train(epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed)\n",
    "\n",
    "    FLAGS = {}\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS, trainer, args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus, \\\n",
    "                 1, 3, False, args.use_apex, False, args.seed), nprocs=8, start_method='fork')\n",
    "    else:\n",
    "        use_wandb = _has_wandb and args.wandb_logging\n",
    "        trainer.train(args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus,  \\\n",
    "                max_grad_norm=1, early_stopping_rounds=3, snapshot_ensemble=False, is_amp=args.use_apex, use_wandb=use_wandb, seed=args.seed)\n",
    "\n",
    "elif args.use_lightning_trainer and _torch_lightning_available:\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.PLMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "    \n",
    "    log_args = {'description': args.transformer_model_pretrained_path, 'loss': loss.__class__.__name__, 'epochs': args.epochs, 'learning_rate': args.lr}\n",
    "\n",
    "    if _has_wandb and not _torch_tpu_available and args.wandb_logging:\n",
    "        wandb.init(project=\"Project\",config=log_args)\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filepath=args.model_save_path,\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "               verbose=False,\n",
    "               mode='min'\n",
    "               )\n",
    "\n",
    "    if args.use_gpu and _torch_gpu_available:\n",
    "        print (\"using GPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    elif args.use_TPU and _torch_tpu_available:\n",
    "        print (\"using TPU\")\n",
    "        if _has_apex:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    else:\n",
    "        print (\"using CPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    pltrainer = PLTrainerQA(num_train_steps, model, scorer, loss, args.lr, \\\n",
    "                          final_activation=final_activation, seed=42)\n",
    "\n",
    "    #try:\n",
    "    #    print (\"Loaded model from previous checkpoint\")\n",
    "    #    pltrainer = PLTrainer.load_from_checkpoint(args.model_save_path)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    trainer.fit(pltrainer, train_data_loader, val_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:32<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_output_start2 = []\n",
    "test_output_end2 = []\n",
    "\n",
    "for val_batch in tqdm(val_data_loader):\n",
    "    out1, out2 = pltrainer(val_batch)\n",
    "    out1 = out1.detach().cpu().numpy()\n",
    "    out2 = out2.detach().cpu().numpy()\n",
    "    test_output_start2.extend(out1.tolist())\n",
    "    test_output_end2.extend(out2.tolist())\n",
    "    \n",
    "#test_output2 = np.concatenate(test_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_start2 = np.array(test_output_start2).argmax(-1)\n",
    "test_output_end2 = np.array(test_output_end2).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2252.51it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_texts = []\n",
    "\n",
    "for i in tqdm(range(val_df.shape[0])):\n",
    "    d = data_utils.process_data_for_transformers_conditional(val_df.text.iloc[i],val_df.sentiment.iloc[i], all_labels=train_df.sentiment.unique(), \\\n",
    "                                                        bpetokenizer=bpetokenizer, tokenizer=tokenizer, max_len=args.max_text_len, \\\n",
    "                                                        concat_type='pre')\n",
    "    start = test_output_start2[i]\n",
    "    end = test_output_end2[i]\n",
    "    \n",
    "    t = d['orig_text'][d['offsets'][start][0]:d['offsets'][end][1]]\n",
    "    \n",
    "    predicted_texts.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['predicted_text'] = predicted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>predicted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on th...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sons of ****</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Soooo high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Both of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe.....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe.....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861  Sons of ****, why couldn`t they put them on th...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d  Journey!? Wow... u just became cooler.  hehe.....   \n",
       "\n",
       "                                       selected_text sentiment  \\\n",
       "0                I`d have responded, if I were going   neutral   \n",
       "1                                           Sooo SAD  negative   \n",
       "2                                        bullying me  negative   \n",
       "3                                     leave me alone  negative   \n",
       "4                                      Sons of ****,  negative   \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral   \n",
       "6                                                fun  positive   \n",
       "7                                         Soooo high   neutral   \n",
       "8                                        Both of you   neutral   \n",
       "9                       Wow... u just became cooler.  positive   \n",
       "\n",
       "                                      predicted_text  \n",
       "0                I`d have responded, if I were going  \n",
       "1      Sooo SAD I will miss you here in San Diego!!!  \n",
       "2                          my boss is bullying me...  \n",
       "3                     what interview! leave me alone  \n",
       "4                                       Sons of ****  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...  \n",
       "6                                                fun  \n",
       "7                                         Soooo high  \n",
       "8                                        Both of you  \n",
       "9  Journey!? Wow... u just became cooler.  hehe.....  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
