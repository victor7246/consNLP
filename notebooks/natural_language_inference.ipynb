{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 20:33:07.706027 4680543680 file_utils.py:41] PyTorch version 1.5.0 available.\n",
      "I0806 20:33:15.409231 4680543680 file_utils.py:57] TensorFlow version 2.2.0-rc3 available.\n",
      "I0806 20:33:17.444828 4680543680 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "I0806 20:33:19.592143 4680543680 textcleaner.py:37] 'pattern' package not found; tag filters are not available for English\n",
      "W0806 20:33:19.841995 4680543680 deprecation.py:323] From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from dotenv import find_dotenv, load_dotenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '../src'))\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.getcwd(), '../src'))\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcontrib.optim import SWA\n",
    "from torch.optim import Adam, SGD \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CyclicLR, \\\n",
    "                                     CosineAnnealingWarmRestarts\n",
    "\n",
    "from consNLP.data import load_data, data_utils, fetch_dataset\n",
    "from consNLP.models import transformer_models, activations, layers, losses, scorers\n",
    "from consNLP.visualization import visualize\n",
    "from consNLP.trainer.trainer import BasicTrainer, PLTrainer, test_pl_trainer, QATrainer, PLTrainerQA\n",
    "from consNLP.trainer.trainer_utils import set_seed, _has_apex, _torch_lightning_available, _has_wandb, _torch_gpu_available, _num_gpus, _torch_tpu_available\n",
    "from consNLP.preprocessing.custom_tokenizer import BERTweetTokenizer\n",
    "\n",
    "if _has_apex:\n",
    "    #from torch.cuda import amp\n",
    "    from apex import amp\n",
    "\n",
    "if _torch_tpu_available:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "if _has_wandb:\n",
    "    import wandb\n",
    "    try:\n",
    "        load_dotenv(find_dotenv())\n",
    "        wandb.login(key=os.environ['WANDB_API_KEY'])\n",
    "    except:\n",
    "        _has_wandb = False\n",
    "\n",
    "if _torch_lightning_available:\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    from pytorch_lightning.loggers import WandbLogger\n",
    "    from pytorch_lightning.metrics.metric import NumpyMetric\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "\n",
    "import tokenizers\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 20:02:45.316612 4476943808 fetch_dataset.py:16] making final data set from raw data\n",
      "I0806 20:02:45.317600 4476943808 fetch_dataset.py:21] project directory ../\n",
      "I0806 20:02:45.318350 4476943808 fetch_dataset.py:30] output path ../data/raw\n",
      "I0806 20:02:45.319162 4476943808 fetch_dataset.py:77] downloading snli_1.0.zip\n",
      "/Users/victor/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "I0806 20:03:40.604937 4476943808 fetch_dataset.py:95] download complete\n"
     ]
    }
   ],
   "source": [
    "fetch_dataset(project_dir='../',download_from_url=True, \\\n",
    "              data_url='https://nlp.stanford.edu/projects/snli/snli_1.0.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Logging: False, GPU: False, Pytorch Lightning: False, TPU: False, Apex: False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog='Torch trainer function',conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/snli_1.0/snli_1.0_train.jsonl', required=False,\n",
    "                    help='train data')\n",
    "parser.add_argument('--val_data', type=str, default='', required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--task_type', type=str, default='multiclass_sequence_classification', required=False,\n",
    "                    help='type of task')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='textattack/roberta-base-MNLI', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='textattack/roberta-base-MNLI', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='textattack/roberta-base-MNLI', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "parser.add_argument('--bpe_vocab_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding vocab file path')\n",
    "parser.add_argument('--bpe_merges_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding merges file path')\n",
    "parser.add_argument('--berttweettokenizer_path', type=str, default='', required=False,\n",
    "                    help='BERTweet tokenizer path')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=128, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--epochs', type=int, default=2, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.00003, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--loss_function', type=str, default='ce', required=False,\n",
    "                    help='loss function')\n",
    "parser.add_argument('--metric', type=str, default='f1_macro', required=False,\n",
    "                    help='scorer metric')\n",
    "\n",
    "parser.add_argument('--use_lightning_trainer', type=bool, default=False, required=False,\n",
    "                    help='if lightning trainer needs to be used')\n",
    "parser.add_argument('--use_torch_trainer', type=bool, default=True, required=False,\n",
    "                    help='if custom torch trainer needs to be used')\n",
    "parser.add_argument('--use_apex', type=bool, default=False, required=False,\n",
    "                    help='if apex needs to be used')\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, required=False,\n",
    "                    help='GPU mode')\n",
    "parser.add_argument('--use_TPU', type=bool, default=False, required=False,\n",
    "                    help='TPU mode')\n",
    "parser.add_argument('--num_gpus', type=int, default=0, required=False,\n",
    "                    help='Number of GPUs')\n",
    "parser.add_argument('--num_tpus', type=int, default=0, required=False,\n",
    "                    help='Number of TPUs')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=16, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/nli/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=False, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print (\"Wandb Logging: {}, GPU: {}, Pytorch Lightning: {}, TPU: {}, Apex: {}\".format(\\\n",
    "            _has_wandb and args.wandb_logging, _torch_gpu_available,\\\n",
    "            _torch_lightning_available and args.use_lightning_trainer, _torch_tpu_available, _has_apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = False\n",
    "final_activation = None\n",
    "convert_output = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "get_transitions = lambda parse: ['reduce' if t == ')' else 'shift' for t in parse if t != '(']\n",
    "examples = []\n",
    "with io.open(args.train_data, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = json.loads(line)\n",
    "        examples.append({\n",
    "            'premise': line['sentence1'],\n",
    "            'hypothesis': line['sentence2'],\n",
    "            'label': line['gold_label'],\n",
    "            'premise_transitions': get_transitions(line['sentence1_binary_parse']),\n",
    "            'hypothesis_transitions': get_transitions(line['sentence2_binary_parse'])\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(550152, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(examples)\n",
    "print (df.shape)\n",
    "df = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>premise_transitions</th>\n",
       "      <th>hypothesis_transitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>entailment</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                                          hypothesis          label  \\\n",
       "0  A person is training his horse for a competition.        neutral   \n",
       "1      A person is at a diner, ordering an omelette.  contradiction   \n",
       "2                  A person is outdoors, on a horse.     entailment   \n",
       "3                  They are smiling at their parents        neutral   \n",
       "4                         There are children present     entailment   \n",
       "\n",
       "                                 premise_transitions  \\\n",
       "0  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "1  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "2  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "3  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "4  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "\n",
       "                              hypothesis_transitions  \n",
       "0  [shift, shift, shift, shift, shift, shift, shi...  \n",
       "1  [shift, shift, shift, shift, shift, shift, shi...  \n",
       "2  [shift, shift, shift, shift, shift, shift, shi...  \n",
       "3  [shift, shift, shift, shift, shift, shift, shi...  \n",
       "4  [shift, shift, shift, shift, shift, shift, shi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = args.model_save_path\n",
    "try:\n",
    "    os.makedirs(model_save_dir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label, label2idx = data_utils.convert_categorical_label_to_int(df.label, \\\n",
    "                                                             save_path=os.path.join(model_save_dir,'label2idx.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0, 'neutral': 1, 'contradiction': 2, '-': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(5)\n",
    "\n",
    "for train_index, val_index in kf.split(df.premise, df.label):\n",
    "    break\n",
    "    \n",
    "train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "val_df = df.iloc[val_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 5), (200, 5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 20:39:54.232226 4680543680 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/textattack/roberta-base-MNLI/config.json from cache at /Users/victor/.cache/torch/transformers/a0e9a64482bfa531a2b455bb5c56da303d4bc5dfcb9d0204a326d1dac03ee18e.5ab50e081c86cc5d316d4bd6224f34c782f19b06be3485cd6e269b5a46d0554d\n",
      "I0806 20:39:54.233466 4680543680 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0806 20:39:54.234462 4680543680 tokenization_utils.py:420] Model name 'textattack/roberta-base-MNLI' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'textattack/roberta-base-MNLI' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0806 20:39:59.603366 4680543680 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/textattack/roberta-base-MNLI/vocab.json from cache at /Users/victor/.cache/torch/transformers/46d0f829e54c823f8120e859ebb7daeada2f4a8b5fd29d42090bab6a3c538b5c.6a4061e8fc00057d21d80413635a86fdcf55b6e7594ad9e25257d2f99a02f4be\n",
      "I0806 20:39:59.604078 4680543680 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/textattack/roberta-base-MNLI/merges.txt from cache at /Users/victor/.cache/torch/transformers/9b0d0e0946ae3fee39fdebe39ee21573798d1cbfde290685fb9acbc2cf395943.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0806 20:39:59.604834 4680543680 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/textattack/roberta-base-MNLI/added_tokens.json from cache at None\n",
      "I0806 20:39:59.605392 4680543680 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/textattack/roberta-base-MNLI/special_tokens_map.json from cache at /Users/victor/.cache/torch/transformers/964dd305851a42837e2d3a231c99e1fee77acb8de6ba4c938bf508e764735a85.16f949018cf247a2ea7465a74ca9a292212875e5fd72f969e0807011e7f192e4\n",
      "I0806 20:39:59.606163 4680543680 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/textattack/roberta-base-MNLI/tokenizer_config.json from cache at /Users/victor/.cache/torch/transformers/48f4a58ef975194a6e6d7b477a816def0c0ea8d7db8bb6195f3cffe2a3357658.70b5dbd5d3b9b4c9bfb3d1f6464291ff52f6a8d96358899aa3834e173b45092d\n"
     ]
    }
   ],
   "source": [
    "if args.berttweettokenizer_path:\n",
    "    tokenizer = BERTweetTokenizer(args.berttweettokenizer_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.transformer_model_pretrained_path)\n",
    "\n",
    "if not args.berttweettokenizer_path:\n",
    "    try:\n",
    "        bpetokenizer = tokenizers.ByteLevelBPETokenizer(args.bpe_vocab_path, \\\n",
    "                                        args.bpe_merges_path)\n",
    "    except:\n",
    "        bpetokenizer = None \n",
    "else:\n",
    "    bpetokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_utils.TransformerDatasetForMNLI(train_df.premise, train_df.hypothesis, \\\n",
    "                                tokenizer=tokenizer, MAX_LEN=args.max_text_len, target_label=train_df.label)\n",
    "val_dataset = data_utils.TransformerDatasetForMNLI(val_df.premise, val_df.hypothesis, \\\n",
    "                                tokenizer=tokenizer, MAX_LEN=args.max_text_len, target_label=val_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, base_model, dropout=.3, n_out=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.base_model = base_model\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(base_model.config.hidden_size, n_out)\n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        o2 = self.base_model(ids)\n",
    "        o2 = o2[1]\n",
    "        bo = self.drop(o2)\n",
    "        logits = self.out(bo)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 20:40:27.774886 4680543680 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/textattack/roberta-base-MNLI/config.json from cache at /Users/victor/.cache/torch/transformers/a0e9a64482bfa531a2b455bb5c56da303d4bc5dfcb9d0204a326d1dac03ee18e.5ab50e081c86cc5d316d4bd6224f34c782f19b06be3485cd6e269b5a46d0554d\n",
      "I0806 20:40:27.775756 4680543680 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0806 20:40:28.892656 4680543680 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/textattack/roberta-base-MNLI/pytorch_model.bin from cache at /Users/victor/.cache/torch/transformers/adf4c3f24b3d74b58a6f270e5720f79958c5de9e0c3cb215b296abb8d73505df.c61b25f9a58caab2798277b401dbab75a3bdcdbacf89a1f8e9f95bdeeb84ee95\n"
     ]
    }
   ],
   "source": [
    "basemodel = AutoModel.from_pretrained(args.transformer_model_pretrained_path)\n",
    "model = TransformerModel(basemodel, n_out=len(label2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (base_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (out): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      train_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True\n",
    "    )\n",
    "\n",
    "    val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      val_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, sampler=train_sampler,\n",
    "        drop_last=True,num_workers=2)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size, sampler=val_sampler,\n",
    "        drop_last=False,num_workers=1)\n",
    "else:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4])\n"
     ]
    }
   ],
   "source": [
    "for d in train_data_loader:\n",
    "    break\n",
    "\n",
    "logit = model(d['ids'],d['mask'],d['token_type_ids'])\n",
    "print (logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3504, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = losses.get_loss(args.loss_function)\n",
    "print (loss(logit,d['targets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2785714285714286\n"
     ]
    }
   ],
   "source": [
    "metric = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "print (metric(d['targets'].detach().cpu().numpy(),logit.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pytorch Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "[LOG] Total number of parameters to learn 124648708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "\n",
      "Current training Loss 1.4:   0%|          | 0/50 [00:10<?, ?it/s]\u001b[A\n",
      "Current training Loss 1.4:   2%|▏         | 1/50 [00:10<08:14, 10.09s/it]\u001b[A\n",
      "Current training Loss 1.345:   2%|▏         | 1/50 [00:20<08:14, 10.09s/it]\u001b[A\n",
      "Current training Loss 1.345:   4%|▍         | 2/50 [00:20<08:05, 10.11s/it]\u001b[A\n",
      "Current training Loss 1.299:   4%|▍         | 2/50 [00:31<08:05, 10.11s/it]\u001b[A\n",
      "Current training Loss 1.299:   6%|▌         | 3/50 [00:31<08:08, 10.40s/it]\u001b[A\n",
      "Current training Loss 1.14:   6%|▌         | 3/50 [00:39<08:08, 10.40s/it] \u001b[A\n",
      "Current training Loss 1.14:   8%|▊         | 4/50 [00:39<07:25,  9.69s/it]\u001b[A\n",
      "Current training Loss 1.203:   8%|▊         | 4/50 [00:49<07:25,  9.69s/it]\u001b[A\n",
      "Current training Loss 1.203:  10%|█         | 5/50 [00:49<07:18,  9.74s/it]\u001b[A\n",
      "Current training Loss 1.245:  10%|█         | 5/50 [00:58<07:18,  9.74s/it]\u001b[A\n",
      "Current training Loss 1.245:  12%|█▏        | 6/50 [00:58<06:59,  9.54s/it]\u001b[A\n",
      "Current training Loss 1.128:  12%|█▏        | 6/50 [01:05<06:59,  9.54s/it]\u001b[A\n",
      "Current training Loss 1.128:  14%|█▍        | 7/50 [01:05<06:15,  8.74s/it]\u001b[A\n",
      "Current training Loss 1.069:  14%|█▍        | 7/50 [01:12<06:15,  8.74s/it]\u001b[A\n",
      "Current training Loss 1.069:  16%|█▌        | 8/50 [01:12<05:43,  8.17s/it]\u001b[A\n",
      "Current training Loss 1.0:  16%|█▌        | 8/50 [01:19<05:43,  8.17s/it]  \u001b[A\n",
      "Current training Loss 1.0:  18%|█▊        | 9/50 [01:19<05:27,  7.98s/it]\u001b[A\n",
      "Current training Loss 0.92:  18%|█▊        | 9/50 [01:27<05:27,  7.98s/it]\u001b[A\n",
      "Current training Loss 0.92:  20%|██        | 10/50 [01:27<05:15,  7.90s/it]\u001b[A\n",
      "Current training Loss 0.879:  20%|██        | 10/50 [01:34<05:15,  7.90s/it]\u001b[A\n",
      "Current training Loss 0.879:  22%|██▏       | 11/50 [01:34<05:00,  7.69s/it]\u001b[A\n",
      "Current training Loss 0.828:  22%|██▏       | 11/50 [01:41<05:00,  7.69s/it]\u001b[A\n",
      "Current training Loss 0.828:  24%|██▍       | 12/50 [01:41<04:41,  7.40s/it]\u001b[A\n",
      "Current training Loss 0.783:  24%|██▍       | 12/50 [01:48<04:41,  7.40s/it]\u001b[A\n",
      "Current training Loss 0.783:  26%|██▌       | 13/50 [01:48<04:30,  7.32s/it]\u001b[A\n",
      "Current training Loss 0.7:  26%|██▌       | 13/50 [01:55<04:30,  7.32s/it]  \u001b[A\n",
      "Current training Loss 0.7:  28%|██▊       | 14/50 [01:55<04:24,  7.34s/it]\u001b[A\n",
      "Current training Loss 0.871:  28%|██▊       | 14/50 [02:02<04:24,  7.34s/it]\u001b[A\n",
      "Current training Loss 0.871:  30%|███       | 15/50 [02:02<04:14,  7.27s/it]\u001b[A\n",
      "Current training Loss 0.622:  30%|███       | 15/50 [02:10<04:14,  7.27s/it]\u001b[A\n",
      "Current training Loss 0.622:  32%|███▏      | 16/50 [02:10<04:10,  7.38s/it]\u001b[A\n",
      "Current training Loss 0.777:  32%|███▏      | 16/50 [02:17<04:10,  7.38s/it]\u001b[A\n",
      "Current training Loss 0.777:  34%|███▍      | 17/50 [02:17<03:57,  7.20s/it]\u001b[A\n",
      "Current training Loss 0.681:  34%|███▍      | 17/50 [02:26<03:57,  7.20s/it]\u001b[A\n",
      "Current training Loss 0.681:  36%|███▌      | 18/50 [02:26<04:05,  7.68s/it]\u001b[A\n",
      "Current training Loss 0.94:  36%|███▌      | 18/50 [02:33<04:05,  7.68s/it] \u001b[A\n",
      "Current training Loss 0.94:  38%|███▊      | 19/50 [02:33<03:54,  7.56s/it]\u001b[A\n",
      "Current training Loss 0.555:  38%|███▊      | 19/50 [02:42<03:54,  7.56s/it]\u001b[A\n",
      "Current training Loss 0.555:  40%|████      | 20/50 [02:42<04:01,  8.05s/it]\u001b[A\n",
      "Current training Loss 1.104:  40%|████      | 20/50 [02:50<04:01,  8.05s/it]\u001b[A\n",
      "Current training Loss 1.104:  42%|████▏     | 21/50 [02:50<03:57,  8.18s/it]\u001b[A\n",
      "Current training Loss 0.566:  42%|████▏     | 21/50 [02:59<03:57,  8.18s/it]\u001b[A\n",
      "Current training Loss 0.566:  44%|████▍     | 22/50 [02:59<03:51,  8.28s/it]\u001b[A\n",
      "Current training Loss 0.616:  44%|████▍     | 22/50 [03:06<03:51,  8.28s/it]\u001b[A\n",
      "Current training Loss 0.616:  46%|████▌     | 23/50 [03:06<03:31,  7.85s/it]\u001b[A\n",
      "Current training Loss 0.634:  46%|████▌     | 23/50 [03:12<03:31,  7.85s/it]\u001b[A\n",
      "Current training Loss 0.634:  48%|████▊     | 24/50 [03:12<03:12,  7.42s/it]\u001b[A\n",
      "Current training Loss 0.549:  48%|████▊     | 24/50 [03:19<03:12,  7.42s/it]\u001b[A\n",
      "Current training Loss 0.549:  50%|█████     | 25/50 [03:19<02:57,  7.10s/it]\u001b[A\n",
      "Current training Loss 0.723:  50%|█████     | 25/50 [03:25<02:57,  7.10s/it]\u001b[A\n",
      "Current training Loss 0.723:  52%|█████▏    | 26/50 [03:25<02:44,  6.87s/it]\u001b[A\n",
      "Current training Loss 0.903:  52%|█████▏    | 26/50 [03:31<02:44,  6.87s/it]\u001b[A\n",
      "Current training Loss 0.903:  54%|█████▍    | 27/50 [03:31<02:34,  6.72s/it]\u001b[A\n",
      "Current training Loss 0.795:  54%|█████▍    | 27/50 [03:38<02:34,  6.72s/it]\u001b[A\n",
      "Current training Loss 0.795:  56%|█████▌    | 28/50 [03:38<02:25,  6.60s/it]\u001b[A\n",
      "Current training Loss 0.434:  56%|█████▌    | 28/50 [03:44<02:25,  6.60s/it]\u001b[A\n",
      "Current training Loss 0.434:  58%|█████▊    | 29/50 [03:44<02:16,  6.52s/it]\u001b[A\n",
      "Current training Loss 0.272:  58%|█████▊    | 29/50 [03:51<02:16,  6.52s/it]\u001b[A\n",
      "Current training Loss 0.272:  60%|██████    | 30/50 [03:51<02:12,  6.61s/it]\u001b[A\n",
      "Current training Loss 0.996:  60%|██████    | 30/50 [03:58<02:12,  6.61s/it]\u001b[A\n",
      "Current training Loss 0.996:  62%|██████▏   | 31/50 [03:58<02:07,  6.71s/it]\u001b[A\n",
      "Current training Loss 0.585:  62%|██████▏   | 31/50 [04:04<02:07,  6.71s/it]\u001b[A\n",
      "Current training Loss 0.585:  64%|██████▍   | 32/50 [04:04<01:59,  6.62s/it]\u001b[A\n",
      "Current training Loss 0.752:  64%|██████▍   | 32/50 [04:10<01:59,  6.62s/it]\u001b[A\n",
      "Current training Loss 0.752:  66%|██████▌   | 33/50 [04:10<01:51,  6.54s/it]\u001b[A\n",
      "Current training Loss 0.885:  66%|██████▌   | 33/50 [04:17<01:51,  6.54s/it]\u001b[A\n",
      "Current training Loss 0.885:  68%|██████▊   | 34/50 [04:17<01:43,  6.49s/it]\u001b[A\n",
      "Current training Loss 0.549:  68%|██████▊   | 34/50 [04:24<01:43,  6.49s/it]\u001b[A\n",
      "Current training Loss 0.549:  70%|███████   | 35/50 [04:24<01:39,  6.65s/it]\u001b[A\n",
      "Current training Loss 0.32:  70%|███████   | 35/50 [04:30<01:39,  6.65s/it] \u001b[A\n",
      "Current training Loss 0.32:  72%|███████▏  | 36/50 [04:30<01:31,  6.53s/it]\u001b[A\n",
      "Current training Loss 0.508:  72%|███████▏  | 36/50 [04:36<01:31,  6.53s/it]\u001b[A\n",
      "Current training Loss 0.508:  74%|███████▍  | 37/50 [04:36<01:23,  6.42s/it]\u001b[A\n",
      "Current training Loss 0.423:  74%|███████▍  | 37/50 [04:43<01:23,  6.42s/it]\u001b[A\n",
      "Current training Loss 0.423:  76%|███████▌  | 38/50 [04:43<01:16,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.612:  76%|███████▌  | 38/50 [04:49<01:16,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.612:  78%|███████▊  | 39/50 [04:49<01:09,  6.33s/it]\u001b[A\n",
      "Current training Loss 0.338:  78%|███████▊  | 39/50 [04:55<01:09,  6.33s/it]\u001b[A\n",
      "Current training Loss 0.338:  80%|████████  | 40/50 [04:55<01:02,  6.29s/it]\u001b[A\n",
      "Current training Loss 0.521:  80%|████████  | 40/50 [05:01<01:02,  6.29s/it]\u001b[A\n",
      "Current training Loss 0.521:  82%|████████▏ | 41/50 [05:01<00:57,  6.35s/it]\u001b[A\n",
      "Current training Loss 0.307:  82%|████████▏ | 41/50 [05:09<00:57,  6.35s/it]\u001b[A\n",
      "Current training Loss 0.307:  84%|████████▍ | 42/50 [05:09<00:52,  6.60s/it]\u001b[A\n",
      "Current training Loss 0.431:  84%|████████▍ | 42/50 [05:15<00:52,  6.60s/it]\u001b[A\n",
      "Current training Loss 0.431:  86%|████████▌ | 43/50 [05:15<00:45,  6.53s/it]\u001b[A\n",
      "Current training Loss 0.518:  86%|████████▌ | 43/50 [05:21<00:45,  6.53s/it]\u001b[A\n",
      "Current training Loss 0.518:  88%|████████▊ | 44/50 [05:21<00:38,  6.43s/it]\u001b[A\n",
      "Current training Loss 0.534:  88%|████████▊ | 44/50 [05:27<00:38,  6.43s/it]\u001b[A\n",
      "Current training Loss 0.534:  90%|█████████ | 45/50 [05:27<00:31,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.673:  90%|█████████ | 45/50 [05:34<00:31,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.673:  92%|█████████▏| 46/50 [05:34<00:25,  6.31s/it]\u001b[A\n",
      "Current training Loss 0.824:  92%|█████████▏| 46/50 [05:40<00:25,  6.31s/it]\u001b[A\n",
      "Current training Loss 0.824:  94%|█████████▍| 47/50 [05:40<00:18,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.377:  94%|█████████▍| 47/50 [05:46<00:18,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.377:  96%|█████████▌| 48/50 [05:46<00:12,  6.25s/it]\u001b[A\n",
      "Current training Loss 0.571:  96%|█████████▌| 48/50 [05:52<00:12,  6.25s/it]\u001b[A\n",
      "Current training Loss 0.571:  98%|█████████▊| 49/50 [05:52<00:06,  6.25s/it]\u001b[A\n",
      "Current training Loss 0.459:  98%|█████████▊| 49/50 [05:59<00:06,  6.25s/it]\u001b[A\n",
      "Current training Loss 0.459: 100%|██████████| 50/50 [05:59<00:00,  7.18s/it]\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.62:   0%|          | 0/50 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.62:   2%|▏         | 1/50 [00:01<01:17,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.447:   2%|▏         | 1/50 [00:03<01:17,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.447:   4%|▍         | 2/50 [00:03<01:14,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.761:   4%|▍         | 2/50 [00:04<01:14,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.761:   6%|▌         | 3/50 [00:04<01:13,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.811:   6%|▌         | 3/50 [00:06<01:13,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.811:   8%|▊         | 4/50 [00:06<01:11,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.542:   8%|▊         | 4/50 [00:07<01:11,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.542:  10%|█         | 5/50 [00:07<01:09,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.752:  10%|█         | 5/50 [00:09<01:09,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.752:  12%|█▏        | 6/50 [00:09<01:07,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.494:  12%|█▏        | 6/50 [00:10<01:07,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.494:  14%|█▍        | 7/50 [00:10<01:06,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.459:  14%|█▍        | 7/50 [00:12<01:06,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.459:  16%|█▌        | 8/50 [00:12<01:04,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.121:  16%|█▌        | 8/50 [00:13<01:04,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.121:  18%|█▊        | 9/50 [00:13<01:04,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.164:  18%|█▊        | 9/50 [00:15<01:04,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.164:  20%|██        | 10/50 [00:15<01:02,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.227:  20%|██        | 10/50 [00:17<01:02,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.227:  22%|██▏       | 11/50 [00:17<01:00,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.413:  22%|██▏       | 11/50 [00:18<01:00,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.413:  24%|██▍       | 12/50 [00:18<00:58,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.35:  24%|██▍       | 12/50 [00:20<00:58,  1.54s/it] \u001b[A\n",
      "Current eval Loss 0.35:  26%|██▌       | 13/50 [00:20<00:57,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.417:  26%|██▌       | 13/50 [00:21<00:57,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.417:  28%|██▊       | 14/50 [00:21<00:55,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.321:  28%|██▊       | 14/50 [00:23<00:55,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.321:  30%|███       | 15/50 [00:23<00:53,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.13:  30%|███       | 15/50 [00:24<00:53,  1.54s/it] \u001b[A\n",
      "Current eval Loss 0.13:  32%|███▏      | 16/50 [00:24<00:52,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.213:  32%|███▏      | 16/50 [00:26<00:52,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.213:  34%|███▍      | 17/50 [00:26<00:50,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.425:  34%|███▍      | 17/50 [00:27<00:50,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.425:  36%|███▌      | 18/50 [00:27<00:49,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.701:  36%|███▌      | 18/50 [00:29<00:49,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.701:  38%|███▊      | 19/50 [00:29<00:47,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.358:  38%|███▊      | 19/50 [00:30<00:47,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.358:  40%|████      | 20/50 [00:30<00:46,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.647:  40%|████      | 20/50 [00:32<00:46,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.647:  42%|████▏     | 21/50 [00:32<00:44,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.316:  42%|████▏     | 21/50 [00:33<00:44,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.316:  44%|████▍     | 22/50 [00:33<00:43,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.441:  44%|████▍     | 22/50 [00:35<00:43,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.441:  46%|████▌     | 23/50 [00:35<00:41,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.181:  46%|████▌     | 23/50 [00:37<00:41,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.181:  48%|████▊     | 24/50 [00:37<00:40,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.149:  48%|████▊     | 24/50 [00:38<00:40,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.149:  50%|█████     | 25/50 [00:38<00:38,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.293:  50%|█████     | 25/50 [00:40<00:38,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.293:  52%|█████▏    | 26/50 [00:40<00:37,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.306:  52%|█████▏    | 26/50 [00:41<00:37,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.306:  54%|█████▍    | 27/50 [00:41<00:35,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.474:  54%|█████▍    | 27/50 [00:43<00:35,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.474:  56%|█████▌    | 28/50 [00:43<00:33,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.199:  56%|█████▌    | 28/50 [00:44<00:33,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.199:  58%|█████▊    | 29/50 [00:44<00:32,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.225:  58%|█████▊    | 29/50 [00:46<00:32,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.225:  60%|██████    | 30/50 [00:46<00:30,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.534:  60%|██████    | 30/50 [00:47<00:30,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.534:  62%|██████▏   | 31/50 [00:47<00:29,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.322:  62%|██████▏   | 31/50 [00:49<00:29,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.322:  64%|██████▍   | 32/50 [00:49<00:27,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.401:  64%|██████▍   | 32/50 [00:50<00:27,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.401:  66%|██████▌   | 33/50 [00:50<00:26,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.309:  66%|██████▌   | 33/50 [00:52<00:26,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.309:  68%|██████▊   | 34/50 [00:52<00:24,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.11:  68%|██████▊   | 34/50 [00:53<00:24,  1.53s/it] \u001b[A\n",
      "Current eval Loss 0.11:  70%|███████   | 35/50 [00:53<00:22,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.127:  70%|███████   | 35/50 [00:55<00:22,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.127:  72%|███████▏  | 36/50 [00:55<00:21,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.391:  72%|███████▏  | 36/50 [00:56<00:21,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.391:  74%|███████▍  | 37/50 [00:56<00:19,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.34:  74%|███████▍  | 37/50 [00:58<00:19,  1.53s/it] \u001b[A\n",
      "Current eval Loss 0.34:  76%|███████▌  | 38/50 [00:58<00:18,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.385:  76%|███████▌  | 38/50 [01:00<00:18,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.385:  78%|███████▊  | 39/50 [01:00<00:16,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.16:  78%|███████▊  | 39/50 [01:01<00:16,  1.53s/it] \u001b[A\n",
      "Current eval Loss 0.16:  80%|████████  | 40/50 [01:01<00:15,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.217:  80%|████████  | 40/50 [01:03<00:15,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.217:  82%|████████▏ | 41/50 [01:03<00:13,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.142:  82%|████████▏ | 41/50 [01:04<00:13,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.142:  84%|████████▍ | 42/50 [01:04<00:12,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.166:  84%|████████▍ | 42/50 [01:06<00:12,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.166:  86%|████████▌ | 43/50 [01:06<00:10,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.287:  86%|████████▌ | 43/50 [01:07<00:10,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.287:  88%|████████▊ | 44/50 [01:07<00:09,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.423:  88%|████████▊ | 44/50 [01:09<00:09,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.423:  90%|█████████ | 45/50 [01:09<00:07,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.463:  90%|█████████ | 45/50 [01:10<00:07,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.463:  92%|█████████▏| 46/50 [01:10<00:06,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.465:  92%|█████████▏| 46/50 [01:12<00:06,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.465:  94%|█████████▍| 47/50 [01:12<00:04,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.21:  94%|█████████▍| 47/50 [01:13<00:04,  1.54s/it] \u001b[A\n",
      "Current eval Loss 0.21:  96%|█████████▌| 48/50 [01:13<00:03,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.344:  96%|█████████▌| 48/50 [01:15<00:03,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.344:  98%|█████████▊| 49/50 [01:15<00:01,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.341:  98%|█████████▊| 49/50 [01:17<00:01,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.341: 100%|██████████| 50/50 [01:17<00:00,  1.54s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.595:   0%|          | 0/13 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.595:   8%|▊         | 1/13 [00:01<00:18,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.227:   8%|▊         | 1/13 [00:03<00:18,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.227:  15%|█▌        | 2/13 [00:03<00:17,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.471:  15%|█▌        | 2/13 [00:04<00:17,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.471:  23%|██▎       | 3/13 [00:04<00:15,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.358:  23%|██▎       | 3/13 [00:06<00:15,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.358:  31%|███       | 4/13 [00:06<00:13,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.228:  31%|███       | 4/13 [00:07<00:13,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.228:  38%|███▊      | 5/13 [00:07<00:12,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.275:  38%|███▊      | 5/13 [00:09<00:12,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.275:  46%|████▌     | 6/13 [00:09<00:10,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.671:  46%|████▌     | 6/13 [00:10<00:10,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.671:  54%|█████▍    | 7/13 [00:10<00:09,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.758:  54%|█████▍    | 7/13 [00:12<00:09,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.758:  62%|██████▏   | 8/13 [00:12<00:07,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.602:  62%|██████▏   | 8/13 [00:13<00:07,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.602:  69%|██████▉   | 9/13 [00:13<00:06,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.535:  69%|██████▉   | 9/13 [00:15<00:06,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.535:  77%|███████▋  | 10/13 [00:15<00:04,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.32:  77%|███████▋  | 10/13 [00:16<00:04,  1.53s/it] \u001b[A\n",
      "Current eval Loss 0.32:  85%|████████▍ | 11/13 [00:16<00:03,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.663:  85%|████████▍ | 11/13 [00:18<00:03,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.663:  92%|█████████▏| 12/13 [00:18<00:01,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.525:  92%|█████████▏| 12/13 [00:19<00:01,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.525: 100%|██████████| 13/13 [00:19<00:00,  1.48s/it]\u001b[A\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.362 Train metric = 0.662 Val loss = 0.479 Val metric = 0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Current training Loss 0.649:   0%|          | 0/50 [00:06<?, ?it/s]\u001b[A\n",
      "Current training Loss 0.649:   2%|▏         | 1/50 [00:06<05:13,  6.41s/it]\u001b[A\n",
      "Current training Loss 0.536:   2%|▏         | 1/50 [00:12<05:13,  6.41s/it]\u001b[A\n",
      "Current training Loss 0.536:   4%|▍         | 2/50 [00:12<05:06,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.695:   4%|▍         | 2/50 [00:19<05:06,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.695:   6%|▌         | 3/50 [00:19<04:59,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.719:   6%|▌         | 3/50 [00:26<04:59,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.719:   8%|▊         | 4/50 [00:26<05:07,  6.68s/it]\u001b[A\n",
      "Current training Loss 0.459:   8%|▊         | 4/50 [00:32<05:07,  6.68s/it]\u001b[A\n",
      "Current training Loss 0.459:  10%|█         | 5/50 [00:32<04:57,  6.60s/it]\u001b[A\n",
      "Current training Loss 0.848:  10%|█         | 5/50 [00:39<04:57,  6.60s/it]\u001b[A\n",
      "Current training Loss 0.848:  12%|█▏        | 6/50 [00:39<04:57,  6.76s/it]\u001b[A\n",
      "Current training Loss 0.508:  12%|█▏        | 6/50 [00:47<04:57,  6.76s/it]\u001b[A\n",
      "Current training Loss 0.508:  14%|█▍        | 7/50 [00:47<05:04,  7.08s/it]\u001b[A\n",
      "Current training Loss 0.6:  14%|█▍        | 7/50 [00:55<05:04,  7.08s/it]  \u001b[A\n",
      "Current training Loss 0.6:  16%|█▌        | 8/50 [00:55<05:05,  7.28s/it]\u001b[A\n",
      "Current training Loss 0.153:  16%|█▌        | 8/50 [01:03<05:05,  7.28s/it]\u001b[A\n",
      "Current training Loss 0.153:  18%|█▊        | 9/50 [01:03<05:01,  7.34s/it]\u001b[A\n",
      "Current training Loss 0.31:  18%|█▊        | 9/50 [01:09<05:01,  7.34s/it] \u001b[A\n",
      "Current training Loss 0.31:  20%|██        | 10/50 [01:09<04:42,  7.07s/it]\u001b[A\n",
      "Current training Loss 0.275:  20%|██        | 10/50 [01:15<04:42,  7.07s/it]\u001b[A\n",
      "Current training Loss 0.275:  22%|██▏       | 11/50 [01:15<04:26,  6.84s/it]\u001b[A\n",
      "Current training Loss 0.477:  22%|██▏       | 11/50 [01:22<04:26,  6.84s/it]\u001b[A\n",
      "Current training Loss 0.477:  24%|██▍       | 12/50 [01:22<04:15,  6.71s/it]\u001b[A\n",
      "Current training Loss 0.247:  24%|██▍       | 12/50 [01:28<04:15,  6.71s/it]\u001b[A\n",
      "Current training Loss 0.247:  26%|██▌       | 13/50 [01:28<04:05,  6.64s/it]\u001b[A\n",
      "Current training Loss 0.218:  26%|██▌       | 13/50 [01:35<04:05,  6.64s/it]\u001b[A\n",
      "Current training Loss 0.218:  28%|██▊       | 14/50 [01:35<04:03,  6.76s/it]\u001b[A\n",
      "Current training Loss 0.437:  28%|██▊       | 14/50 [01:42<04:03,  6.76s/it]\u001b[A\n",
      "Current training Loss 0.437:  30%|███       | 15/50 [01:42<03:55,  6.73s/it]\u001b[A\n",
      "Current training Loss 0.254:  30%|███       | 15/50 [01:48<03:55,  6.73s/it]\u001b[A\n",
      "Current training Loss 0.254:  32%|███▏      | 16/50 [01:48<03:47,  6.68s/it]\u001b[A\n",
      "Current training Loss 0.149:  32%|███▏      | 16/50 [01:55<03:47,  6.68s/it]\u001b[A\n",
      "Current training Loss 0.149:  34%|███▍      | 17/50 [01:55<03:36,  6.57s/it]\u001b[A\n",
      "Current training Loss 0.42:  34%|███▍      | 17/50 [02:01<03:36,  6.57s/it] \u001b[A\n",
      "Current training Loss 0.42:  36%|███▌      | 18/50 [02:01<03:27,  6.48s/it]\u001b[A\n",
      "Current training Loss 0.682:  36%|███▌      | 18/50 [02:07<03:27,  6.48s/it]\u001b[A\n",
      "Current training Loss 0.682:  38%|███▊      | 19/50 [02:07<03:19,  6.44s/it]\u001b[A\n",
      "Current training Loss 0.475:  38%|███▊      | 19/50 [02:14<03:19,  6.44s/it]\u001b[A\n",
      "Current training Loss 0.475:  40%|████      | 20/50 [02:14<03:11,  6.39s/it]\u001b[A\n",
      "Current training Loss 0.897:  40%|████      | 20/50 [02:20<03:11,  6.39s/it]\u001b[A\n",
      "Current training Loss 0.897:  42%|████▏     | 21/50 [02:20<03:04,  6.37s/it]\u001b[A\n",
      "Current training Loss 0.358:  42%|████▏     | 21/50 [02:28<03:04,  6.37s/it]\u001b[A\n",
      "Current training Loss 0.358:  44%|████▍     | 22/50 [02:28<03:09,  6.77s/it]\u001b[A\n",
      "Current training Loss 0.323:  44%|████▍     | 22/50 [02:34<03:09,  6.77s/it]\u001b[A\n",
      "Current training Loss 0.323:  46%|████▌     | 23/50 [02:34<02:59,  6.65s/it]\u001b[A\n",
      "Current training Loss 0.172:  46%|████▌     | 23/50 [02:40<02:59,  6.65s/it]\u001b[A\n",
      "Current training Loss 0.172:  48%|████▊     | 24/50 [02:40<02:49,  6.52s/it]\u001b[A\n",
      "Current training Loss 0.159:  48%|████▊     | 24/50 [02:47<02:49,  6.52s/it]\u001b[A\n",
      "Current training Loss 0.159:  50%|█████     | 25/50 [02:47<02:40,  6.44s/it]\u001b[A\n",
      "Current training Loss 0.375:  50%|█████     | 25/50 [02:53<02:40,  6.44s/it]\u001b[A\n",
      "Current training Loss 0.375:  52%|█████▏    | 26/50 [02:53<02:32,  6.37s/it]\u001b[A\n",
      "Current training Loss 0.243:  52%|█████▏    | 26/50 [02:59<02:32,  6.37s/it]\u001b[A\n",
      "Current training Loss 0.243:  54%|█████▍    | 27/50 [02:59<02:25,  6.32s/it]\u001b[A\n",
      "Current training Loss 0.554:  54%|█████▍    | 27/50 [03:05<02:25,  6.32s/it]\u001b[A\n",
      "Current training Loss 0.554:  56%|█████▌    | 28/50 [03:05<02:18,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.138:  56%|█████▌    | 28/50 [03:11<02:18,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.138:  58%|█████▊    | 29/50 [03:11<02:11,  6.27s/it]\u001b[A\n",
      "Current training Loss 0.392:  58%|█████▊    | 29/50 [03:18<02:11,  6.27s/it]\u001b[A\n",
      "Current training Loss 0.392:  60%|██████    | 30/50 [03:18<02:05,  6.26s/it]\u001b[A\n",
      "Current training Loss 0.549:  60%|██████    | 30/50 [03:24<02:05,  6.26s/it]\u001b[A\n",
      "Current training Loss 0.549:  62%|██████▏   | 31/50 [03:24<01:58,  6.25s/it]\u001b[A\n",
      "Current training Loss 0.241:  62%|██████▏   | 31/50 [03:30<01:58,  6.25s/it]\u001b[A\n",
      "Current training Loss 0.241:  64%|██████▍   | 32/50 [03:30<01:52,  6.23s/it]\u001b[A\n",
      "Current training Loss 0.368:  64%|██████▍   | 32/50 [03:36<01:52,  6.23s/it]\u001b[A\n",
      "Current training Loss 0.368:  66%|██████▌   | 33/50 [03:36<01:46,  6.24s/it]\u001b[A\n",
      "Current training Loss 0.358:  66%|██████▌   | 33/50 [03:43<01:46,  6.24s/it]\u001b[A\n",
      "Current training Loss 0.358:  68%|██████▊   | 34/50 [03:43<01:40,  6.29s/it]\u001b[A\n",
      "Current training Loss 0.075:  68%|██████▊   | 34/50 [03:49<01:40,  6.29s/it]\u001b[A\n",
      "Current training Loss 0.075:  70%|███████   | 35/50 [03:49<01:35,  6.37s/it]\u001b[A\n",
      "Current training Loss 0.1:  70%|███████   | 35/50 [03:56<01:35,  6.37s/it]  \u001b[A\n",
      "Current training Loss 0.1:  72%|███████▏  | 36/50 [03:56<01:29,  6.40s/it]\u001b[A\n",
      "Current training Loss 0.401:  72%|███████▏  | 36/50 [04:02<01:29,  6.40s/it]\u001b[A\n",
      "Current training Loss 0.401:  74%|███████▍  | 37/50 [04:02<01:23,  6.44s/it]\u001b[A\n",
      "Current training Loss 0.2:  74%|███████▍  | 37/50 [04:09<01:23,  6.44s/it]  \u001b[A\n",
      "Current training Loss 0.2:  76%|███████▌  | 38/50 [04:09<01:17,  6.42s/it]\u001b[A\n",
      "Current training Loss 0.347:  76%|███████▌  | 38/50 [04:15<01:17,  6.42s/it]\u001b[A\n",
      "Current training Loss 0.347:  78%|███████▊  | 39/50 [04:15<01:10,  6.40s/it]\u001b[A\n",
      "Current training Loss 0.145:  78%|███████▊  | 39/50 [04:21<01:10,  6.40s/it]\u001b[A\n",
      "Current training Loss 0.145:  80%|████████  | 40/50 [04:21<01:03,  6.39s/it]\u001b[A\n",
      "Current training Loss 0.15:  80%|████████  | 40/50 [04:28<01:03,  6.39s/it] \u001b[A\n",
      "Current training Loss 0.15:  82%|████████▏ | 41/50 [04:28<00:57,  6.37s/it]\u001b[A\n",
      "Current training Loss 0.135:  82%|████████▏ | 41/50 [04:34<00:57,  6.37s/it]\u001b[A\n",
      "Current training Loss 0.135:  84%|████████▍ | 42/50 [04:34<00:51,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.177:  84%|████████▍ | 42/50 [04:40<00:51,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.177:  86%|████████▌ | 43/50 [04:40<00:44,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.262:  86%|████████▌ | 43/50 [04:47<00:44,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.262:  88%|████████▊ | 44/50 [04:47<00:39,  6.52s/it]\u001b[A\n",
      "Current training Loss 0.521:  88%|████████▊ | 44/50 [04:56<00:39,  6.52s/it]\u001b[A\n",
      "Current training Loss 0.521:  90%|█████████ | 45/50 [04:56<00:35,  7.09s/it]\u001b[A\n",
      "Current training Loss 0.326:  90%|█████████ | 45/50 [05:07<00:35,  7.09s/it]\u001b[A\n",
      "Current training Loss 0.326:  92%|█████████▏| 46/50 [05:07<00:33,  8.41s/it]\u001b[A\n",
      "Current training Loss 0.201:  92%|█████████▏| 46/50 [05:15<00:33,  8.41s/it]\u001b[A\n",
      "Current training Loss 0.201:  94%|█████████▍| 47/50 [05:15<00:25,  8.36s/it]\u001b[A\n",
      "Current training Loss 0.239:  94%|█████████▍| 47/50 [05:23<00:25,  8.36s/it]\u001b[A\n",
      "Current training Loss 0.239:  96%|█████████▌| 48/50 [05:23<00:16,  8.19s/it]\u001b[A\n",
      "Current training Loss 0.251:  96%|█████████▌| 48/50 [05:30<00:16,  8.19s/it]\u001b[A\n",
      "Current training Loss 0.251:  98%|█████████▊| 49/50 [05:30<00:07,  7.67s/it]\u001b[A\n",
      "Current training Loss 0.294:  98%|█████████▊| 49/50 [05:36<00:07,  7.67s/it]\u001b[A\n",
      "Current training Loss 0.294: 100%|██████████| 50/50 [05:36<00:00,  6.73s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.33:   0%|          | 0/50 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.33:   2%|▏         | 1/50 [00:01<01:20,  1.65s/it]\u001b[A\n",
      "Current eval Loss 0.366:   2%|▏         | 1/50 [00:03<01:20,  1.65s/it]\u001b[A\n",
      "Current eval Loss 0.366:   4%|▍         | 2/50 [00:03<01:18,  1.64s/it]\u001b[A\n",
      "Current eval Loss 0.521:   4%|▍         | 2/50 [00:04<01:18,  1.64s/it]\u001b[A\n",
      "Current eval Loss 0.521:   6%|▌         | 3/50 [00:04<01:16,  1.62s/it]\u001b[A\n",
      "Current eval Loss 0.413:   6%|▌         | 3/50 [00:06<01:16,  1.62s/it]\u001b[A\n",
      "Current eval Loss 0.413:   8%|▊         | 4/50 [00:06<01:13,  1.61s/it]\u001b[A\n",
      "Current eval Loss 0.509:   8%|▊         | 4/50 [00:08<01:13,  1.61s/it]\u001b[A\n",
      "Current eval Loss 0.509:  10%|█         | 5/50 [00:08<01:12,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.439:  10%|█         | 5/50 [00:09<01:12,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.439:  12%|█▏        | 6/50 [00:09<01:10,  1.59s/it]\u001b[A\n",
      "Current eval Loss 0.318:  12%|█▏        | 6/50 [00:11<01:10,  1.59s/it]\u001b[A\n",
      "Current eval Loss 0.318:  14%|█▍        | 7/50 [00:11<01:08,  1.59s/it]\u001b[A\n",
      "Current eval Loss 0.306:  14%|█▍        | 7/50 [00:12<01:08,  1.59s/it]\u001b[A\n",
      "Current eval Loss 0.306:  16%|█▌        | 8/50 [00:12<01:07,  1.61s/it]\u001b[A\n",
      "Current eval Loss 0.06:  16%|█▌        | 8/50 [00:14<01:07,  1.61s/it] \u001b[A\n",
      "Current eval Loss 0.06:  18%|█▊        | 9/50 [00:14<01:07,  1.63s/it]\u001b[A\n",
      "Current eval Loss 0.069:  18%|█▊        | 9/50 [00:16<01:07,  1.63s/it]\u001b[A\n",
      "Current eval Loss 0.069:  20%|██        | 10/50 [00:16<01:05,  1.65s/it]\u001b[A\n",
      "Current eval Loss 0.062:  20%|██        | 10/50 [00:17<01:05,  1.65s/it]\u001b[A\n",
      "Current eval Loss 0.062:  22%|██▏       | 11/50 [00:17<01:05,  1.67s/it]\u001b[A\n",
      "Current eval Loss 0.232:  22%|██▏       | 11/50 [00:19<01:05,  1.67s/it]\u001b[A\n",
      "Current eval Loss 0.232:  24%|██▍       | 12/50 [00:19<01:07,  1.79s/it]\u001b[A\n",
      "Current eval Loss 0.191:  24%|██▍       | 12/50 [00:22<01:07,  1.79s/it]\u001b[A\n",
      "Current eval Loss 0.191:  26%|██▌       | 13/50 [00:22<01:16,  2.07s/it]\u001b[A\n",
      "Current eval Loss 0.248:  26%|██▌       | 13/50 [00:24<01:16,  2.07s/it]\u001b[A\n",
      "Current eval Loss 0.248:  28%|██▊       | 14/50 [00:24<01:12,  2.02s/it]\u001b[A\n",
      "Current eval Loss 0.237:  28%|██▊       | 14/50 [00:26<01:12,  2.02s/it]\u001b[A\n",
      "Current eval Loss 0.237:  30%|███       | 15/50 [00:26<01:13,  2.10s/it]\u001b[A\n",
      "Current eval Loss 0.067:  30%|███       | 15/50 [00:28<01:13,  2.10s/it]\u001b[A\n",
      "Current eval Loss 0.067:  32%|███▏      | 16/50 [00:28<01:10,  2.06s/it]\u001b[A\n",
      "Current eval Loss 0.066:  32%|███▏      | 16/50 [00:30<01:10,  2.06s/it]\u001b[A\n",
      "Current eval Loss 0.066:  34%|███▍      | 17/50 [00:30<01:06,  2.01s/it]\u001b[A\n",
      "Current eval Loss 0.205:  34%|███▍      | 17/50 [00:32<01:06,  2.01s/it]\u001b[A\n",
      "Current eval Loss 0.205:  36%|███▌      | 18/50 [00:32<01:01,  1.91s/it]\u001b[A\n",
      "Current eval Loss 0.737:  36%|███▌      | 18/50 [00:34<01:01,  1.91s/it]\u001b[A\n",
      "Current eval Loss 0.737:  38%|███▊      | 19/50 [00:34<01:01,  1.98s/it]\u001b[A\n",
      "Current eval Loss 0.202:  38%|███▊      | 19/50 [00:36<01:01,  1.98s/it]\u001b[A\n",
      "Current eval Loss 0.202:  40%|████      | 20/50 [00:36<00:59,  1.98s/it]\u001b[A\n",
      "Current eval Loss 0.58:  40%|████      | 20/50 [00:38<00:59,  1.98s/it] \u001b[A\n",
      "Current eval Loss 0.58:  42%|████▏     | 21/50 [00:38<00:56,  1.96s/it]\u001b[A\n",
      "Current eval Loss 0.199:  42%|████▏     | 21/50 [00:40<00:56,  1.96s/it]\u001b[A\n",
      "Current eval Loss 0.199:  44%|████▍     | 22/50 [00:40<00:55,  1.98s/it]\u001b[A\n",
      "Current eval Loss 0.395:  44%|████▍     | 22/50 [00:42<00:55,  1.98s/it]\u001b[A\n",
      "Current eval Loss 0.395:  46%|████▌     | 23/50 [00:42<00:52,  1.94s/it]\u001b[A\n",
      "Current eval Loss 0.096:  46%|████▌     | 23/50 [00:44<00:52,  1.94s/it]\u001b[A\n",
      "Current eval Loss 0.096:  48%|████▊     | 24/50 [00:44<00:49,  1.89s/it]\u001b[A\n",
      "Current eval Loss 0.061:  48%|████▊     | 24/50 [00:45<00:49,  1.89s/it]\u001b[A\n",
      "Current eval Loss 0.061:  50%|█████     | 25/50 [00:45<00:46,  1.88s/it]\u001b[A\n",
      "Current eval Loss 0.211:  50%|█████     | 25/50 [00:47<00:46,  1.88s/it]\u001b[A\n",
      "Current eval Loss 0.211:  52%|█████▏    | 26/50 [00:47<00:44,  1.86s/it]\u001b[A\n",
      "Current eval Loss 0.116:  52%|█████▏    | 26/50 [00:49<00:44,  1.86s/it]\u001b[A\n",
      "Current eval Loss 0.116:  54%|█████▍    | 27/50 [00:49<00:41,  1.81s/it]\u001b[A\n",
      "Current eval Loss 0.244:  54%|█████▍    | 27/50 [00:51<00:41,  1.81s/it]\u001b[A\n",
      "Current eval Loss 0.244:  56%|█████▌    | 28/50 [00:51<00:38,  1.75s/it]\u001b[A\n",
      "Current eval Loss 0.066:  56%|█████▌    | 28/50 [00:52<00:38,  1.75s/it]\u001b[A\n",
      "Current eval Loss 0.066:  58%|█████▊    | 29/50 [00:52<00:36,  1.75s/it]\u001b[A\n",
      "Current eval Loss 0.069:  58%|█████▊    | 29/50 [00:54<00:36,  1.75s/it]\u001b[A\n",
      "Current eval Loss 0.069:  60%|██████    | 30/50 [00:54<00:34,  1.72s/it]\u001b[A\n",
      "Current eval Loss 0.478:  60%|██████    | 30/50 [00:56<00:34,  1.72s/it]\u001b[A\n",
      "Current eval Loss 0.478:  62%|██████▏   | 31/50 [00:56<00:32,  1.69s/it]\u001b[A\n",
      "Current eval Loss 0.298:  62%|██████▏   | 31/50 [00:58<00:32,  1.69s/it]\u001b[A\n",
      "Current eval Loss 0.298:  64%|██████▍   | 32/50 [00:58<00:32,  1.80s/it]\u001b[A\n",
      "Current eval Loss 0.378:  64%|██████▍   | 32/50 [01:00<00:32,  1.80s/it]\u001b[A\n",
      "Current eval Loss 0.378:  66%|██████▌   | 33/50 [01:00<00:32,  1.88s/it]\u001b[A\n",
      "Current eval Loss 0.136:  66%|██████▌   | 33/50 [01:02<00:32,  1.88s/it]\u001b[A\n",
      "Current eval Loss 0.136:  68%|██████▊   | 34/50 [01:02<00:30,  1.90s/it]\u001b[A\n",
      "Current eval Loss 0.067:  68%|██████▊   | 34/50 [01:04<00:30,  1.90s/it]\u001b[A\n",
      "Current eval Loss 0.067:  70%|███████   | 35/50 [01:04<00:29,  1.94s/it]\u001b[A\n",
      "Current eval Loss 0.057:  70%|███████   | 35/50 [01:06<00:29,  1.94s/it]\u001b[A\n",
      "Current eval Loss 0.057:  72%|███████▏  | 36/50 [01:06<00:28,  2.05s/it]\u001b[A\n",
      "Current eval Loss 0.352:  72%|███████▏  | 36/50 [01:08<00:28,  2.05s/it]\u001b[A\n",
      "Current eval Loss 0.352:  74%|███████▍  | 37/50 [01:08<00:26,  2.05s/it]\u001b[A\n",
      "Current eval Loss 0.148:  74%|███████▍  | 37/50 [01:10<00:26,  2.05s/it]\u001b[A\n",
      "Current eval Loss 0.148:  76%|███████▌  | 38/50 [01:10<00:23,  1.96s/it]\u001b[A\n",
      "Current eval Loss 0.251:  76%|███████▌  | 38/50 [01:12<00:23,  1.96s/it]\u001b[A\n",
      "Current eval Loss 0.251:  78%|███████▊  | 39/50 [01:12<00:21,  1.94s/it]\u001b[A\n",
      "Current eval Loss 0.069:  78%|███████▊  | 39/50 [01:13<00:21,  1.94s/it]\u001b[A\n",
      "Current eval Loss 0.069:  80%|████████  | 40/50 [01:13<00:18,  1.89s/it]\u001b[A\n",
      "Current eval Loss 0.103:  80%|████████  | 40/50 [01:15<00:18,  1.89s/it]\u001b[A\n",
      "Current eval Loss 0.103:  82%|████████▏ | 41/50 [01:15<00:17,  1.89s/it]\u001b[A\n",
      "Current eval Loss 0.074:  82%|████████▏ | 41/50 [01:17<00:17,  1.89s/it]\u001b[A\n",
      "Current eval Loss 0.074:  84%|████████▍ | 42/50 [01:17<00:14,  1.82s/it]\u001b[A\n",
      "Current eval Loss 0.1:  84%|████████▍ | 42/50 [01:19<00:14,  1.82s/it]  \u001b[A\n",
      "Current eval Loss 0.1:  86%|████████▌ | 43/50 [01:19<00:12,  1.79s/it]\u001b[A\n",
      "Current eval Loss 0.151:  86%|████████▌ | 43/50 [01:20<00:12,  1.79s/it]\u001b[A\n",
      "Current eval Loss 0.151:  88%|████████▊ | 44/50 [01:20<00:10,  1.73s/it]\u001b[A\n",
      "Current eval Loss 0.511:  88%|████████▊ | 44/50 [01:22<00:10,  1.73s/it]\u001b[A\n",
      "Current eval Loss 0.511:  90%|█████████ | 45/50 [01:22<00:08,  1.73s/it]\u001b[A\n",
      "Current eval Loss 0.393:  90%|█████████ | 45/50 [01:24<00:08,  1.73s/it]\u001b[A\n",
      "Current eval Loss 0.393:  92%|█████████▏| 46/50 [01:24<00:06,  1.73s/it]\u001b[A\n",
      "Current eval Loss 0.149:  92%|█████████▏| 46/50 [01:25<00:06,  1.73s/it]\u001b[A\n",
      "Current eval Loss 0.149:  94%|█████████▍| 47/50 [01:25<00:05,  1.70s/it]\u001b[A\n",
      "Current eval Loss 0.113:  94%|█████████▍| 47/50 [01:27<00:05,  1.70s/it]\u001b[A\n",
      "Current eval Loss 0.113:  96%|█████████▌| 48/50 [01:27<00:03,  1.68s/it]\u001b[A\n",
      "Current eval Loss 0.117:  96%|█████████▌| 48/50 [01:29<00:03,  1.68s/it]\u001b[A\n",
      "Current eval Loss 0.117:  98%|█████████▊| 49/50 [01:29<00:01,  1.71s/it]\u001b[A\n",
      "Current eval Loss 0.225:  98%|█████████▊| 49/50 [01:30<00:01,  1.71s/it]\u001b[A\n",
      "Current eval Loss 0.225: 100%|██████████| 50/50 [01:30<00:00,  1.82s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.584:   0%|          | 0/13 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.584:   8%|▊         | 1/13 [00:01<00:18,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.092:   8%|▊         | 1/13 [00:03<00:18,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.092:  15%|█▌        | 2/13 [00:03<00:17,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.536:  15%|█▌        | 2/13 [00:04<00:17,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.536:  23%|██▎       | 3/13 [00:04<00:16,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.419:  23%|██▎       | 3/13 [00:06<00:16,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.419:  31%|███       | 4/13 [00:06<00:14,  1.64s/it]\u001b[A\n",
      "Current eval Loss 0.136:  31%|███       | 4/13 [00:08<00:14,  1.64s/it]\u001b[A\n",
      "Current eval Loss 0.136:  38%|███▊      | 5/13 [00:08<00:13,  1.63s/it]\u001b[A\n",
      "Current eval Loss 0.34:  38%|███▊      | 5/13 [00:09<00:13,  1.63s/it] \u001b[A\n",
      "Current eval Loss 0.34:  46%|████▌     | 6/13 [00:09<00:11,  1.61s/it]\u001b[A\n",
      "Current eval Loss 0.612:  46%|████▌     | 6/13 [00:11<00:11,  1.61s/it]\u001b[A\n",
      "Current eval Loss 0.612:  54%|█████▍    | 7/13 [00:11<00:09,  1.61s/it]\u001b[A\n",
      "Current eval Loss 0.798:  54%|█████▍    | 7/13 [00:12<00:09,  1.61s/it]\u001b[A\n",
      "Current eval Loss 0.798:  62%|██████▏   | 8/13 [00:12<00:08,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.686:  62%|██████▏   | 8/13 [00:14<00:08,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.686:  69%|██████▉   | 9/13 [00:14<00:06,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.673:  69%|██████▉   | 9/13 [00:16<00:06,  1.60s/it]\u001b[A\n",
      "Current eval Loss 0.673:  77%|███████▋  | 10/13 [00:16<00:04,  1.62s/it]\u001b[A\n",
      "Current eval Loss 0.305:  77%|███████▋  | 10/13 [00:17<00:04,  1.62s/it]\u001b[A\n",
      "Current eval Loss 0.305:  85%|████████▍ | 11/13 [00:17<00:03,  1.63s/it]\u001b[A\n",
      "Current eval Loss 0.516:  85%|████████▍ | 11/13 [00:19<00:03,  1.63s/it]\u001b[A\n",
      "Current eval Loss 0.516:  92%|█████████▏| 12/13 [00:19<00:01,  1.62s/it]\u001b[A\n",
      "Current eval Loss 0.448:  92%|█████████▏| 12/13 [00:20<00:01,  1.62s/it]\u001b[A\n",
      "Current eval Loss 0.448: 100%|██████████| 13/13 [00:20<00:00,  1.56s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.236 Train metric = 0.697 Val loss = 0.473 Val metric = 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/13 [00:01<00:19,  1.65s/it]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:03<00:18,  1.66s/it]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:05<00:16,  1.69s/it]\u001b[A\n",
      " 31%|███       | 4/13 [00:07<00:15,  1.78s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:08<00:14,  1.77s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:10<00:12,  1.81s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:12<00:10,  1.79s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:14<00:08,  1.76s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:15<00:06,  1.72s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:17<00:05,  1.71s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:19<00:03,  1.69s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:20<00:01,  1.67s/it]\u001b[A\n",
      "100%|██████████| 13/13 [00:21<00:00,  1.66s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "if args.use_torch_trainer:\n",
    "    device = torch.device(\"cuda\" if _torch_gpu_available and args.use_gpu else \"cpu\")\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        device=xm.xla_device()\n",
    "\n",
    "    print (\"Device: {}\".format(device))\n",
    "    \n",
    "    if args.use_TPU and _torch_tpu_available and args.num_tpus > 1:\n",
    "        train_data_loader = torch_xla.distributed.parallel_loader.ParallelLoader(train_data_loader, [device])\n",
    "        train_data_loader = train_data_loader.per_device_loader(device)\n",
    "\n",
    "\n",
    "    trainer = BasicTrainer(model, train_data_loader, val_data_loader, device, args.transformer_model_pretrained_path, \\\n",
    "                               final_activation=final_activation, \\\n",
    "                               test_data_loader=val_data_loader)\n",
    "\n",
    "    param_optimizer = list(trainer.model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr*xm.xrt_world_size())\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr)\n",
    "\n",
    "    if args.use_apex and _has_apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape) \n",
    "    \n",
    "    def _mp_fn(rank, flags, trainer, epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        a = trainer.train(epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed)\n",
    "\n",
    "    FLAGS = {}\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS, trainer, args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus, \\\n",
    "                 1, 3, False, args.use_apex, False, args.seed), nprocs=8, start_method='fork')\n",
    "    else:\n",
    "        use_wandb = _has_wandb and args.wandb_logging\n",
    "        trainer.train(args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus,  \\\n",
    "                max_grad_norm=1, early_stopping_rounds=3, snapshot_ensemble=False, is_amp=args.use_apex, use_wandb=use_wandb, seed=args.seed)\n",
    "\n",
    "elif args.use_lightning_trainer and _torch_lightning_available:\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.PLMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "    \n",
    "    log_args = {'description': args.transformer_model_pretrained_path, 'loss': loss.__class__.__name__, 'epochs': args.epochs, 'learning_rate': args.lr}\n",
    "\n",
    "    if _has_wandb and not _torch_tpu_available and args.wandb_logging:\n",
    "        wandb.init(project=\"Project\",config=log_args)\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filepath=args.model_save_path,\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "               verbose=False,\n",
    "               mode='min'\n",
    "               )\n",
    "\n",
    "    if args.use_gpu and _torch_gpu_available:\n",
    "        print (\"using GPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    elif args.use_TPU and _torch_tpu_available:\n",
    "        print (\"using TPU\")\n",
    "        if _has_apex:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    else:\n",
    "        print (\"using CPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    pltrainer = PLTrainer(num_train_steps, model, scorer, loss, args.lr, \\\n",
    "                          final_activation=final_activation, seed=42)\n",
    "\n",
    "    #try:\n",
    "    #    print (\"Loaded model from previous checkpoint\")\n",
    "    #    pltrainer = PLTrainer.load_from_checkpoint(args.model_save_path)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    trainer.fit(pltrainer, train_data_loader, val_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output1 = trainer.test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pytorch Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Logging: False, GPU: False, Pytorch Lightning: True, TPU: False, Apex: False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog='Torch trainer function',conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/snli_1.0/snli_1.0_train.jsonl', required=False,\n",
    "                    help='train data')\n",
    "parser.add_argument('--val_data', type=str, default='', required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--task_type', type=str, default='multiclass_sequence_classification', required=False,\n",
    "                    help='type of task')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='textattack/roberta-base-MNLI', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='textattack/roberta-base-MNLI', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='textattack/roberta-base-MNLI', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "parser.add_argument('--bpe_vocab_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding vocab file path')\n",
    "parser.add_argument('--bpe_merges_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding merges file path')\n",
    "parser.add_argument('--berttweettokenizer_path', type=str, default='', required=False,\n",
    "                    help='BERTweet tokenizer path')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=128, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--epochs', type=int, default=2, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.00003, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--loss_function', type=str, default='ce', required=False,\n",
    "                    help='loss function')\n",
    "parser.add_argument('--metric', type=str, default='f1_macro', required=False,\n",
    "                    help='scorer metric')\n",
    "\n",
    "parser.add_argument('--use_lightning_trainer', type=bool, default=True, required=False,\n",
    "                    help='if lightning trainer needs to be used')\n",
    "parser.add_argument('--use_torch_trainer', type=bool, default=False, required=False,\n",
    "                    help='if custom torch trainer needs to be used')\n",
    "parser.add_argument('--use_apex', type=bool, default=False, required=False,\n",
    "                    help='if apex needs to be used')\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, required=False,\n",
    "                    help='GPU mode')\n",
    "parser.add_argument('--use_TPU', type=bool, default=False, required=False,\n",
    "                    help='TPU mode')\n",
    "parser.add_argument('--num_gpus', type=int, default=0, required=False,\n",
    "                    help='Number of GPUs')\n",
    "parser.add_argument('--num_tpus', type=int, default=0, required=False,\n",
    "                    help='Number of TPUs')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=16, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/nli/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=False, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print (\"Wandb Logging: {}, GPU: {}, Pytorch Lightning: {}, TPU: {}, Apex: {}\".format(\\\n",
    "            _has_wandb and args.wandb_logging, _torch_gpu_available,\\\n",
    "            _torch_lightning_available and args.use_lightning_trainer, _torch_tpu_available, _has_apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Checkpoint directory ../models/nli/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "I0806 20:58:47.524595 4680543680 distributed.py:29] GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0806 20:58:47.533794 4680543680 distributed.py:29] TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU\n",
      "[LOG] Total number of parameters to learn 124648708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | model  | TransformerModel | 124 M \n",
      "1 | metric | PLMetric         | 0     \n",
      "I0806 20:58:48.175644 4680543680 lightning.py:1495] \n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | model  | TransformerModel | 124 M \n",
      "1 | metric | PLMetric         | 0     \n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.338 val metric = 0.939 \n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b3dd505a5f4f09931429a61c2d30f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: val_loss reached 0.53188 (best 0.53188), saving model to ../models/nli/epoch=0.ckpt as top 1\n",
      "I0806 21:04:50.426769 4680543680 model_checkpoint.py:346] \n",
      "Epoch 00000: val_loss reached 0.53188 (best 0.53188), saving model to ../models/nli/epoch=0.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.532 val metric = 0.785 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.413 Train metric = 0.841\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss  was not in top 1\n",
      "I0806 21:10:30.835139 4680543680 model_checkpoint.py:314] \n",
      "Epoch 00001: val_loss  was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.645 val metric = 0.792 \n",
      "Train loss = 0.203 Train metric = 0.922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.use_torch_trainer:\n",
    "    device = torch.device(\"cuda\" if _torch_gpu_available and args.use_gpu else \"cpu\")\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        device=xm.xla_device()\n",
    "\n",
    "    print (\"Device: {}\".format(device))\n",
    "    \n",
    "    if args.use_TPU and _torch_tpu_available and args.num_tpus > 1:\n",
    "        train_data_loader = torch_xla.distributed.parallel_loader.ParallelLoader(train_data_loader, [device])\n",
    "        train_data_loader = train_data_loader.per_device_loader(device)\n",
    "\n",
    "\n",
    "    trainer = BasicTrainer(model, train_data_loader, val_data_loader, device, args.transformer_model_pretrained_path, \\\n",
    "                               final_activation=final_activation, \\\n",
    "                               test_data_loader=val_data_loader)\n",
    "\n",
    "    param_optimizer = list(trainer.model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr*xm.xrt_world_size())\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr)\n",
    "\n",
    "    if args.use_apex and _has_apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape) \n",
    "    \n",
    "    def _mp_fn(rank, flags, trainer, epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        a = trainer.train(epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed)\n",
    "\n",
    "    FLAGS = {}\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS, trainer, args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus, \\\n",
    "                 1, 3, False, args.use_apex, False, args.seed), nprocs=8, start_method='fork')\n",
    "    else:\n",
    "        use_wandb = _has_wandb and args.wandb_logging\n",
    "        trainer.train(args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus,  \\\n",
    "                max_grad_norm=1, early_stopping_rounds=3, snapshot_ensemble=False, is_amp=args.use_apex, use_wandb=use_wandb, seed=args.seed)\n",
    "\n",
    "elif args.use_lightning_trainer and _torch_lightning_available:\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.PLMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "    \n",
    "    log_args = {'description': args.transformer_model_pretrained_path, 'loss': loss.__class__.__name__, 'epochs': args.epochs, 'learning_rate': args.lr}\n",
    "\n",
    "    if _has_wandb and not _torch_tpu_available and args.wandb_logging:\n",
    "        wandb.init(project=\"Project\",config=log_args)\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filepath=args.model_save_path,\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "               verbose=False,\n",
    "               mode='min'\n",
    "               )\n",
    "\n",
    "    if args.use_gpu and _torch_gpu_available:\n",
    "        print (\"using GPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    elif args.use_TPU and _torch_tpu_available:\n",
    "        print (\"using TPU\")\n",
    "        if _has_apex:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    else:\n",
    "        print (\"using CPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    pltrainer = PLTrainer(num_train_steps, model, scorer, loss, args.lr, \\\n",
    "                          final_activation=final_activation, seed=42)\n",
    "\n",
    "    #try:\n",
    "    #    print (\"Loaded model from previous checkpoint\")\n",
    "    #    pltrainer = PLTrainer.load_from_checkpoint(args.model_save_path)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    trainer.fit(pltrainer, train_data_loader, val_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/13 [00:03<00:44,  3.67s/it]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:06<00:36,  3.30s/it]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:08<00:31,  3.11s/it]\u001b[A\n",
      " 31%|███       | 4/13 [00:11<00:26,  2.91s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:13<00:22,  2.75s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:15<00:18,  2.62s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:18<00:15,  2.55s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:20<00:12,  2.48s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:22<00:09,  2.44s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:25<00:07,  2.41s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:27<00:04,  2.41s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:30<00:02,  2.39s/it]\u001b[A\n",
      "100%|██████████| 13/13 [00:31<00:00,  2.40s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_output2 = []\n",
    "\n",
    "for val_batch in tqdm(val_data_loader):\n",
    "    out = pltrainer(val_batch).detach().cpu().numpy()\n",
    "    test_output2.extend(out.tolist())\n",
    "    \n",
    "#test_output2 = np.concatenate(test_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output1 = np.array(test_output1).argmax(-1)\n",
    "test_output2 = np.array(test_output2).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'neutral', 2: 'contradiction', 3: '-'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label = {i:w for (w,i) in label2idx.items()}\n",
    "idx2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>premise_transitions</th>\n",
       "      <th>hypothesis_transitions</th>\n",
       "      <th>prediction1</th>\n",
       "      <th>prediction2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>entailment</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>The kids are frowning</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
       "      <td>The boy skates down the sidewalk.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
       "      <td>The boy does a skateboarding trick.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
       "      <td>The boy is wearing safety equipment.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>An older man sits with his orange juice at a s...</td>\n",
       "      <td>An older man drinks his juice as he waits for ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>[shift, shift, shift, shift, shift, shift, shi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "5              Children smiling and waving at camera   \n",
       "6  A boy is jumping on skateboard in the middle o...   \n",
       "7  A boy is jumping on skateboard in the middle o...   \n",
       "8  A boy is jumping on skateboard in the middle o...   \n",
       "9  An older man sits with his orange juice at a s...   \n",
       "\n",
       "                                          hypothesis          label  \\\n",
       "0  A person is training his horse for a competition.        neutral   \n",
       "1      A person is at a diner, ordering an omelette.  contradiction   \n",
       "2                  A person is outdoors, on a horse.     entailment   \n",
       "3                  They are smiling at their parents        neutral   \n",
       "4                         There are children present     entailment   \n",
       "5                              The kids are frowning  contradiction   \n",
       "6                  The boy skates down the sidewalk.  contradiction   \n",
       "7                The boy does a skateboarding trick.     entailment   \n",
       "8               The boy is wearing safety equipment.        neutral   \n",
       "9  An older man drinks his juice as he waits for ...        neutral   \n",
       "\n",
       "                                 premise_transitions  \\\n",
       "0  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "1  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "2  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "3  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "4  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "5  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "6  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "7  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "8  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "9  [shift, shift, shift, shift, shift, shift, shi...   \n",
       "\n",
       "                              hypothesis_transitions    prediction1  \\\n",
       "0  [shift, shift, shift, shift, shift, shift, shi...        neutral   \n",
       "1  [shift, shift, shift, shift, shift, shift, shi...  contradiction   \n",
       "2  [shift, shift, shift, shift, shift, shift, shi...     entailment   \n",
       "3  [shift, shift, shift, shift, shift, shift, shi...        neutral   \n",
       "4  [shift, shift, shift, shift, shift, shift, shi...     entailment   \n",
       "5  [shift, shift, shift, shift, shift, shift, shi...  contradiction   \n",
       "6  [shift, shift, shift, shift, shift, shift, shi...  contradiction   \n",
       "7  [shift, shift, shift, shift, shift, shift, shi...     entailment   \n",
       "8  [shift, shift, shift, shift, shift, shift, shi...        neutral   \n",
       "9  [shift, shift, shift, shift, shift, shift, shi...        neutral   \n",
       "\n",
       "     prediction2  \n",
       "0        neutral  \n",
       "1  contradiction  \n",
       "2     entailment  \n",
       "3        neutral  \n",
       "4     entailment  \n",
       "5  contradiction  \n",
       "6  contradiction  \n",
       "7     entailment  \n",
       "8        neutral  \n",
       "9        neutral  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['prediction1'] = [idx2label[i] for i in test_output1]\n",
    "val_df['prediction2'] = [idx2label[i] for i in test_output2]\n",
    "val_df['label'] = [idx2label[val_df.label.iloc[i]] for i in range(val_df.shape[0])]\n",
    "val_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00         1\n",
      "contradiction       0.88      0.91      0.89        65\n",
      "   entailment       0.81      0.91      0.86        67\n",
      "      neutral       0.83      0.72      0.77        67\n",
      "\n",
      "     accuracy                           0.84       200\n",
      "    macro avg       0.63      0.63      0.63       200\n",
      " weighted avg       0.84      0.84      0.84       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(val_df.label, val_df.prediction1)\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00         1\n",
      "contradiction       0.78      0.91      0.84        65\n",
      "   entailment       0.83      0.79      0.81        67\n",
      "      neutral       0.77      0.69      0.72        67\n",
      "\n",
      "     accuracy                           0.79       200\n",
      "    macro avg       0.59      0.60      0.59       200\n",
      " weighted avg       0.79      0.79      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(val_df.label, val_df.prediction2)\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
