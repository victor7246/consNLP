{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 13:03:47.291222 4570049984 file_utils.py:41] PyTorch version 1.5.0 available.\n",
      "I0806 13:03:53.762058 4570049984 file_utils.py:57] TensorFlow version 2.2.0-rc3 available.\n",
      "I0806 13:03:56.373564 4570049984 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "I0806 13:03:58.931196 4570049984 textcleaner.py:37] 'pattern' package not found; tag filters are not available for English\n",
      "W0806 13:03:59.205211 4570049984 deprecation.py:323] From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from dotenv import find_dotenv, load_dotenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '../src'))\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.getcwd(), '../src'))\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcontrib.optim import SWA\n",
    "from torch.optim import Adam, SGD \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CyclicLR, \\\n",
    "                                     CosineAnnealingWarmRestarts\n",
    "\n",
    "from consNLP.data import load_data, data_utils, fetch_dataset\n",
    "from consNLP.models import transformer_models, activations, layers, losses, scorers\n",
    "from consNLP.visualization import visualize\n",
    "from consNLP.trainer.trainer import BasicTrainer, PLTrainer, test_pl_trainer\n",
    "from consNLP.trainer.trainer_utils import set_seed, _has_apex, _torch_lightning_available, _has_wandb, _torch_gpu_available, _num_gpus, _torch_tpu_available\n",
    "from consNLP.preprocessing.custom_tokenizer import BERTweetTokenizer\n",
    "\n",
    "if _has_apex:\n",
    "    #from torch.cuda import amp\n",
    "    from apex import amp\n",
    "\n",
    "if _torch_tpu_available:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "if _has_wandb:\n",
    "    import wandb\n",
    "    try:\n",
    "        load_dotenv(find_dotenv())\n",
    "        wandb.login(key=os.environ['WANDB_API_KEY'])\n",
    "    except:\n",
    "        _has_wandb = False\n",
    "\n",
    "if _torch_lightning_available:\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    from pytorch_lightning.loggers import WandbLogger\n",
    "    from pytorch_lightning.metrics.metric import NumpyMetric\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "\n",
    "import tokenizers\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 13:03:59.275779 4570049984 fetch_dataset.py:16] making final data set from raw data\n",
      "I0806 13:03:59.276736 4570049984 fetch_dataset.py:21] project directory ../\n",
      "I0806 13:03:59.277626 4570049984 fetch_dataset.py:30] output path ../data/raw\n",
      "I0806 13:04:05.523024 4570049984 fetch_dataset.py:95] download complete\n"
     ]
    }
   ],
   "source": [
    "fetch_dataset(project_dir='../',download_from_kaggle=True,\\\n",
    "              kaggle_dataset='abhinavwalia95/entity-annotated-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Logging: False, GPU: False, Pytorch Lightning: False, TPU: False, Apex: False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog='Torch trainer function',conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/ner_dataset.csv', required=False,\n",
    "                    help='train data')\n",
    "parser.add_argument('--val_data', type=str, default='', required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--task_type', type=str, default='multiclass_token_classification', required=False,\n",
    "                    help='type of task')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "parser.add_argument('--bpe_vocab_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding vocab file path')\n",
    "parser.add_argument('--bpe_merges_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding merges file path')\n",
    "parser.add_argument('--berttweettokenizer_path', type=str, default='', required=False,\n",
    "                    help='BERTweet tokenizer path')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=128, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--epochs', type=int, default=5, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.00003, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--loss_function', type=str, default='ce', required=False,\n",
    "                    help='loss function')\n",
    "parser.add_argument('--metric', type=str, default='f1_macro', required=False,\n",
    "                    help='scorer metric')\n",
    "\n",
    "parser.add_argument('--use_lightning_trainer', type=bool, default=False, required=False,\n",
    "                    help='if lightning trainer needs to be used')\n",
    "parser.add_argument('--use_torch_trainer', type=bool, default=True, required=False,\n",
    "                    help='if custom torch trainer needs to be used')\n",
    "parser.add_argument('--use_apex', type=bool, default=False, required=False,\n",
    "                    help='if apex needs to be used')\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, required=False,\n",
    "                    help='GPU mode')\n",
    "parser.add_argument('--use_TPU', type=bool, default=False, required=False,\n",
    "                    help='TPU mode')\n",
    "parser.add_argument('--num_gpus', type=int, default=0, required=False,\n",
    "                    help='Number of GPUs')\n",
    "parser.add_argument('--num_tpus', type=int, default=0, required=False,\n",
    "                    help='Number of TPUs')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=32, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=8, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/ner/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=False, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print (\"Wandb Logging: {}, GPU: {}, Pytorch Lightning: {}, TPU: {}, Apex: {}\".format(\\\n",
    "            _has_wandb and args.wandb_logging, _torch_gpu_available,\\\n",
    "            _torch_lightning_available and args.use_lightning_trainer, _torch_tpu_available, _has_apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = False\n",
    "final_activation = None\n",
    "convert_output = None\n",
    "\n",
    "if args.task_type == 'binary_sequence_classification':\n",
    "    if args.metric != 'roc_auc_score': \n",
    "        convert_output = 'round'\n",
    "    if args.loss_function == 'bcelogit':\n",
    "        final_activation = 'sigmoid'\n",
    "        \n",
    "elif args.task_type == 'multiclass_sequence_classification':\n",
    "    convert_output = 'max'\n",
    "    \n",
    "elif args.task_type == 'binary_token_classification':\n",
    "    reshape = True\n",
    "    if args.metric != 'roc_auc_score': \n",
    "        convert_output = 'round'\n",
    "    if args.loss_function == 'bcelogit':\n",
    "        final_activation = 'sigmoid'\n",
    "        \n",
    "elif args.task_type == 'multiclass_token_classification':\n",
    "    reshape = True\n",
    "    convert_output = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data.load_pandas_df(args.train_data,sep=',',encoding='latin-1', \\\n",
    "                                      text_column=['Word'], target_column=['Tag'])\n",
    "#df = df.iloc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>words</th>\n",
       "      <th>POS</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #          words  POS labels\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1          NaN             of   IN      O\n",
       "2          NaN  demonstrators  NNS      O\n",
       "3          NaN           have  VBP      O\n",
       "4          NaN        marched  VBN      O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\n",
       "       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\n",
       "       'I-eve', 'I-nat'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = args.model_save_path\n",
    "try:\n",
    "    os.makedirs(model_save_dir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47959, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>[Police, put, the, number, of, marchers, at, 1...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                              words  \\\n",
       "0  Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1  Sentence: 2  [Families, of, soldiers, killed, in, the, conf...   \n",
       "2  Sentence: 3  [They, marched, from, the, Houses, of, Parliam...   \n",
       "3  Sentence: 4  [Police, put, the, number, of, marchers, at, 1...   \n",
       "4  Sentence: 5  [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...  \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence = df.groupby(['Sentence #'], sort=False)['words', 'labels'].agg(lambda x: list(x)).reset_index(drop=False)\n",
    "print (df_sentence.shape)\n",
    "df_sentence = df_sentence.iloc[:1000]\n",
    "df_sentence.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence.labels, label2idx = data_utils.convert_categorical_label_to_int(df_sentence.labels, \\\n",
    "                                                             save_path=os.path.join(model_save_dir,'label2idx.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 9, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>[Police, put, the, number, of, marchers, at, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                              words  \\\n",
       "0  Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1  Sentence: 2  [Families, of, soldiers, killed, in, the, conf...   \n",
       "2  Sentence: 3  [They, marched, from, the, Houses, of, Parliam...   \n",
       "3  Sentence: 4  [Police, put, the, number, of, marchers, at, 1...   \n",
       "4  Sentence: 5  [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                              labels  \n",
       "0  [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 9, 1]  \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 4, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    47959.000000\n",
       "mean        21.863988\n",
       "std          7.963680\n",
       "min          1.000000\n",
       "25%         16.000000\n",
       "50%         21.000000\n",
       "75%         27.000000\n",
       "max        104.000000\n",
       "Name: words, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Sentence #'])['words'].count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'B-geo': 2,\n",
       " 'B-gpe': 3,\n",
       " 'B-org': 4,\n",
       " 'I-per': 5,\n",
       " 'B-tim': 6,\n",
       " 'B-per': 7,\n",
       " 'I-org': 8,\n",
       " 'I-geo': 9,\n",
       " 'I-tim': 10,\n",
       " 'B-art': 11,\n",
       " 'I-gpe': 12,\n",
       " 'I-art': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'B-nat': 16,\n",
       " 'I-nat': 17}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(5)\n",
    "\n",
    "for train_index, val_index in kf.split(df_sentence.words, df_sentence.labels):\n",
    "    break\n",
    "    \n",
    "train_df = df_sentence.iloc[train_index].reset_index(drop=True)\n",
    "val_df = df_sentence.iloc[val_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 3), (200, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 13:05:55.144843 4570049984 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0806 13:05:55.145660 4570049984 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0806 13:05:57.427182 4570049984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /Users/victor/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0806 13:05:57.427954 4570049984 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /Users/victor/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "if args.berttweettokenizer_path:\n",
    "    tokenizer = BERTweetTokenizer(args.berttweettokenizer_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.transformer_model_pretrained_path)\n",
    "\n",
    "if not args.berttweettokenizer_path:\n",
    "    try:\n",
    "        bpetokenizer = tokenizers.ByteLevelBPETokenizer(args.bpe_vocab_path, \\\n",
    "                                        args.bpe_merges_path)\n",
    "    except:\n",
    "        bpetokenizer = None \n",
    "else:\n",
    "    bpetokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_utils.TransformerDataset(train_df.words, bpetokenizer=bpetokenizer, tokenizer=tokenizer, MAX_LEN=args.max_text_len, \\\n",
    "              target_label=train_df.labels, n_target_class=len(label2idx), convert=True, sequence_target=True)\n",
    "\n",
    "val_dataset = data_utils.TransformerDataset(val_df.words, bpetokenizer=bpetokenizer, tokenizer=tokenizer, MAX_LEN=args.max_text_len, \\\n",
    "              target_label=val_df.labels, n_target_class=len(label2idx), convert=True, sequence_target=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'B-geo': 2,\n",
       " 'B-gpe': 3,\n",
       " 'B-org': 4,\n",
       " 'I-per': 5,\n",
       " 'B-tim': 6,\n",
       " 'B-per': 7,\n",
       " 'I-org': 8,\n",
       " 'I-geo': 9,\n",
       " 'I-tim': 10,\n",
       " 'B-art': 11,\n",
       " 'I-gpe': 12,\n",
       " 'I-art': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'B-nat': 16,\n",
       " 'I-nat': 17}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, base_model, dropout=.3, n_out=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.base_model = base_model\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(base_model.config.hidden_size, n_out)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        o2 = self.base_model(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        o2 = o2[0]\n",
    "        bo = self.drop(o2)\n",
    "        output = self.out(bo)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 13:06:54.836287 4570049984 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0806 13:06:54.837488 4570049984 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0806 13:06:56.041882 4570049984 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /Users/victor/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(args.transformer_config_path, output_hidden_states=True, output_attentions=True)\n",
    "basemodel = AutoModel.from_pretrained(args.transformer_model_pretrained_path,config=config)\n",
    "model = TransformerModel(basemodel, n_out = len(label2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      train_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True\n",
    "    )\n",
    "\n",
    "    val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      val_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, sampler=train_sampler,\n",
    "        drop_last=True,num_workers=2)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size, sampler=val_sampler,\n",
    "        drop_last=False,num_workers=1)\n",
    "else:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 35779,    11,  ...,     1,     1,     1],\n",
      "        [    0,   133,   665,  ...,     1,     1,     1],\n",
      "        [    0, 35779,   224,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 24270,  1443,  ...,     1,     1,     1],\n",
      "        [    0,   250,  6169,  ...,     1,     1,     1],\n",
      "        [    0, 35779,   224,  ...,     1,     1,     1]]) tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
      "        [0, 1, 1,  ..., 0, 0, 0],\n",
      "        [0, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 3, 1,  ..., 0, 0, 0],\n",
      "        [0, 1, 4,  ..., 0, 0, 0],\n",
      "        [0, 1, 1,  ..., 0, 0, 0]])\n",
      "torch.Size([32, 128, 18])\n",
      "tensor(3.2894, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0909, grad_fn=<NllLossBackward>)\n",
      "0.0023696628022900346\n"
     ]
    }
   ],
   "source": [
    "for d in train_data_loader:\n",
    "    break\n",
    "\n",
    "print (d['ids'], d['targets'])\n",
    "out = model(d['ids'],d['mask'],d['token_type_ids'])\n",
    "print (out.shape)\n",
    "\n",
    "loss1 = losses.get_loss(args.loss_function)\n",
    "print (loss1(out,d['targets']))\n",
    "\n",
    "loss2 = losses.get_loss('masked_ce')\n",
    "print (loss2(out,d['targets'],d['mask']))\n",
    "\n",
    "metric = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "print (metric(d['targets'].detach().cpu().numpy(),out.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pytorch Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "[LOG] Total number of parameters to learn 124659474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Current training Loss 0.238: 100%|██████████| 25/25 [06:58<00:00, 16.76s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.228: 100%|██████████| 25/25 [01:25<00:00,  3.42s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.217: 100%|██████████| 25/25 [00:22<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.224 Train metric = 0.103 Val loss = 0.216 Val metric = 0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "Current training Loss 0.199: 100%|██████████| 25/25 [06:08<00:00, 14.75s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.188: 100%|██████████| 25/25 [01:21<00:00,  3.24s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.171: 100%|██████████| 25/25 [00:20<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.181 Train metric = 0.103 Val loss = 0.171 Val metric = 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current training Loss 0.182: 100%|██████████| 25/25 [06:07<00:00, 14.69s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.173: 100%|██████████| 25/25 [01:21<00:00,  3.26s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.155: 100%|██████████| 25/25 [00:20<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.166 Train metric = 0.103 Val loss = 0.157 Val metric = 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current training Loss 0.17: 100%|██████████| 25/25 [06:11<00:00, 14.87s/it] \n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.159: 100%|██████████| 25/25 [01:20<00:00,  3.21s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.141: 100%|██████████| 25/25 [00:21<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.152 Train metric = 0.104 Val loss = 0.145 Val metric = 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current training Loss 0.163: 100%|██████████| 25/25 [06:18<00:00, 15.14s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.155: 100%|██████████| 25/25 [01:21<00:00,  3.25s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current eval Loss 0.137: 100%|██████████| 25/25 [00:23<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.148 Train metric = 0.104 Val loss = 0.142 Val metric = 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:22<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "if args.use_torch_trainer:\n",
    "    device = torch.device(\"cuda\" if _torch_gpu_available and args.use_gpu else \"cpu\")\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        device=xm.xla_device()\n",
    "\n",
    "    print (\"Device: {}\".format(device))\n",
    "    \n",
    "    if args.use_TPU and _torch_tpu_available and args.num_tpus > 1:\n",
    "        train_data_loader = torch_xla.distributed.parallel_loader.ParallelLoader(train_data_loader, [device])\n",
    "        train_data_loader = train_data_loader.per_device_loader(device)\n",
    "\n",
    "\n",
    "    trainer = BasicTrainer(model, train_data_loader, val_data_loader, device, args.transformer_model_pretrained_path, \\\n",
    "                               final_activation=final_activation, \\\n",
    "                               test_data_loader=val_data_loader)\n",
    "\n",
    "    param_optimizer = list(trainer.model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr*xm.xrt_world_size())\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr)\n",
    "\n",
    "    if args.use_apex and _has_apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape) \n",
    "    \n",
    "    def _mp_fn(rank, flags, trainer, epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        a = trainer.train(epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed)\n",
    "\n",
    "    FLAGS = {}\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS, trainer, args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus, \\\n",
    "                 1, 3, False, args.use_apex, False, args.seed), nprocs=8, start_method='fork')\n",
    "    else:\n",
    "        use_wandb = _has_wandb and args.wandb_logging\n",
    "        trainer.train(args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus,  \\\n",
    "                max_grad_norm=1, early_stopping_rounds=3, snapshot_ensemble=False, is_amp=args.use_apex, use_wandb=use_wandb, seed=args.seed)\n",
    "\n",
    "elif args.use_lightning_trainer and _torch_lightning_available:\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.PLMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "    \n",
    "    log_args = {'description': args.transformer_model_pretrained_path, 'loss': loss.__class__.__name__, 'epochs': args.epochs, 'learning_rate': args.lr}\n",
    "\n",
    "    if _has_wandb and not _torch_tpu_available and args.wandb_logging:\n",
    "        wandb.init(project=\"Project\",config=log_args)\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filepath=args.model_save_path,\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "               verbose=False,\n",
    "               mode='min'\n",
    "               )\n",
    "\n",
    "    if args.use_gpu and _torch_gpu_available:\n",
    "        print (\"using GPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    elif args.use_TPU and _torch_tpu_available:\n",
    "        print (\"using TPU\")\n",
    "        if _has_apex:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    else:\n",
    "        print (\"using CPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    pltrainer = PLTrainer(num_train_steps, model, scorer, loss, args.lr, \\\n",
    "                          final_activation=final_activation, seed=42)\n",
    "\n",
    "    #try:\n",
    "    #    print (\"Loaded model from previous checkpoint\")\n",
    "    #    pltrainer = PLTrainer.load_from_checkpoint(args.model_save_path)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    trainer.fit(pltrainer, train_data_loader, val_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output1 = trainer.test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pytorch Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Logging: False, GPU: False, Pytorch Lightning: True, TPU: False, Apex: False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog='Torch trainer function',conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/ner_dataset.csv', required=False,\n",
    "                    help='train data')\n",
    "parser.add_argument('--val_data', type=str, default='', required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--task_type', type=str, default='multiclass_token_classification', required=False,\n",
    "                    help='type of task')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "parser.add_argument('--bpe_vocab_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding vocab file path')\n",
    "parser.add_argument('--bpe_merges_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding merges file path')\n",
    "parser.add_argument('--berttweettokenizer_path', type=str, default='', required=False,\n",
    "                    help='BERTweet tokenizer path')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=100, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--epochs', type=int, default=2, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.00003, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--loss_function', type=str, default='masked_ce', required=False,\n",
    "                    help='loss function')\n",
    "parser.add_argument('--metric', type=str, default='f1_macro', required=False,\n",
    "                    help='scorer metric')\n",
    "\n",
    "parser.add_argument('--use_lightning_trainer', type=bool, default=True, required=False,\n",
    "                    help='if lightning trainer needs to be used')\n",
    "parser.add_argument('--use_torch_trainer', type=bool, default=False, required=False,\n",
    "                    help='if custom torch trainer needs to be used')\n",
    "parser.add_argument('--use_apex', type=bool, default=False, required=False,\n",
    "                    help='if apex needs to be used')\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, required=False,\n",
    "                    help='GPU mode')\n",
    "parser.add_argument('--use_TPU', type=bool, default=False, required=False,\n",
    "                    help='TPU mode')\n",
    "parser.add_argument('--num_gpus', type=int, default=0, required=False,\n",
    "                    help='Number of GPUs')\n",
    "parser.add_argument('--num_tpus', type=int, default=0, required=False,\n",
    "                    help='Number of TPUs')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=16, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/ner/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=False, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print (\"Wandb Logging: {}, GPU: {}, Pytorch Lightning: {}, TPU: {}, Apex: {}\".format(\\\n",
    "            _has_wandb and args.wandb_logging, _torch_gpu_available,\\\n",
    "            _torch_lightning_available and args.use_lightning_trainer, _torch_tpu_available, _has_apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "I0806 13:59:11.998070 4570049984 distributed.py:29] GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0806 13:59:11.999614 4570049984 distributed.py:29] TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | model   | TransformerModel | 124 M \n",
      "1 | loss_fn | masked_CELoss    | 0     \n",
      "2 | metric  | PLMetric         | 0     \n",
      "I0806 13:59:12.015772 4570049984 lightning.py:1495] \n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | model   | TransformerModel | 124 M \n",
      "1 | loss_fn | masked_CELoss    | 0     \n",
      "2 | metric  | PLMetric         | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU\n",
      "[LOG] Total number of parameters to learn 124659474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.789 val metric = 0.181 \n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fa47cf33654615b1d4da968a6a5802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: val_loss reached 0.58900 (best 0.58900), saving model to ../models/ner/epoch=0.ckpt as top 1\n",
      "I0806 14:06:41.302335 4570049984 model_checkpoint.py:346] \n",
      "Epoch 00000: val_loss reached 0.58900 (best 0.58900), saving model to ../models/ner/epoch=0.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.589 val metric = 0.217 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.71 Train metric = 0.155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss reached 0.51611 (best 0.51611), saving model to ../models/ner/epoch=1.ckpt as top 1\n",
      "I0806 14:13:25.458095 4570049984 model_checkpoint.py:346] \n",
      "Epoch 00001: val_loss reached 0.51611 (best 0.51611), saving model to ../models/ner/epoch=1.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.516 val metric = 0.313 \n",
      "Train loss = 0.591 Train metric = 0.206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.use_torch_trainer:\n",
    "    device = torch.device(\"cuda\" if _torch_gpu_available and args.use_gpu else \"cpu\")\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        device=xm.xla_device()\n",
    "\n",
    "    print (\"Device: {}\".format(device))\n",
    "    \n",
    "    if args.use_TPU and _torch_tpu_available and args.num_tpus > 1:\n",
    "        train_data_loader = torch_xla.distributed.parallel_loader.ParallelLoader(train_data_loader, [device])\n",
    "        train_data_loader = train_data_loader.per_device_loader(device)\n",
    "\n",
    "\n",
    "    trainer = BasicTrainer(model, train_data_loader, val_data_loader, device, args.transformer_model_pretrained_path, \\\n",
    "                               final_activation=final_activation, \\\n",
    "                               test_data_loader=val_data_loader)\n",
    "\n",
    "    param_optimizer = list(trainer.model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr*xm.xrt_world_size())\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr)\n",
    "\n",
    "    if args.use_apex and _has_apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape) \n",
    "    \n",
    "    def _mp_fn(rank, flags, trainer, epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        a = trainer.train(epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed)\n",
    "\n",
    "    FLAGS = {}\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS, trainer, args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus, \\\n",
    "                 1, 3, False, args.use_apex, False, args.seed), nprocs=8, start_method='fork')\n",
    "    else:\n",
    "        use_wandb = _has_wandb and args.wandb_logging\n",
    "        trainer.train(args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus,  \\\n",
    "                max_grad_norm=1, early_stopping_rounds=3, snapshot_ensemble=False, is_amp=args.use_apex, use_wandb=use_wandb, seed=args.seed)\n",
    "\n",
    "elif args.use_lightning_trainer and _torch_lightning_available:\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.PLMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "    \n",
    "    log_args = {'description': args.transformer_model_pretrained_path, 'loss': loss.__class__.__name__, 'epochs': args.epochs, 'learning_rate': args.lr}\n",
    "\n",
    "    if _has_wandb and not _torch_tpu_available and args.wandb_logging:\n",
    "        wandb.init(project=\"Project\",config=log_args)\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filepath=args.model_save_path,\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "               verbose=False,\n",
    "               mode='min'\n",
    "               )\n",
    "\n",
    "    if args.use_gpu and _torch_gpu_available:\n",
    "        print (\"using GPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    elif args.use_TPU and _torch_tpu_available:\n",
    "        print (\"using TPU\")\n",
    "        if _has_apex:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    else:\n",
    "        print (\"using CPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    pltrainer = PLTrainer(num_train_steps, model, scorer, loss, args.lr, \\\n",
    "                          final_activation=final_activation, seed=42)\n",
    "\n",
    "    #try:\n",
    "    #    print (\"Loaded model from previous checkpoint\")\n",
    "    #    pltrainer = PLTrainer.load_from_checkpoint(args.model_save_path)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    trainer.fit(pltrainer, train_data_loader, val_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [00:01<00:38,  1.59s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [00:02<00:34,  1.48s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [00:04<00:30,  1.40s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [00:05<00:27,  1.33s/it]\u001b[A\n",
      " 20%|██        | 5/25 [00:06<00:25,  1.28s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [00:07<00:23,  1.25s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [00:08<00:22,  1.23s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [00:09<00:20,  1.22s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [00:11<00:19,  1.20s/it]\u001b[A\n",
      " 40%|████      | 10/25 [00:12<00:17,  1.19s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [00:13<00:16,  1.19s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [00:14<00:15,  1.18s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [00:15<00:14,  1.18s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [00:16<00:12,  1.18s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [00:18<00:11,  1.17s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [00:19<00:10,  1.17s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [00:20<00:09,  1.17s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [00:21<00:08,  1.17s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [00:22<00:07,  1.20s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [00:24<00:05,  1.20s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [00:25<00:04,  1.19s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [00:26<00:03,  1.18s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [00:27<00:02,  1.18s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [00:28<00:01,  1.18s/it]\u001b[A\n",
      "100%|██████████| 25/25 [00:29<00:00,  1.20s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_output2 = []\n",
    "\n",
    "for val_batch in tqdm(val_data_loader):\n",
    "    out = pltrainer(val_batch).detach().cpu().numpy()\n",
    "    test_output2.extend(out.tolist())\n",
    "    \n",
    "#test_output2 = np.concatenate(test_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 320.45it/s]\n"
     ]
    }
   ],
   "source": [
    "test_target = []\n",
    "\n",
    "for val_batch in tqdm(val_data_loader):\n",
    "    test_target.extend(val_batch['targets'].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output1 = np.array(test_output1).argmax(-1)\n",
    "test_output2 = np.array(test_output2).argmax(-1)\n",
    "test_target = np.array(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 128), (200, 128), (200, 128))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output1.shape, test_output2.shape, test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'O',\n",
       " 2: 'B-geo',\n",
       " 3: 'B-gpe',\n",
       " 4: 'B-org',\n",
       " 5: 'I-per',\n",
       " 6: 'B-tim',\n",
       " 7: 'B-per',\n",
       " 8: 'I-org',\n",
       " 9: 'I-geo',\n",
       " 10: 'I-tim',\n",
       " 11: 'B-art',\n",
       " 12: 'I-gpe',\n",
       " 13: 'I-art',\n",
       " 14: 'B-eve',\n",
       " 15: 'I-eve',\n",
       " 16: 'B-nat',\n",
       " 17: 'I-nat'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label = {i:w for (w,i) in label2idx.items()}\n",
    "idx2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output1 = np.array([[idx2label.get(i,'PAD') for i in x] for x in test_output1])\n",
    "test_output2 = np.array([[idx2label.get(i,'PAD') for i in x] for x in test_output2])\n",
    "test_target = np.array([[idx2label.get(i,'PAD') for i in x] for x in test_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 17429.43it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 18389.21it/s]\n"
     ]
    }
   ],
   "source": [
    "pred1 = []\n",
    "\n",
    "for i in tqdm(range(test_output1.shape[0])):\n",
    "    l = test_output1[i].tolist()[1:]\n",
    "    if len(val_df.labels[i]) > len(l):\n",
    "        pred1.append(l + [1]*(len(val_df.labels[i]) - len(l)))\n",
    "    else:\n",
    "        pred1.append(l[:len(val_df.labels[i])])\n",
    "    \n",
    "val_df['predicted1'] = pred1\n",
    "\n",
    "pred2 = []\n",
    "\n",
    "for i in tqdm(range(test_output2.shape[0])):\n",
    "    l = test_output2[i].tolist()[1:]\n",
    "    if len(val_df.labels[i]) > len(l):\n",
    "        pred2.append(l + [1]*(len(val_df.labels[i]) - len(l)))\n",
    "    else:\n",
    "        pred2.append(l[:len(val_df.labels[i])])\n",
    "    \n",
    "val_df['predicted2'] = pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['labels'] = val_df['labels'].apply(lambda x: np.array([idx2label.get(i,\"PAD\") for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>predicted1</th>\n",
       "      <th>predicted2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, PAD]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, PAD, PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 4</td>\n",
       "      <td>[Police, put, the, number, of, marchers, at, 1...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 5</td>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                              words  \\\n",
       "0  Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1  Sentence: 2  [Families, of, soldiers, killed, in, the, conf...   \n",
       "2  Sentence: 3  [They, marched, from, the, Houses, of, Parliam...   \n",
       "3  Sentence: 4  [Police, put, the, number, of, marchers, at, 1...   \n",
       "4  Sentence: 5  [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...   \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...   \n",
       "\n",
       "                                          predicted1  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2       [O, O, O, O, O, O, O, O, O, O, O, O, O, PAD]   \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                          predicted2  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, PAD, PAD]  \n",
       "3      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        14\n",
      "       B-geo       0.00      0.00      0.00       118\n",
      "       B-gpe       0.00      0.00      0.00       107\n",
      "       B-nat       0.00      0.00      0.00         1\n",
      "       B-org       0.00      0.00      0.00        96\n",
      "       B-per       0.00      0.00      0.00        57\n",
      "       B-tim       0.00      0.00      0.00        55\n",
      "       I-art       0.00      0.00      0.00         6\n",
      "       I-geo       0.00      0.00      0.00        23\n",
      "       I-gpe       0.00      0.00      0.00         5\n",
      "       I-org       0.00      0.00      0.00        68\n",
      "       I-per       0.00      0.00      0.00        71\n",
      "       I-tim       0.00      0.00      0.00         4\n",
      "           O       0.82      0.97      0.89      3807\n",
      "         PAD       0.99      0.99      0.99     21168\n",
      "\n",
      "    accuracy                           0.96     25600\n",
      "   macro avg       0.12      0.13      0.13     25600\n",
      "weighted avg       0.94      0.96      0.95     25600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = flat_classification_report(test_target, test_output1)\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        14\n",
      "       B-geo       0.33      0.01      0.02       118\n",
      "       B-gpe       0.86      0.06      0.11       107\n",
      "       B-nat       0.00      0.00      0.00         1\n",
      "       B-org       1.00      0.01      0.02        96\n",
      "       B-per       1.00      0.09      0.16        57\n",
      "       B-tim       0.44      0.20      0.28        55\n",
      "       I-art       0.00      0.00      0.00         6\n",
      "       I-geo       0.00      0.00      0.00        23\n",
      "       I-gpe       0.00      0.00      0.00         5\n",
      "       I-org       0.55      0.09      0.15        68\n",
      "       I-per       0.43      0.14      0.21        71\n",
      "       I-tim       0.00      0.00      0.00         4\n",
      "           O       0.86      0.94      0.90      3807\n",
      "         PAD       0.99      1.00      0.99     21168\n",
      "\n",
      "    accuracy                           0.97     25600\n",
      "   macro avg       0.43      0.17      0.19     25600\n",
      "weighted avg       0.96      0.97      0.96     25600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = flat_classification_report(test_target, test_output2)\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
