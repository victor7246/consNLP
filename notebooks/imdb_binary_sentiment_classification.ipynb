{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 14:58:45.963731 4539198912 file_utils.py:41] PyTorch version 1.5.0 available.\n",
      "I0806 14:58:53.984734 4539198912 file_utils.py:57] TensorFlow version 2.2.0-rc3 available.\n",
      "I0806 14:58:56.724100 4539198912 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "I0806 14:58:59.111385 4539198912 textcleaner.py:37] 'pattern' package not found; tag filters are not available for English\n",
      "W0806 14:58:59.377578 4539198912 deprecation.py:323] From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from dotenv import find_dotenv, load_dotenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '../src'))\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.getcwd(), '../src'))\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcontrib.optim import SWA\n",
    "from torch.optim import Adam, SGD \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CyclicLR, \\\n",
    "                                     CosineAnnealingWarmRestarts\n",
    "\n",
    "from consNLP.data import load_data, data_utils, fetch_dataset\n",
    "from consNLP.models import transformer_models, activations, layers, losses, scorers\n",
    "from consNLP.visualization import visualize\n",
    "from consNLP.trainer.trainer import BasicTrainer, PLTrainer, test_pl_trainer\n",
    "from consNLP.trainer.trainer_utils import set_seed, _has_apex, _torch_lightning_available, _has_wandb, _torch_gpu_available, _num_gpus, _torch_tpu_available\n",
    "from consNLP.preprocessing.custom_tokenizer import BERTweetTokenizer\n",
    "\n",
    "if _has_apex:\n",
    "    #from torch.cuda import amp\n",
    "    from apex import amp\n",
    "\n",
    "if _torch_tpu_available:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "if _has_wandb:\n",
    "    import wandb\n",
    "    try:\n",
    "        load_dotenv(find_dotenv())\n",
    "        wandb.login(key=os.environ['WANDB_API_KEY'])\n",
    "    except:\n",
    "        _has_wandb = False\n",
    "\n",
    "if _torch_lightning_available:\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    from pytorch_lightning.loggers import WandbLogger\n",
    "    from pytorch_lightning.metrics.metric import NumpyMetric\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "\n",
    "import tokenizers\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 14:59:06.972804 4539198912 fetch_dataset.py:16] making final data set from raw data\n",
      "I0806 14:59:06.973808 4539198912 fetch_dataset.py:21] project directory ../\n",
      "I0806 14:59:06.974637 4539198912 fetch_dataset.py:30] output path ../data/raw\n",
      "I0806 14:59:13.335819 4539198912 fetch_dataset.py:95] download complete\n"
     ]
    }
   ],
   "source": [
    "fetch_dataset(project_dir='../',download_from_kaggle=True,\\\n",
    "              kaggle_dataset='lakshmi25npathi/imdb-dataset-of-50k-movie-reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Logging: False, GPU: False, Pytorch Lightning: False, TPU: False, Apex: False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog='Torch trainer function',conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/IMDB Dataset.csv', required=False,\n",
    "                    help='train data')\n",
    "parser.add_argument('--val_data', type=str, default='', required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--task_type', type=str, default='binary_sequence_classification', required=False,\n",
    "                    help='type of task')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "parser.add_argument('--bpe_vocab_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding vocab file path')\n",
    "parser.add_argument('--bpe_merges_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding merges file path')\n",
    "parser.add_argument('--berttweettokenizer_path', type=str, default='', required=False,\n",
    "                    help='BERTweet tokenizer path')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=100, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--epochs', type=int, default=5, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.00003, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--loss_function', type=str, default='bcelogit', required=False,\n",
    "                    help='loss function')\n",
    "parser.add_argument('--metric', type=str, default='f1', required=False,\n",
    "                    help='scorer metric')\n",
    "\n",
    "parser.add_argument('--use_lightning_trainer', type=bool, default=False, required=False,\n",
    "                    help='if lightning trainer needs to be used')\n",
    "parser.add_argument('--use_torch_trainer', type=bool, default=True, required=False,\n",
    "                    help='if custom torch trainer needs to be used')\n",
    "parser.add_argument('--use_apex', type=bool, default=False, required=False,\n",
    "                    help='if apex needs to be used')\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, required=False,\n",
    "                    help='GPU mode')\n",
    "parser.add_argument('--use_TPU', type=bool, default=False, required=False,\n",
    "                    help='TPU mode')\n",
    "parser.add_argument('--num_gpus', type=int, default=0, required=False,\n",
    "                    help='Number of GPUs')\n",
    "parser.add_argument('--num_tpus', type=int, default=0, required=False,\n",
    "                    help='Number of TPUs')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=16, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/sentiment_classification/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=False, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print (\"Wandb Logging: {}, GPU: {}, Pytorch Lightning: {}, TPU: {}, Apex: {}\".format(\\\n",
    "            _has_wandb and args.wandb_logging, _torch_gpu_available,\\\n",
    "            _torch_lightning_available and args.use_lightning_trainer, _torch_tpu_available, _has_apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = False\n",
    "final_activation = None\n",
    "convert_output = None\n",
    "\n",
    "if args.task_type == 'binary_sequence_classification':\n",
    "    if args.metric != 'roc_auc_score': \n",
    "        convert_output = 'round'\n",
    "    if args.loss_function == 'bcelogit':\n",
    "        final_activation = 'sigmoid'\n",
    "        \n",
    "elif args.task_type == 'multiclass_sequence_classification':\n",
    "    convert_output = 'max'\n",
    "    \n",
    "elif args.task_type == 'binary_token_classification':\n",
    "    reshape = True\n",
    "    if args.metric != 'roc_auc_score': \n",
    "        convert_output = 'round'\n",
    "    if args.loss_function == 'bcelogit':\n",
    "        final_activation = 'sigmoid'\n",
    "        \n",
    "elif args.task_type == 'multiclass_token_classification':\n",
    "    reshape = True\n",
    "    convert_output = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data.load_pandas_df(args.train_data,sep=',')\n",
    "df = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = args.model_save_path\n",
    "try:\n",
    "    os.makedirs(model_save_dir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment, label2idx = data_utils.convert_categorical_label_to_int(df.sentiment, \\\n",
    "                                                             save_path=os.path.join(model_save_dir,'label2idx.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(5)\n",
    "\n",
    "for train_index, val_index in kf.split(df.review, df.sentiment):\n",
    "    break\n",
    "    \n",
    "train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "val_df = df.iloc[val_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 2), (200, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 14:59:18.932564 4539198912 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0806 14:59:18.933578 4539198912 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0806 14:59:21.161639 4539198912 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /Users/victor/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0806 14:59:21.162390 4539198912 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /Users/victor/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "if args.berttweettokenizer_path:\n",
    "    tokenizer = BERTweetTokenizer(args.berttweettokenizer_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.transformer_model_pretrained_path)\n",
    "\n",
    "if not args.berttweettokenizer_path:\n",
    "    try:\n",
    "        bpetokenizer = tokenizers.ByteLevelBPETokenizer(args.bpe_vocab_path, \\\n",
    "                                        args.bpe_merges_path)\n",
    "    except:\n",
    "        bpetokenizer = None \n",
    "else:\n",
    "    bpetokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_utils.TransformerDataset(train_df.review, bpetokenizer=bpetokenizer, tokenizer=tokenizer, MAX_LEN=args.max_text_len, \\\n",
    "              target_label=train_df.sentiment, sequence_target=False, target_text=None, conditional_label=None, conditional_all_labels=None)\n",
    "\n",
    "val_dataset = data_utils.TransformerDataset(val_df.review, bpetokenizer=bpetokenizer, tokenizer=tokenizer, MAX_LEN=args.max_text_len, \\\n",
    "              target_label=val_df.sentiment, sequence_target=False, target_text=None, conditional_label=None, conditional_all_labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 14:59:30.627707 4539198912 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0806 14:59:30.628615 4539198912 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0806 14:59:31.515705 4539198912 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /Users/victor/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(args.transformer_config_path, output_hidden_states=True, output_attentions=True)\n",
    "basemodel = AutoModel.from_pretrained(args.transformer_model_pretrained_path,config=config)\n",
    "model = transformer_models.TransformerWithCLS(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      train_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True\n",
    "    )\n",
    "\n",
    "    val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      val_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, sampler=train_sampler,\n",
    "        drop_last=True,num_workers=2)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size, sampler=val_sampler,\n",
    "        drop_last=False,num_workers=1)\n",
    "else:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pytorch Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "[LOG] Total number of parameters to learn 124646401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "\n",
      "Current training Loss 0.704:   0%|          | 0/50 [00:08<?, ?it/s]\u001b[A\n",
      "Current training Loss 0.704:   2%|▏         | 1/50 [00:08<07:17,  8.92s/it]\u001b[A\n",
      "Current training Loss 0.653:   2%|▏         | 1/50 [00:17<07:17,  8.92s/it]\u001b[A\n",
      "Current training Loss 0.653:   4%|▍         | 2/50 [00:17<06:56,  8.67s/it]\u001b[A\n",
      "Current training Loss 0.692:   4%|▍         | 2/50 [00:25<06:56,  8.67s/it]\u001b[A\n",
      "Current training Loss 0.692:   6%|▌         | 3/50 [00:25<06:40,  8.52s/it]\u001b[A\n",
      "Current training Loss 0.757:   6%|▌         | 3/50 [00:31<06:40,  8.52s/it]\u001b[A\n",
      "Current training Loss 0.757:   8%|▊         | 4/50 [00:31<06:00,  7.85s/it]\u001b[A\n",
      "Current training Loss 0.704:   8%|▊         | 4/50 [00:38<06:00,  7.85s/it]\u001b[A\n",
      "Current training Loss 0.704:  10%|█         | 5/50 [00:38<05:37,  7.49s/it]\u001b[A\n",
      "Current training Loss 0.637:  10%|█         | 5/50 [00:44<05:37,  7.49s/it]\u001b[A\n",
      "Current training Loss 0.637:  12%|█▏        | 6/50 [00:44<05:10,  7.06s/it]\u001b[A\n",
      "Current training Loss 0.688:  12%|█▏        | 6/50 [00:50<05:10,  7.06s/it]\u001b[A\n",
      "Current training Loss 0.688:  14%|█▍        | 7/50 [00:50<04:49,  6.73s/it]\u001b[A\n",
      "Current training Loss 0.714:  14%|█▍        | 7/50 [00:56<04:49,  6.73s/it]\u001b[A\n",
      "Current training Loss 0.714:  16%|█▌        | 8/50 [00:56<04:33,  6.52s/it]\u001b[A\n",
      "Current training Loss 0.671:  16%|█▌        | 8/50 [01:02<04:33,  6.52s/it]\u001b[A\n",
      "Current training Loss 0.671:  18%|█▊        | 9/50 [01:02<04:20,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.742:  18%|█▊        | 9/50 [01:08<04:20,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.742:  20%|██        | 10/50 [01:08<04:18,  6.46s/it]\u001b[A\n",
      "Current training Loss 0.679:  20%|██        | 10/50 [01:15<04:18,  6.46s/it]\u001b[A\n",
      "Current training Loss 0.679:  22%|██▏       | 11/50 [01:15<04:10,  6.44s/it]\u001b[A\n",
      "Current training Loss 0.7:  22%|██▏       | 11/50 [01:21<04:10,  6.44s/it]  \u001b[A\n",
      "Current training Loss 0.7:  24%|██▍       | 12/50 [01:21<04:01,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.677:  24%|██▍       | 12/50 [01:27<04:01,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.677:  26%|██▌       | 13/50 [01:27<03:54,  6.33s/it]\u001b[A\n",
      "Current training Loss 0.687:  26%|██▌       | 13/50 [01:33<03:54,  6.33s/it]\u001b[A\n",
      "Current training Loss 0.687:  28%|██▊       | 14/50 [01:33<03:46,  6.30s/it]\u001b[A\n",
      "Current training Loss 0.697:  28%|██▊       | 14/50 [01:40<03:46,  6.30s/it]\u001b[A\n",
      "Current training Loss 0.697:  30%|███       | 15/50 [01:40<03:39,  6.27s/it]\u001b[A\n",
      "Current training Loss 0.691:  30%|███       | 15/50 [01:46<03:39,  6.27s/it]\u001b[A\n",
      "Current training Loss 0.691:  32%|███▏      | 16/50 [01:46<03:33,  6.27s/it]\u001b[A\n",
      "Current training Loss 0.673:  32%|███▏      | 16/50 [01:52<03:33,  6.27s/it]\u001b[A\n",
      "Current training Loss 0.673:  34%|███▍      | 17/50 [01:52<03:27,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.695:  34%|███▍      | 17/50 [01:58<03:27,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.695:  36%|███▌      | 18/50 [01:58<03:16,  6.15s/it]\u001b[A\n",
      "Current training Loss 0.685:  36%|███▌      | 18/50 [02:04<03:16,  6.15s/it]\u001b[A\n",
      "Current training Loss 0.685:  38%|███▊      | 19/50 [02:04<03:08,  6.08s/it]\u001b[A\n",
      "Current training Loss 0.669:  38%|███▊      | 19/50 [02:10<03:08,  6.08s/it]\u001b[A\n",
      "Current training Loss 0.669:  40%|████      | 20/50 [02:10<03:00,  6.03s/it]\u001b[A\n",
      "Current training Loss 0.671:  40%|████      | 20/50 [02:15<03:00,  6.03s/it]\u001b[A\n",
      "Current training Loss 0.671:  42%|████▏     | 21/50 [02:15<02:50,  5.89s/it]\u001b[A\n",
      "Current training Loss 0.629:  42%|████▏     | 21/50 [02:21<02:50,  5.89s/it]\u001b[A\n",
      "Current training Loss 0.629:  44%|████▍     | 22/50 [02:21<02:44,  5.87s/it]\u001b[A\n",
      "Current training Loss 0.65:  44%|████▍     | 22/50 [02:26<02:44,  5.87s/it] \u001b[A\n",
      "Current training Loss 0.65:  46%|████▌     | 23/50 [02:26<02:32,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.658:  46%|████▌     | 23/50 [02:32<02:32,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.658:  48%|████▊     | 24/50 [02:32<02:23,  5.51s/it]\u001b[A\n",
      "Current training Loss 0.644:  48%|████▊     | 24/50 [02:37<02:23,  5.51s/it]\u001b[A\n",
      "Current training Loss 0.644:  50%|█████     | 25/50 [02:37<02:16,  5.47s/it]\u001b[A\n",
      "Current training Loss 0.564:  50%|█████     | 25/50 [02:43<02:16,  5.47s/it]\u001b[A\n",
      "Current training Loss 0.564:  52%|█████▏    | 26/50 [02:43<02:13,  5.56s/it]\u001b[A\n",
      "Current training Loss 0.562:  52%|█████▏    | 26/50 [02:48<02:13,  5.56s/it]\u001b[A\n",
      "Current training Loss 0.562:  54%|█████▍    | 27/50 [02:48<02:06,  5.49s/it]\u001b[A\n",
      "Current training Loss 0.546:  54%|█████▍    | 27/50 [02:54<02:06,  5.49s/it]\u001b[A\n",
      "Current training Loss 0.546:  56%|█████▌    | 28/50 [02:54<02:03,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.642:  56%|█████▌    | 28/50 [02:59<02:03,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.642:  58%|█████▊    | 29/50 [02:59<01:55,  5.49s/it]\u001b[A\n",
      "Current training Loss 0.315:  58%|█████▊    | 29/50 [03:05<01:55,  5.49s/it]\u001b[A\n",
      "Current training Loss 0.315:  60%|██████    | 30/50 [03:05<01:54,  5.71s/it]\u001b[A\n",
      "Current training Loss 0.372:  60%|██████    | 30/50 [03:12<01:54,  5.71s/it]\u001b[A\n",
      "Current training Loss 0.372:  62%|██████▏   | 31/50 [03:12<01:53,  5.96s/it]\u001b[A\n",
      "Current training Loss 0.603:  62%|██████▏   | 31/50 [03:18<01:53,  5.96s/it]\u001b[A\n",
      "Current training Loss 0.603:  64%|██████▍   | 32/50 [03:18<01:48,  6.04s/it]\u001b[A\n",
      "Current training Loss 0.374:  64%|██████▍   | 32/50 [03:25<01:48,  6.04s/it]\u001b[A\n",
      "Current training Loss 0.374:  66%|██████▌   | 33/50 [03:25<01:44,  6.15s/it]\u001b[A\n",
      "Current training Loss 0.293:  66%|██████▌   | 33/50 [03:31<01:44,  6.15s/it]\u001b[A\n",
      "Current training Loss 0.293:  68%|██████▊   | 34/50 [03:31<01:39,  6.20s/it]\u001b[A\n",
      "Current training Loss 0.485:  68%|██████▊   | 34/50 [03:37<01:39,  6.20s/it]\u001b[A\n",
      "Current training Loss 0.485:  70%|███████   | 35/50 [03:37<01:33,  6.23s/it]\u001b[A\n",
      "Current training Loss 0.831:  70%|███████   | 35/50 [03:43<01:33,  6.23s/it]\u001b[A\n",
      "Current training Loss 0.831:  72%|███████▏  | 36/50 [03:43<01:26,  6.21s/it]\u001b[A\n",
      "Current training Loss 0.745:  72%|███████▏  | 36/50 [03:49<01:26,  6.21s/it]\u001b[A\n",
      "Current training Loss 0.745:  74%|███████▍  | 37/50 [03:49<01:20,  6.20s/it]\u001b[A\n",
      "Current training Loss 0.692:  74%|███████▍  | 37/50 [03:56<01:20,  6.20s/it]\u001b[A\n",
      "Current training Loss 0.692:  76%|███████▌  | 38/50 [03:56<01:14,  6.23s/it]\u001b[A\n",
      "Current training Loss 0.449:  76%|███████▌  | 38/50 [04:03<01:14,  6.23s/it]\u001b[A\n",
      "Current training Loss 0.449:  78%|███████▊  | 39/50 [04:03<01:10,  6.40s/it]\u001b[A\n",
      "Current training Loss 0.455:  78%|███████▊  | 39/50 [04:09<01:10,  6.40s/it]\u001b[A\n",
      "Current training Loss 0.455:  80%|████████  | 40/50 [04:09<01:03,  6.32s/it]\u001b[A\n",
      "Current training Loss 0.28:  80%|████████  | 40/50 [04:15<01:03,  6.32s/it] \u001b[A\n",
      "Current training Loss 0.28:  82%|████████▏ | 41/50 [04:15<00:57,  6.34s/it]\u001b[A\n",
      "Current training Loss 0.52:  82%|████████▏ | 41/50 [04:21<00:57,  6.34s/it]\u001b[A\n",
      "Current training Loss 0.52:  84%|████████▍ | 42/50 [04:21<00:50,  6.31s/it]\u001b[A\n",
      "Current training Loss 0.35:  84%|████████▍ | 42/50 [04:28<00:50,  6.31s/it]\u001b[A\n",
      "Current training Loss 0.35:  86%|████████▌ | 43/50 [04:28<00:44,  6.29s/it]\u001b[A\n",
      "Current training Loss 0.39:  86%|████████▌ | 43/50 [04:34<00:44,  6.29s/it]\u001b[A\n",
      "Current training Loss 0.39:  88%|████████▊ | 44/50 [04:34<00:37,  6.33s/it]\u001b[A\n",
      "Current training Loss 0.549:  88%|████████▊ | 44/50 [04:40<00:37,  6.33s/it]\u001b[A\n",
      "Current training Loss 0.549:  90%|█████████ | 45/50 [04:40<00:31,  6.21s/it]\u001b[A\n",
      "Current training Loss 0.367:  90%|█████████ | 45/50 [04:45<00:31,  6.21s/it]\u001b[A\n",
      "Current training Loss 0.367:  92%|█████████▏| 46/50 [04:45<00:23,  5.89s/it]\u001b[A\n",
      "Current training Loss 0.421:  92%|█████████▏| 46/50 [04:50<00:23,  5.89s/it]\u001b[A\n",
      "Current training Loss 0.421:  94%|█████████▍| 47/50 [04:50<00:16,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.266:  94%|█████████▍| 47/50 [04:55<00:16,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.266:  96%|█████████▌| 48/50 [04:55<00:10,  5.48s/it]\u001b[A\n",
      "Current training Loss 0.713:  96%|█████████▌| 48/50 [05:01<00:10,  5.48s/it]\u001b[A\n",
      "Current training Loss 0.713:  98%|█████████▊| 49/50 [05:01<00:05,  5.50s/it]\u001b[A\n",
      "Current training Loss 0.156:  98%|█████████▊| 49/50 [05:06<00:05,  5.50s/it]\u001b[A\n",
      "Current training Loss 0.156: 100%|██████████| 50/50 [05:06<00:00,  6.13s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.47:   0%|          | 0/50 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.47:   2%|▏         | 1/50 [00:01<01:04,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.279:   2%|▏         | 1/50 [00:02<01:04,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.279:   4%|▍         | 2/50 [00:02<01:03,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.275:   4%|▍         | 2/50 [00:03<01:03,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.275:   6%|▌         | 3/50 [00:03<01:01,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.612:   6%|▌         | 3/50 [00:05<01:01,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.612:   8%|▊         | 4/50 [00:05<01:00,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.322:   8%|▊         | 4/50 [00:06<01:00,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.322:  10%|█         | 5/50 [00:06<00:59,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.341:  10%|█         | 5/50 [00:07<00:59,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.341:  12%|█▏        | 6/50 [00:07<00:57,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.364:  12%|█▏        | 6/50 [00:09<00:57,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.364:  14%|█▍        | 7/50 [00:09<00:56,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.13:  14%|█▍        | 7/50 [00:10<00:56,  1.31s/it] \u001b[A\n",
      "Current eval Loss 0.13:  16%|█▌        | 8/50 [00:10<00:55,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.222:  16%|█▌        | 8/50 [00:11<00:55,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.222:  18%|█▊        | 9/50 [00:11<00:54,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.334:  18%|█▊        | 9/50 [00:13<00:54,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.334:  20%|██        | 10/50 [00:13<00:53,  1.34s/it]\u001b[A\n",
      "Current eval Loss 0.462:  20%|██        | 10/50 [00:14<00:53,  1.34s/it]\u001b[A\n",
      "Current eval Loss 0.462:  22%|██▏       | 11/50 [00:14<00:52,  1.34s/it]\u001b[A\n",
      "Current eval Loss 0.352:  22%|██▏       | 11/50 [00:15<00:52,  1.34s/it]\u001b[A\n",
      "Current eval Loss 0.352:  24%|██▍       | 12/50 [00:15<00:50,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.503:  24%|██▍       | 12/50 [00:17<00:50,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.503:  26%|██▌       | 13/50 [00:17<00:48,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.53:  26%|██▌       | 13/50 [00:18<00:48,  1.32s/it] \u001b[A\n",
      "Current eval Loss 0.53:  28%|██▊       | 14/50 [00:18<00:47,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.214:  28%|██▊       | 14/50 [00:19<00:47,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.214:  30%|███       | 15/50 [00:19<00:46,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.42:  30%|███       | 15/50 [00:21<00:46,  1.32s/it] \u001b[A\n",
      "Current eval Loss 0.42:  32%|███▏      | 16/50 [00:21<00:44,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.163:  32%|███▏      | 16/50 [00:22<00:44,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.163:  34%|███▍      | 17/50 [00:22<00:43,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.534:  34%|███▍      | 17/50 [00:23<00:43,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.534:  36%|███▌      | 18/50 [00:23<00:42,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.201:  36%|███▌      | 18/50 [00:25<00:42,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.201:  38%|███▊      | 19/50 [00:25<00:40,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.34:  38%|███▊      | 19/50 [00:26<00:40,  1.32s/it] \u001b[A\n",
      "Current eval Loss 0.34:  40%|████      | 20/50 [00:26<00:39,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.288:  40%|████      | 20/50 [00:27<00:39,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.288:  42%|████▏     | 21/50 [00:27<00:38,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.219:  42%|████▏     | 21/50 [00:29<00:38,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.219:  44%|████▍     | 22/50 [00:29<00:36,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.219:  44%|████▍     | 22/50 [00:30<00:36,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.219:  46%|████▌     | 23/50 [00:30<00:35,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.229:  46%|████▌     | 23/50 [00:31<00:35,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.229:  48%|████▊     | 24/50 [00:31<00:34,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.291:  48%|████▊     | 24/50 [00:33<00:34,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.291:  50%|█████     | 25/50 [00:33<00:33,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.113:  50%|█████     | 25/50 [00:34<00:33,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.113:  52%|█████▏    | 26/50 [00:34<00:31,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.126:  52%|█████▏    | 26/50 [00:35<00:31,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.126:  54%|█████▍    | 27/50 [00:35<00:30,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.41:  54%|█████▍    | 27/50 [00:37<00:30,  1.33s/it] \u001b[A\n",
      "Current eval Loss 0.41:  56%|█████▌    | 28/50 [00:37<00:29,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.273:  56%|█████▌    | 28/50 [00:38<00:29,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.273:  58%|█████▊    | 29/50 [00:38<00:27,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.177:  58%|█████▊    | 29/50 [00:39<00:27,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.177:  60%|██████    | 30/50 [00:39<00:26,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.162:  60%|██████    | 30/50 [00:40<00:26,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.162:  62%|██████▏   | 31/50 [00:40<00:25,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.399:  62%|██████▏   | 31/50 [00:42<00:25,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.399:  64%|██████▍   | 32/50 [00:42<00:23,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.148:  64%|██████▍   | 32/50 [00:43<00:23,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.148:  66%|██████▌   | 33/50 [00:43<00:22,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.301:  66%|██████▌   | 33/50 [00:44<00:22,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.301:  68%|██████▊   | 34/50 [00:44<00:21,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.359:  68%|██████▊   | 34/50 [00:46<00:21,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.359:  70%|███████   | 35/50 [00:46<00:19,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.394:  70%|███████   | 35/50 [00:47<00:19,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.394:  72%|███████▏  | 36/50 [00:47<00:18,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.639:  72%|███████▏  | 36/50 [00:48<00:18,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.639:  74%|███████▍  | 37/50 [00:48<00:17,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.496:  74%|███████▍  | 37/50 [00:50<00:17,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.496:  76%|███████▌  | 38/50 [00:50<00:15,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.296:  76%|███████▌  | 38/50 [00:51<00:15,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.296:  78%|███████▊  | 39/50 [00:51<00:14,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.293:  78%|███████▊  | 39/50 [00:52<00:14,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.293:  80%|████████  | 40/50 [00:52<00:13,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.091:  80%|████████  | 40/50 [00:54<00:13,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.091:  82%|████████▏ | 41/50 [00:54<00:11,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.348:  82%|████████▏ | 41/50 [00:55<00:11,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.348:  84%|████████▍ | 42/50 [00:55<00:10,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.21:  84%|████████▍ | 42/50 [00:56<00:10,  1.31s/it] \u001b[A\n",
      "Current eval Loss 0.21:  86%|████████▌ | 43/50 [00:56<00:09,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.229:  86%|████████▌ | 43/50 [00:58<00:09,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.229:  88%|████████▊ | 44/50 [00:58<00:07,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.386:  88%|████████▊ | 44/50 [00:59<00:07,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.386:  90%|█████████ | 45/50 [00:59<00:06,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.282:  90%|█████████ | 45/50 [01:00<00:06,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.282:  92%|█████████▏| 46/50 [01:00<00:05,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.356:  92%|█████████▏| 46/50 [01:02<00:05,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.356:  94%|█████████▍| 47/50 [01:02<00:03,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.196:  94%|█████████▍| 47/50 [01:03<00:03,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.196:  96%|█████████▌| 48/50 [01:03<00:02,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.33:  96%|█████████▌| 48/50 [01:04<00:02,  1.31s/it] \u001b[A\n",
      "Current eval Loss 0.33:  98%|█████████▊| 49/50 [01:04<00:01,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.132:  98%|█████████▊| 49/50 [01:05<00:01,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.132: 100%|██████████| 50/50 [01:05<00:00,  1.32s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.189:   0%|          | 0/13 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.189:   8%|▊         | 1/13 [00:01<00:15,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.484:   8%|▊         | 1/13 [00:02<00:15,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.484:  15%|█▌        | 2/13 [00:02<00:14,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.221:  15%|█▌        | 2/13 [00:03<00:14,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.221:  23%|██▎       | 3/13 [00:03<00:13,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.2:  23%|██▎       | 3/13 [00:05<00:13,  1.32s/it]  \u001b[A\n",
      "Current eval Loss 0.2:  31%|███       | 4/13 [00:05<00:11,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.46:  31%|███       | 4/13 [00:06<00:11,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.46:  38%|███▊      | 5/13 [00:06<00:10,  1.34s/it]\u001b[A\n",
      "Current eval Loss 0.438:  38%|███▊      | 5/13 [00:07<00:10,  1.34s/it]\u001b[A\n",
      "Current eval Loss 0.438:  46%|████▌     | 6/13 [00:07<00:09,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.556:  46%|████▌     | 6/13 [00:09<00:09,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.556:  54%|█████▍    | 7/13 [00:09<00:07,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.649:  54%|█████▍    | 7/13 [00:10<00:07,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.649:  62%|██████▏   | 8/13 [00:10<00:06,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.535:  62%|██████▏   | 8/13 [00:11<00:06,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.535:  69%|██████▉   | 9/13 [00:11<00:05,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.365:  69%|██████▉   | 9/13 [00:13<00:05,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.365:  77%|███████▋  | 10/13 [00:13<00:03,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.484:  77%|███████▋  | 10/13 [00:14<00:03,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.484:  85%|████████▍ | 11/13 [00:14<00:02,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.415:  85%|████████▍ | 11/13 [00:15<00:02,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.415:  92%|█████████▏| 12/13 [00:15<00:01,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.389:  92%|█████████▏| 12/13 [00:16<00:01,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.389: 100%|██████████| 13/13 [00:16<00:00,  1.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.309 Train metric = 0.882 Val loss = 0.414 Val metric = 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Current training Loss 0.471:   0%|          | 0/50 [00:05<?, ?it/s]\u001b[A\n",
      "Current training Loss 0.471:   2%|▏         | 1/50 [00:05<04:28,  5.47s/it]\u001b[A\n",
      "Current training Loss 0.275:   2%|▏         | 1/50 [00:10<04:28,  5.47s/it]\u001b[A\n",
      "Current training Loss 0.275:   4%|▍         | 2/50 [00:10<04:18,  5.40s/it]\u001b[A\n",
      "Current training Loss 0.421:   4%|▍         | 2/50 [00:16<04:18,  5.40s/it]\u001b[A\n",
      "Current training Loss 0.421:   6%|▌         | 3/50 [00:16<04:14,  5.41s/it]\u001b[A\n",
      "Current training Loss 0.617:   6%|▌         | 3/50 [00:23<04:14,  5.41s/it]\u001b[A\n",
      "Current training Loss 0.617:   8%|▊         | 4/50 [00:23<04:42,  6.14s/it]\u001b[A\n",
      "Current training Loss 0.437:   8%|▊         | 4/50 [00:31<04:42,  6.14s/it]\u001b[A\n",
      "Current training Loss 0.437:  10%|█         | 5/50 [00:31<05:00,  6.69s/it]\u001b[A\n",
      "Current training Loss 0.4:  10%|█         | 5/50 [00:39<05:00,  6.69s/it]  \u001b[A\n",
      "Current training Loss 0.4:  12%|█▏        | 6/50 [00:39<05:09,  7.03s/it]\u001b[A\n",
      "Current training Loss 0.414:  12%|█▏        | 6/50 [00:46<05:09,  7.03s/it]\u001b[A\n",
      "Current training Loss 0.414:  14%|█▍        | 7/50 [00:46<04:55,  6.87s/it]\u001b[A\n",
      "Current training Loss 0.345:  14%|█▍        | 7/50 [00:52<04:55,  6.87s/it]\u001b[A\n",
      "Current training Loss 0.345:  16%|█▌        | 8/50 [00:52<04:41,  6.71s/it]\u001b[A\n",
      "Current training Loss 0.318:  16%|█▌        | 8/50 [01:00<04:41,  6.71s/it]\u001b[A\n",
      "Current training Loss 0.318:  18%|█▊        | 9/50 [01:00<04:53,  7.17s/it]\u001b[A\n",
      "Current training Loss 0.418:  18%|█▊        | 9/50 [01:08<04:53,  7.17s/it]\u001b[A\n",
      "Current training Loss 0.418:  20%|██        | 10/50 [01:08<04:53,  7.34s/it]\u001b[A\n",
      "Current training Loss 0.337:  20%|██        | 10/50 [01:14<04:53,  7.34s/it]\u001b[A\n",
      "Current training Loss 0.337:  22%|██▏       | 11/50 [01:14<04:27,  6.86s/it]\u001b[A\n",
      "Current training Loss 0.377:  22%|██▏       | 11/50 [01:19<04:27,  6.86s/it]\u001b[A\n",
      "Current training Loss 0.377:  24%|██▍       | 12/50 [01:19<04:06,  6.49s/it]\u001b[A\n",
      "Current training Loss 0.484:  24%|██▍       | 12/50 [01:25<04:06,  6.49s/it]\u001b[A\n",
      "Current training Loss 0.484:  26%|██▌       | 13/50 [01:25<03:51,  6.25s/it]\u001b[A\n",
      "Current training Loss 0.481:  26%|██▌       | 13/50 [01:31<03:51,  6.25s/it]\u001b[A\n",
      "Current training Loss 0.481:  28%|██▊       | 14/50 [01:31<03:37,  6.05s/it]\u001b[A\n",
      "Current training Loss 0.275:  28%|██▊       | 14/50 [01:36<03:37,  6.05s/it]\u001b[A\n",
      "Current training Loss 0.275:  30%|███       | 15/50 [01:36<03:27,  5.92s/it]\u001b[A\n",
      "Current training Loss 0.309:  30%|███       | 15/50 [01:42<03:27,  5.92s/it]\u001b[A\n",
      "Current training Loss 0.309:  32%|███▏      | 16/50 [01:42<03:18,  5.83s/it]\u001b[A\n",
      "Current training Loss 0.268:  32%|███▏      | 16/50 [01:48<03:18,  5.83s/it]\u001b[A\n",
      "Current training Loss 0.268:  34%|███▍      | 17/50 [01:48<03:10,  5.79s/it]\u001b[A\n",
      "Current training Loss 0.507:  34%|███▍      | 17/50 [01:53<03:10,  5.79s/it]\u001b[A\n",
      "Current training Loss 0.507:  36%|███▌      | 18/50 [01:53<03:04,  5.76s/it]\u001b[A\n",
      "Current training Loss 0.235:  36%|███▌      | 18/50 [01:59<03:04,  5.76s/it]\u001b[A\n",
      "Current training Loss 0.235:  38%|███▊      | 19/50 [01:59<02:57,  5.72s/it]\u001b[A\n",
      "Current training Loss 0.371:  38%|███▊      | 19/50 [02:05<02:57,  5.72s/it]\u001b[A\n",
      "Current training Loss 0.371:  40%|████      | 20/50 [02:05<02:51,  5.73s/it]\u001b[A\n",
      "Current training Loss 0.358:  40%|████      | 20/50 [02:10<02:51,  5.73s/it]\u001b[A\n",
      "Current training Loss 0.358:  42%|████▏     | 21/50 [02:10<02:44,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.174:  42%|████▏     | 21/50 [02:16<02:44,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.174:  44%|████▍     | 22/50 [02:16<02:39,  5.69s/it]\u001b[A\n",
      "Current training Loss 0.189:  44%|████▍     | 22/50 [02:22<02:39,  5.69s/it]\u001b[A\n",
      "Current training Loss 0.189:  46%|████▌     | 23/50 [02:22<02:35,  5.76s/it]\u001b[A\n",
      "Current training Loss 0.374:  46%|████▌     | 23/50 [02:28<02:35,  5.76s/it]\u001b[A\n",
      "Current training Loss 0.374:  48%|████▊     | 24/50 [02:28<02:29,  5.73s/it]\u001b[A\n",
      "Current training Loss 0.347:  48%|████▊     | 24/50 [02:33<02:29,  5.73s/it]\u001b[A\n",
      "Current training Loss 0.347:  50%|█████     | 25/50 [02:33<02:23,  5.73s/it]\u001b[A\n",
      "Current training Loss 0.308:  50%|█████     | 25/50 [02:39<02:23,  5.73s/it]\u001b[A\n",
      "Current training Loss 0.308:  52%|█████▏    | 26/50 [02:39<02:16,  5.68s/it]\u001b[A\n",
      "Current training Loss 0.157:  52%|█████▏    | 26/50 [02:45<02:16,  5.68s/it]\u001b[A\n",
      "Current training Loss 0.157:  54%|█████▍    | 27/50 [02:45<02:10,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.322:  54%|█████▍    | 27/50 [02:50<02:10,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.322:  56%|█████▌    | 28/50 [02:50<02:04,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.526:  56%|█████▌    | 28/50 [02:56<02:04,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.526:  58%|█████▊    | 29/50 [02:56<01:58,  5.63s/it]\u001b[A\n",
      "Current training Loss 0.288:  58%|█████▊    | 29/50 [03:01<01:58,  5.63s/it]\u001b[A\n",
      "Current training Loss 0.288:  60%|██████    | 30/50 [03:01<01:53,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.19:  60%|██████    | 30/50 [03:07<01:53,  5.65s/it] \u001b[A\n",
      "Current training Loss 0.19:  62%|██████▏   | 31/50 [03:07<01:46,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.483:  62%|██████▏   | 31/50 [03:13<01:46,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.483:  64%|██████▍   | 32/50 [03:13<01:41,  5.61s/it]\u001b[A\n",
      "Current training Loss 0.138:  64%|██████▍   | 32/50 [03:18<01:41,  5.61s/it]\u001b[A\n",
      "Current training Loss 0.138:  66%|██████▌   | 33/50 [03:18<01:35,  5.63s/it]\u001b[A\n",
      "Current training Loss 0.256:  66%|██████▌   | 33/50 [03:24<01:35,  5.63s/it]\u001b[A\n",
      "Current training Loss 0.256:  68%|██████▊   | 34/50 [03:24<01:29,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.239:  68%|██████▊   | 34/50 [03:29<01:29,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.239:  70%|███████   | 35/50 [03:29<01:24,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.232:  70%|███████   | 35/50 [03:35<01:24,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.232:  72%|███████▏  | 36/50 [03:35<01:19,  5.66s/it]\u001b[A\n",
      "Current training Loss 0.619:  72%|███████▏  | 36/50 [03:42<01:19,  5.66s/it]\u001b[A\n",
      "Current training Loss 0.619:  74%|███████▍  | 37/50 [03:42<01:19,  6.10s/it]\u001b[A\n",
      "Current training Loss 0.277:  74%|███████▍  | 37/50 [03:49<01:19,  6.10s/it]\u001b[A\n",
      "Current training Loss 0.277:  76%|███████▌  | 38/50 [03:49<01:14,  6.24s/it]\u001b[A\n",
      "Current training Loss 0.296:  76%|███████▌  | 38/50 [03:54<01:14,  6.24s/it]\u001b[A\n",
      "Current training Loss 0.296:  78%|███████▊  | 39/50 [03:54<01:06,  6.03s/it]\u001b[A\n",
      "Current training Loss 0.377:  78%|███████▊  | 39/50 [04:00<01:06,  6.03s/it]\u001b[A\n",
      "Current training Loss 0.377:  80%|████████  | 40/50 [04:00<00:59,  5.91s/it]\u001b[A\n",
      "Current training Loss 0.112:  80%|████████  | 40/50 [04:06<00:59,  5.91s/it]\u001b[A\n",
      "Current training Loss 0.112:  82%|████████▏ | 41/50 [04:06<00:52,  5.82s/it]\u001b[A\n",
      "Current training Loss 0.389:  82%|████████▏ | 41/50 [04:11<00:52,  5.82s/it]\u001b[A\n",
      "Current training Loss 0.389:  84%|████████▍ | 42/50 [04:11<00:46,  5.77s/it]\u001b[A\n",
      "Current training Loss 0.325:  84%|████████▍ | 42/50 [04:17<00:46,  5.77s/it]\u001b[A\n",
      "Current training Loss 0.325:  86%|████████▌ | 43/50 [04:17<00:40,  5.72s/it]\u001b[A\n",
      "Current training Loss 0.19:  86%|████████▌ | 43/50 [04:23<00:40,  5.72s/it] \u001b[A\n",
      "Current training Loss 0.19:  88%|████████▊ | 44/50 [04:23<00:34,  5.74s/it]\u001b[A\n",
      "Current training Loss 0.325:  88%|████████▊ | 44/50 [04:28<00:34,  5.74s/it]\u001b[A\n",
      "Current training Loss 0.325:  90%|█████████ | 45/50 [04:28<00:28,  5.69s/it]\u001b[A\n",
      "Current training Loss 0.219:  90%|█████████ | 45/50 [04:34<00:28,  5.69s/it]\u001b[A\n",
      "Current training Loss 0.219:  92%|█████████▏| 46/50 [04:34<00:22,  5.64s/it]\u001b[A\n",
      "Current training Loss 0.201:  92%|█████████▏| 46/50 [04:39<00:22,  5.64s/it]\u001b[A\n",
      "Current training Loss 0.201:  94%|█████████▍| 47/50 [04:39<00:16,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.108:  94%|█████████▍| 47/50 [04:45<00:16,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.108:  96%|█████████▌| 48/50 [04:45<00:11,  5.58s/it]\u001b[A\n",
      "Current training Loss 0.488:  96%|█████████▌| 48/50 [04:51<00:11,  5.58s/it]\u001b[A\n",
      "Current training Loss 0.488:  98%|█████████▊| 49/50 [04:51<00:05,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.155:  98%|█████████▊| 49/50 [04:56<00:05,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.155: 100%|██████████| 50/50 [04:56<00:00,  5.94s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.309:   0%|          | 0/50 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.309:   2%|▏         | 1/50 [00:01<01:14,  1.52s/it]\u001b[A\n",
      "Current eval Loss 0.097:   2%|▏         | 1/50 [00:02<01:14,  1.52s/it]\u001b[A\n",
      "Current eval Loss 0.097:   4%|▍         | 2/50 [00:02<01:11,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.102:   4%|▍         | 2/50 [00:04<01:11,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.102:   6%|▌         | 3/50 [00:04<01:09,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.337:   6%|▌         | 3/50 [00:06<01:09,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.337:   8%|▊         | 4/50 [00:06<01:09,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.135:   8%|▊         | 4/50 [00:07<01:09,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.135:  10%|█         | 5/50 [00:07<01:07,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.203:  10%|█         | 5/50 [00:08<01:07,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.203:  12%|█▏        | 6/50 [00:08<01:05,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.319:  12%|█▏        | 6/50 [00:10<01:05,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.319:  14%|█▍        | 7/50 [00:10<01:03,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.127:  14%|█▍        | 7/50 [00:11<01:03,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.127:  16%|█▌        | 8/50 [00:11<01:03,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.133:  16%|█▌        | 8/50 [00:13<01:03,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.133:  18%|█▊        | 9/50 [00:13<01:00,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.136:  18%|█▊        | 9/50 [00:14<01:00,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.136:  20%|██        | 10/50 [00:14<00:59,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.16:  20%|██        | 10/50 [00:16<00:59,  1.48s/it] \u001b[A\n",
      "Current eval Loss 0.16:  22%|██▏       | 11/50 [00:16<00:58,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.154:  22%|██▏       | 11/50 [00:17<00:58,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.154:  24%|██▍       | 12/50 [00:17<00:56,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.34:  24%|██▍       | 12/50 [00:19<00:56,  1.49s/it] \u001b[A\n",
      "Current eval Loss 0.34:  26%|██▌       | 13/50 [00:19<00:54,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.416:  26%|██▌       | 13/50 [00:20<00:54,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.416:  28%|██▊       | 14/50 [00:20<00:53,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.108:  28%|██▊       | 14/50 [00:22<00:53,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.108:  30%|███       | 15/50 [00:22<00:52,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.099:  30%|███       | 15/50 [00:23<00:52,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.099:  32%|███▏      | 16/50 [00:23<00:50,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.103:  32%|███▏      | 16/50 [00:25<00:50,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.103:  34%|███▍      | 17/50 [00:25<00:49,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.206:  34%|███▍      | 17/50 [00:26<00:49,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.206:  36%|███▌      | 18/50 [00:26<00:47,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.129:  36%|███▌      | 18/50 [00:28<00:47,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.129:  38%|███▊      | 19/50 [00:28<00:46,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.27:  38%|███▊      | 19/50 [00:29<00:46,  1.49s/it] \u001b[A\n",
      "Current eval Loss 0.27:  40%|████      | 20/50 [00:29<00:44,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.239:  40%|████      | 20/50 [00:31<00:44,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.239:  42%|████▏     | 21/50 [00:31<00:42,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.133:  42%|████▏     | 21/50 [00:32<00:42,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.133:  44%|████▍     | 22/50 [00:32<00:41,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.132:  44%|████▍     | 22/50 [00:34<00:41,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.132:  46%|████▌     | 23/50 [00:34<00:39,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.124:  46%|████▌     | 23/50 [00:35<00:39,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.124:  48%|████▊     | 24/50 [00:35<00:40,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.089:  48%|████▊     | 24/50 [00:37<00:40,  1.54s/it]\u001b[A\n",
      "Current eval Loss 0.089:  50%|█████     | 25/50 [00:37<00:39,  1.59s/it]\u001b[A\n",
      "Current eval Loss 0.103:  50%|█████     | 25/50 [00:39<00:39,  1.59s/it]\u001b[A\n",
      "Current eval Loss 0.103:  52%|█████▏    | 26/50 [00:39<00:37,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.089:  52%|█████▏    | 26/50 [00:40<00:37,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.089:  54%|█████▍    | 27/50 [00:40<00:35,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.088:  54%|█████▍    | 27/50 [00:41<00:35,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.088:  56%|█████▌    | 28/50 [00:41<00:33,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.23:  56%|█████▌    | 28/50 [00:43<00:33,  1.51s/it] \u001b[A\n",
      "Current eval Loss 0.23:  58%|█████▊    | 29/50 [00:43<00:31,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.108:  58%|█████▊    | 29/50 [00:44<00:31,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.108:  60%|██████    | 30/50 [00:44<00:29,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.11:  60%|██████    | 30/50 [00:46<00:29,  1.48s/it] \u001b[A\n",
      "Current eval Loss 0.11:  62%|██████▏   | 31/50 [00:46<00:28,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.166:  62%|██████▏   | 31/50 [00:47<00:28,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.166:  64%|██████▍   | 32/50 [00:47<00:26,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.095:  64%|██████▍   | 32/50 [00:49<00:26,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.095:  66%|██████▌   | 33/50 [00:49<00:25,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.198:  66%|██████▌   | 33/50 [00:50<00:25,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.198:  68%|██████▊   | 34/50 [00:50<00:23,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.123:  68%|██████▊   | 34/50 [00:52<00:23,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.123:  70%|███████   | 35/50 [00:52<00:21,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.111:  70%|███████   | 35/50 [00:53<00:21,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.111:  72%|███████▏  | 36/50 [00:53<00:20,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.499:  72%|███████▏  | 36/50 [00:55<00:20,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.499:  74%|███████▍  | 37/50 [00:55<00:19,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.174:  74%|███████▍  | 37/50 [00:56<00:19,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.174:  76%|███████▌  | 38/50 [00:56<00:17,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.207:  76%|███████▌  | 38/50 [00:58<00:17,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.207:  78%|███████▊  | 39/50 [00:58<00:15,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.11:  78%|███████▊  | 39/50 [00:59<00:15,  1.45s/it] \u001b[A\n",
      "Current eval Loss 0.11:  80%|████████  | 40/50 [00:59<00:14,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.051:  80%|████████  | 40/50 [01:00<00:14,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.051:  82%|████████▏ | 41/50 [01:00<00:13,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.277:  82%|████████▏ | 41/50 [01:02<00:13,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.277:  84%|████████▍ | 42/50 [01:02<00:11,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.095:  84%|████████▍ | 42/50 [01:03<00:11,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.095:  86%|████████▌ | 43/50 [01:03<00:10,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.079:  86%|████████▌ | 43/50 [01:05<00:10,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.079:  88%|████████▊ | 44/50 [01:05<00:08,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.233:  88%|████████▊ | 44/50 [01:06<00:08,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.233:  90%|█████████ | 45/50 [01:06<00:07,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.107:  90%|█████████ | 45/50 [01:08<00:07,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.107:  92%|█████████▏| 46/50 [01:08<00:05,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.15:  92%|█████████▏| 46/50 [01:09<00:05,  1.47s/it] \u001b[A\n",
      "Current eval Loss 0.15:  94%|█████████▍| 47/50 [01:09<00:04,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.075:  94%|█████████▍| 47/50 [01:11<00:04,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.075:  96%|█████████▌| 48/50 [01:11<00:02,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.316:  96%|█████████▌| 48/50 [01:12<00:02,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.316:  98%|█████████▊| 49/50 [01:12<00:01,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.117:  98%|█████████▊| 49/50 [01:14<00:01,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.117: 100%|██████████| 50/50 [01:14<00:00,  1.49s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.135:   0%|          | 0/13 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.135:   8%|▊         | 1/13 [00:01<00:17,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.371:   8%|▊         | 1/13 [00:02<00:17,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.371:  15%|█▌        | 2/13 [00:02<00:16,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.103:  15%|█▌        | 2/13 [00:04<00:16,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.103:  23%|██▎       | 3/13 [00:04<00:14,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.22:  23%|██▎       | 3/13 [00:05<00:14,  1.48s/it] \u001b[A\n",
      "Current eval Loss 0.22:  31%|███       | 4/13 [00:05<00:13,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.517:  31%|███       | 4/13 [00:07<00:13,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.517:  38%|███▊      | 5/13 [00:07<00:12,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.272:  38%|███▊      | 5/13 [00:09<00:12,  1.55s/it]\u001b[A\n",
      "Current eval Loss 0.272:  46%|████▌     | 6/13 [00:09<00:10,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.283:  46%|████▌     | 6/13 [00:10<00:10,  1.53s/it]\u001b[A\n",
      "Current eval Loss 0.283:  54%|█████▍    | 7/13 [00:10<00:09,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.492:  54%|█████▍    | 7/13 [00:12<00:09,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.492:  62%|██████▏   | 8/13 [00:12<00:07,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.503:  62%|██████▏   | 8/13 [00:13<00:07,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.503:  69%|██████▉   | 9/13 [00:13<00:05,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.296:  69%|██████▉   | 9/13 [00:15<00:05,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.296:  77%|███████▋  | 10/13 [00:15<00:04,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.35:  77%|███████▋  | 10/13 [00:16<00:04,  1.49s/it] \u001b[A\n",
      "Current eval Loss 0.35:  85%|████████▍ | 11/13 [00:16<00:02,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.392:  85%|████████▍ | 11/13 [00:18<00:02,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.392:  92%|█████████▏| 12/13 [00:18<00:01,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.212:  92%|█████████▏| 12/13 [00:18<00:01,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.212: 100%|██████████| 13/13 [00:18<00:00,  1.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.17 Train metric = 0.949 Val loss = 0.319 Val metric = 0.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Current training Loss 0.338:   0%|          | 0/50 [00:06<?, ?it/s]\u001b[A\n",
      "Current training Loss 0.338:   2%|▏         | 1/50 [00:06<05:33,  6.80s/it]\u001b[A\n",
      "Current training Loss 0.127:   2%|▏         | 1/50 [00:12<05:33,  6.80s/it]\u001b[A\n",
      "Current training Loss 0.127:   4%|▍         | 2/50 [00:12<05:10,  6.47s/it]\u001b[A\n",
      "Current training Loss 0.104:   4%|▍         | 2/50 [00:18<05:10,  6.47s/it]\u001b[A\n",
      "Current training Loss 0.104:   6%|▌         | 3/50 [00:18<04:53,  6.24s/it]\u001b[A\n",
      "Current training Loss 0.292:   6%|▌         | 3/50 [00:23<04:53,  6.24s/it]\u001b[A\n",
      "Current training Loss 0.292:   8%|▊         | 4/50 [00:23<04:38,  6.06s/it]\u001b[A\n",
      "Current training Loss 0.171:   8%|▊         | 4/50 [00:29<04:38,  6.06s/it]\u001b[A\n",
      "Current training Loss 0.171:  10%|█         | 5/50 [00:29<04:25,  5.91s/it]\u001b[A\n",
      "Current training Loss 0.251:  10%|█         | 5/50 [00:35<04:25,  5.91s/it]\u001b[A\n",
      "Current training Loss 0.251:  12%|█▏        | 6/50 [00:35<04:18,  5.86s/it]\u001b[A\n",
      "Current training Loss 0.422:  12%|█▏        | 6/50 [00:41<04:18,  5.86s/it]\u001b[A\n",
      "Current training Loss 0.422:  14%|█▍        | 7/50 [00:41<04:19,  6.03s/it]\u001b[A\n",
      "Current training Loss 0.22:  14%|█▍        | 7/50 [00:50<04:19,  6.03s/it] \u001b[A\n",
      "Current training Loss 0.22:  16%|█▌        | 8/50 [00:50<04:55,  7.04s/it]\u001b[A\n",
      "Current training Loss 0.134:  16%|█▌        | 8/50 [00:57<04:55,  7.04s/it]\u001b[A\n",
      "Current training Loss 0.134:  18%|█▊        | 9/50 [00:57<04:46,  6.98s/it]\u001b[A\n",
      "Current training Loss 0.242:  18%|█▊        | 9/50 [01:03<04:46,  6.98s/it]\u001b[A\n",
      "Current training Loss 0.242:  20%|██        | 10/50 [01:03<04:24,  6.61s/it]\u001b[A\n",
      "Current training Loss 0.228:  20%|██        | 10/50 [01:09<04:24,  6.61s/it]\u001b[A\n",
      "Current training Loss 0.228:  22%|██▏       | 11/50 [01:09<04:08,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.209:  22%|██▏       | 11/50 [01:15<04:08,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.209:  24%|██▍       | 12/50 [01:15<03:55,  6.20s/it]\u001b[A\n",
      "Current training Loss 0.386:  24%|██▍       | 12/50 [01:20<03:55,  6.20s/it]\u001b[A\n",
      "Current training Loss 0.386:  26%|██▌       | 13/50 [01:20<03:43,  6.05s/it]\u001b[A\n",
      "Current training Loss 0.52:  26%|██▌       | 13/50 [01:26<03:43,  6.05s/it] \u001b[A\n",
      "Current training Loss 0.52:  28%|██▊       | 14/50 [01:26<03:34,  5.96s/it]\u001b[A\n",
      "Current training Loss 0.094:  28%|██▊       | 14/50 [01:32<03:34,  5.96s/it]\u001b[A\n",
      "Current training Loss 0.094:  30%|███       | 15/50 [01:32<03:26,  5.89s/it]\u001b[A\n",
      "Current training Loss 0.106:  30%|███       | 15/50 [01:37<03:26,  5.89s/it]\u001b[A\n",
      "Current training Loss 0.106:  32%|███▏      | 16/50 [01:37<03:17,  5.81s/it]\u001b[A\n",
      "Current training Loss 0.137:  32%|███▏      | 16/50 [01:43<03:17,  5.81s/it]\u001b[A\n",
      "Current training Loss 0.137:  34%|███▍      | 17/50 [01:43<03:11,  5.80s/it]\u001b[A\n",
      "Current training Loss 0.147:  34%|███▍      | 17/50 [01:49<03:11,  5.80s/it]\u001b[A\n",
      "Current training Loss 0.147:  36%|███▌      | 18/50 [01:49<03:04,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.122:  36%|███▌      | 18/50 [01:55<03:04,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.122:  38%|███▊      | 19/50 [01:55<02:59,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.303:  38%|███▊      | 19/50 [02:00<02:59,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.303:  40%|████      | 20/50 [02:00<02:51,  5.73s/it]\u001b[A\n",
      "Current training Loss 0.324:  40%|████      | 20/50 [02:06<02:51,  5.73s/it]\u001b[A\n",
      "Current training Loss 0.324:  42%|████▏     | 21/50 [02:06<02:45,  5.72s/it]\u001b[A\n",
      "Current training Loss 0.152:  42%|████▏     | 21/50 [02:12<02:45,  5.72s/it]\u001b[A\n",
      "Current training Loss 0.152:  44%|████▍     | 22/50 [02:12<02:39,  5.70s/it]\u001b[A\n",
      "Current training Loss 0.113:  44%|████▍     | 22/50 [02:17<02:39,  5.70s/it]\u001b[A\n",
      "Current training Loss 0.113:  46%|████▌     | 23/50 [02:17<02:33,  5.69s/it]\u001b[A\n",
      "Current training Loss 0.042:  46%|████▌     | 23/50 [02:23<02:33,  5.69s/it]\u001b[A\n",
      "Current training Loss 0.042:  48%|████▊     | 24/50 [02:23<02:27,  5.68s/it]\u001b[A\n",
      "Current training Loss 0.056:  48%|████▊     | 24/50 [02:29<02:27,  5.68s/it]\u001b[A\n",
      "Current training Loss 0.056:  50%|█████     | 25/50 [02:29<02:21,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.201:  50%|█████     | 25/50 [02:34<02:21,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.201:  52%|█████▏    | 26/50 [02:34<02:15,  5.66s/it]\u001b[A\n",
      "Current training Loss 0.181:  52%|█████▏    | 26/50 [02:40<02:15,  5.66s/it]\u001b[A\n",
      "Current training Loss 0.181:  54%|█████▍    | 27/50 [02:40<02:12,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.108:  54%|█████▍    | 27/50 [02:47<02:12,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.108:  56%|█████▌    | 28/50 [02:47<02:10,  5.93s/it]\u001b[A\n",
      "Current training Loss 0.271:  56%|█████▌    | 28/50 [02:53<02:10,  5.93s/it]\u001b[A\n",
      "Current training Loss 0.271:  58%|█████▊    | 29/50 [02:53<02:05,  5.99s/it]\u001b[A\n",
      "Current training Loss 0.079:  58%|█████▊    | 29/50 [02:59<02:05,  5.99s/it]\u001b[A\n",
      "Current training Loss 0.079:  60%|██████    | 30/50 [02:59<01:58,  5.91s/it]\u001b[A\n",
      "Current training Loss 0.061:  60%|██████    | 30/50 [03:04<01:58,  5.91s/it]\u001b[A\n",
      "Current training Loss 0.061:  62%|██████▏   | 31/50 [03:05<01:52,  5.93s/it]\u001b[A\n",
      "Current training Loss 0.182:  62%|██████▏   | 31/50 [03:10<01:52,  5.93s/it]\u001b[A\n",
      "Current training Loss 0.182:  64%|██████▍   | 32/50 [03:10<01:47,  5.95s/it]\u001b[A\n",
      "Current training Loss 0.052:  64%|██████▍   | 32/50 [03:18<01:47,  5.95s/it]\u001b[A\n",
      "Current training Loss 0.052:  66%|██████▌   | 33/50 [03:18<01:47,  6.33s/it]\u001b[A\n",
      "Current training Loss 0.269:  66%|██████▌   | 33/50 [03:25<01:47,  6.33s/it]\u001b[A\n",
      "Current training Loss 0.269:  68%|██████▊   | 34/50 [03:25<01:46,  6.64s/it]\u001b[A\n",
      "Current training Loss 0.09:  68%|██████▊   | 34/50 [03:31<01:46,  6.64s/it] \u001b[A\n",
      "Current training Loss 0.09:  70%|███████   | 35/50 [03:31<01:36,  6.46s/it]\u001b[A\n",
      "Current training Loss 0.07:  70%|███████   | 35/50 [03:37<01:36,  6.46s/it]\u001b[A\n",
      "Current training Loss 0.07:  72%|███████▏  | 36/50 [03:37<01:27,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.631:  72%|███████▏  | 36/50 [03:43<01:27,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.631:  74%|███████▍  | 37/50 [03:43<01:19,  6.14s/it]\u001b[A\n",
      "Current training Loss 0.151:  74%|███████▍  | 37/50 [03:49<01:19,  6.14s/it]\u001b[A\n",
      "Current training Loss 0.151:  76%|███████▌  | 38/50 [03:49<01:12,  6.04s/it]\u001b[A\n",
      "Current training Loss 0.256:  76%|███████▌  | 38/50 [03:55<01:12,  6.04s/it]\u001b[A\n",
      "Current training Loss 0.256:  78%|███████▊  | 39/50 [03:55<01:06,  6.00s/it]\u001b[A\n",
      "Current training Loss 0.256:  78%|███████▊  | 39/50 [04:00<01:06,  6.00s/it]\u001b[A\n",
      "Current training Loss 0.256:  80%|████████  | 40/50 [04:00<00:58,  5.88s/it]\u001b[A\n",
      "Current training Loss 0.024:  80%|████████  | 40/50 [04:06<00:58,  5.88s/it]\u001b[A\n",
      "Current training Loss 0.024:  82%|████████▏ | 41/50 [04:06<00:52,  5.84s/it]\u001b[A\n",
      "Current training Loss 0.242:  82%|████████▏ | 41/50 [04:12<00:52,  5.84s/it]\u001b[A\n",
      "Current training Loss 0.242:  84%|████████▍ | 42/50 [04:12<00:47,  5.91s/it]\u001b[A\n",
      "Current training Loss 0.099:  84%|████████▍ | 42/50 [04:18<00:47,  5.91s/it]\u001b[A\n",
      "Current training Loss 0.099:  86%|████████▌ | 43/50 [04:18<00:41,  5.88s/it]\u001b[A\n",
      "Current training Loss 0.037:  86%|████████▌ | 43/50 [04:23<00:41,  5.88s/it]\u001b[A\n",
      "Current training Loss 0.037:  88%|████████▊ | 44/50 [04:23<00:35,  5.84s/it]\u001b[A\n",
      "Current training Loss 0.183:  88%|████████▊ | 44/50 [04:29<00:35,  5.84s/it]\u001b[A\n",
      "Current training Loss 0.183:  90%|█████████ | 45/50 [04:29<00:28,  5.80s/it]\u001b[A\n",
      "Current training Loss 0.048:  90%|█████████ | 45/50 [04:35<00:28,  5.80s/it]\u001b[A\n",
      "Current training Loss 0.048:  92%|█████████▏| 46/50 [04:35<00:23,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.079:  92%|█████████▏| 46/50 [04:41<00:23,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.079:  94%|█████████▍| 47/50 [04:41<00:17,  5.76s/it]\u001b[A\n",
      "Current training Loss 0.038:  94%|█████████▍| 47/50 [04:46<00:17,  5.76s/it]\u001b[A\n",
      "Current training Loss 0.038:  96%|█████████▌| 48/50 [04:46<00:11,  5.74s/it]\u001b[A\n",
      "Current training Loss 0.309:  96%|█████████▌| 48/50 [04:52<00:11,  5.74s/it]\u001b[A\n",
      "Current training Loss 0.309:  98%|█████████▊| 49/50 [04:52<00:05,  5.73s/it]\u001b[A\n",
      "Current training Loss 0.06:  98%|█████████▊| 49/50 [04:58<00:05,  5.73s/it] \u001b[A\n",
      "Current training Loss 0.06: 100%|██████████| 50/50 [04:58<00:00,  5.97s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.238:   0%|          | 0/50 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.238:   2%|▏         | 1/50 [00:01<01:12,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.073:   2%|▏         | 1/50 [00:02<01:12,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.073:   4%|▍         | 2/50 [00:02<01:10,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.066:   4%|▍         | 2/50 [00:04<01:10,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.066:   6%|▌         | 3/50 [00:04<01:08,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.129:   6%|▌         | 3/50 [00:05<01:08,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.129:   8%|▊         | 4/50 [00:05<01:07,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.101:   8%|▊         | 4/50 [00:07<01:07,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.101:  10%|█         | 5/50 [00:07<01:05,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.148:  10%|█         | 5/50 [00:08<01:05,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.148:  12%|█▏        | 6/50 [00:08<01:05,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.212:  12%|█▏        | 6/50 [00:10<01:05,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.212:  14%|█▍        | 7/50 [00:10<01:03,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.033:  14%|█▍        | 7/50 [00:11<01:03,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.033:  16%|█▌        | 8/50 [00:11<01:02,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.062:  16%|█▌        | 8/50 [00:13<01:02,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.062:  18%|█▊        | 9/50 [00:13<01:01,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.079:  18%|█▊        | 9/50 [00:14<01:01,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.079:  20%|██        | 10/50 [00:14<01:00,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.106:  20%|██        | 10/50 [00:16<01:00,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.106:  22%|██▏       | 11/50 [00:16<00:58,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.102:  22%|██▏       | 11/50 [00:17<00:58,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.102:  24%|██▍       | 12/50 [00:17<00:57,  1.52s/it]\u001b[A\n",
      "Current eval Loss 0.103:  24%|██▍       | 12/50 [00:19<00:57,  1.52s/it]\u001b[A\n",
      "Current eval Loss 0.103:  26%|██▌       | 13/50 [00:19<00:55,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.418:  26%|██▌       | 13/50 [00:20<00:55,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.418:  28%|██▊       | 14/50 [00:20<00:53,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.053:  28%|██▊       | 14/50 [00:22<00:53,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.053:  30%|███       | 15/50 [00:22<00:52,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.045:  30%|███       | 15/50 [00:23<00:52,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.045:  32%|███▏      | 16/50 [00:23<00:50,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.046:  32%|███▏      | 16/50 [00:25<00:50,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.046:  34%|███▍      | 17/50 [00:25<00:48,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.082:  34%|███▍      | 17/50 [00:26<00:48,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.082:  36%|███▌      | 18/50 [00:26<00:47,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.066:  36%|███▌      | 18/50 [00:28<00:47,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.066:  38%|███▊      | 19/50 [00:28<00:46,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.089:  38%|███▊      | 19/50 [00:29<00:46,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.089:  40%|████      | 20/50 [00:29<00:44,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.146:  40%|████      | 20/50 [00:31<00:44,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.146:  42%|████▏     | 21/50 [00:31<00:42,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.1:  42%|████▏     | 21/50 [00:32<00:42,  1.47s/it]  \u001b[A\n",
      "Current eval Loss 0.1:  44%|████▍     | 22/50 [00:32<00:41,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.086:  44%|████▍     | 22/50 [00:34<00:41,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.086:  46%|████▌     | 23/50 [00:34<00:40,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.091:  46%|████▌     | 23/50 [00:35<00:40,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.091:  48%|████▊     | 24/50 [00:35<00:38,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.035:  48%|████▊     | 24/50 [00:37<00:38,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.035:  50%|█████     | 25/50 [00:37<00:37,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.042:  50%|█████     | 25/50 [00:38<00:37,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.042:  52%|█████▏    | 26/50 [00:38<00:35,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.034:  52%|█████▏    | 26/50 [00:40<00:35,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.034:  54%|█████▍    | 27/50 [00:40<00:33,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.045:  54%|█████▍    | 27/50 [00:41<00:33,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.045:  56%|█████▌    | 28/50 [00:41<00:32,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.153:  56%|█████▌    | 28/50 [00:43<00:32,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.153:  58%|█████▊    | 29/50 [00:43<00:30,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.061:  58%|█████▊    | 29/50 [00:44<00:30,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.061:  60%|██████    | 30/50 [00:44<00:29,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.047:  60%|██████    | 30/50 [00:46<00:29,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.047:  62%|██████▏   | 31/50 [00:46<00:28,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.063:  62%|██████▏   | 31/50 [00:47<00:28,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.063:  64%|██████▍   | 32/50 [00:47<00:26,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.047:  64%|██████▍   | 32/50 [00:48<00:26,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.047:  66%|██████▌   | 33/50 [00:48<00:25,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.125:  66%|██████▌   | 33/50 [00:50<00:25,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.125:  68%|██████▊   | 34/50 [00:50<00:23,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.076:  68%|██████▊   | 34/50 [00:51<00:23,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.076:  70%|███████   | 35/50 [00:51<00:22,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.055:  70%|███████   | 35/50 [00:53<00:22,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.055:  72%|███████▏  | 36/50 [00:53<00:20,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.482:  72%|███████▏  | 36/50 [00:54<00:20,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.482:  74%|███████▍  | 37/50 [00:54<00:19,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.102:  74%|███████▍  | 37/50 [00:56<00:19,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.102:  76%|███████▌  | 38/50 [00:56<00:17,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.211:  76%|███████▌  | 38/50 [00:57<00:17,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.211:  78%|███████▊  | 39/50 [00:57<00:16,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.057:  78%|███████▊  | 39/50 [00:59<00:16,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.057:  80%|████████  | 40/50 [00:59<00:14,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.023:  80%|████████  | 40/50 [01:00<00:14,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.023:  82%|████████▏ | 41/50 [01:00<00:13,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.148:  82%|████████▏ | 41/50 [01:02<00:13,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.148:  84%|████████▍ | 42/50 [01:02<00:11,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.052:  84%|████████▍ | 42/50 [01:03<00:11,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.052:  86%|████████▌ | 43/50 [01:03<00:10,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.025:  86%|████████▌ | 43/50 [01:05<00:10,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.025:  88%|████████▊ | 44/50 [01:05<00:08,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.131:  88%|████████▊ | 44/50 [01:06<00:08,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.131:  90%|█████████ | 45/50 [01:06<00:07,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.039:  90%|█████████ | 45/50 [01:08<00:07,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.039:  92%|█████████▏| 46/50 [01:08<00:05,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.072:  92%|█████████▏| 46/50 [01:09<00:05,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.072:  94%|█████████▍| 47/50 [01:09<00:04,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.042:  94%|█████████▍| 47/50 [01:11<00:04,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.042:  96%|█████████▌| 48/50 [01:11<00:02,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.056:  96%|█████████▌| 48/50 [01:12<00:02,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.056:  98%|█████████▊| 49/50 [01:12<00:01,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.063:  98%|█████████▊| 49/50 [01:14<00:01,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.063: 100%|██████████| 50/50 [01:14<00:00,  1.48s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.203:   0%|          | 0/13 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.203:   8%|▊         | 1/13 [00:01<00:17,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.563:   8%|▊         | 1/13 [00:02<00:17,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.563:  15%|█▌        | 2/13 [00:02<00:15,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.057:  15%|█▌        | 2/13 [00:04<00:15,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.057:  23%|██▎       | 3/13 [00:04<00:14,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.257:  23%|██▎       | 3/13 [00:05<00:14,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.257:  31%|███       | 4/13 [00:05<00:13,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.494:  31%|███       | 4/13 [00:07<00:13,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.494:  38%|███▊      | 5/13 [00:07<00:11,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.463:  38%|███▊      | 5/13 [00:08<00:11,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.463:  46%|████▌     | 6/13 [00:08<00:10,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.662:  46%|████▌     | 6/13 [00:10<00:10,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.662:  54%|█████▍    | 7/13 [00:10<00:08,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.485:  54%|█████▍    | 7/13 [00:11<00:08,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.485:  62%|██████▏   | 8/13 [00:11<00:07,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.528:  62%|██████▏   | 8/13 [00:13<00:07,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.528:  69%|██████▉   | 9/13 [00:13<00:05,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.351:  69%|██████▉   | 9/13 [00:14<00:05,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.351:  77%|███████▋  | 10/13 [00:14<00:04,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.345:  77%|███████▋  | 10/13 [00:16<00:04,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.345:  85%|████████▍ | 11/13 [00:16<00:02,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.479:  85%|████████▍ | 11/13 [00:17<00:02,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.479:  92%|█████████▏| 12/13 [00:17<00:01,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.326:  92%|█████████▏| 12/13 [00:18<00:01,  1.51s/it]\u001b[A\n",
      "Current eval Loss 0.326: 100%|██████████| 13/13 [00:18<00:00,  1.43s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.099 Train metric = 0.972 Val loss = 0.401 Val metric = 0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current training Loss 0.241:   0%|          | 0/50 [00:05<?, ?it/s]\u001b[A\n",
      "Current training Loss 0.241:   2%|▏         | 1/50 [00:05<04:48,  5.88s/it]\u001b[A\n",
      "Current training Loss 0.079:   2%|▏         | 1/50 [00:11<04:48,  5.88s/it]\u001b[A\n",
      "Current training Loss 0.079:   4%|▍         | 2/50 [00:11<04:41,  5.87s/it]\u001b[A\n",
      "Current training Loss 0.081:   4%|▍         | 2/50 [00:17<04:41,  5.87s/it]\u001b[A\n",
      "Current training Loss 0.081:   6%|▌         | 3/50 [00:17<04:32,  5.80s/it]\u001b[A\n",
      "Current training Loss 0.133:   6%|▌         | 3/50 [00:23<04:32,  5.80s/it]\u001b[A\n",
      "Current training Loss 0.133:   8%|▊         | 4/50 [00:23<04:26,  5.79s/it]\u001b[A\n",
      "Current training Loss 0.108:   8%|▊         | 4/50 [00:28<04:26,  5.79s/it]\u001b[A\n",
      "Current training Loss 0.108:  10%|█         | 5/50 [00:28<04:18,  5.75s/it]\u001b[A\n",
      "Current training Loss 0.167:  10%|█         | 5/50 [00:34<04:18,  5.75s/it]\u001b[A\n",
      "Current training Loss 0.167:  12%|█▏        | 6/50 [00:34<04:11,  5.72s/it]\u001b[A\n",
      "Current training Loss 0.308:  12%|█▏        | 6/50 [00:40<04:11,  5.72s/it]\u001b[A\n",
      "Current training Loss 0.308:  14%|█▍        | 7/50 [00:40<04:05,  5.70s/it]\u001b[A\n",
      "Current training Loss 0.047:  14%|█▍        | 7/50 [00:45<04:05,  5.70s/it]\u001b[A\n",
      "Current training Loss 0.047:  16%|█▌        | 8/50 [00:45<03:59,  5.70s/it]\u001b[A\n",
      "Current training Loss 0.066:  16%|█▌        | 8/50 [00:51<03:59,  5.70s/it]\u001b[A\n",
      "Current training Loss 0.066:  18%|█▊        | 9/50 [00:51<03:52,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.059:  18%|█▊        | 9/50 [00:56<03:52,  5.67s/it]\u001b[A\n",
      "Current training Loss 0.059:  20%|██        | 10/50 [00:56<03:46,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.102:  20%|██        | 10/50 [01:02<03:46,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.102:  22%|██▏       | 11/50 [01:02<03:39,  5.64s/it]\u001b[A\n",
      "Current training Loss 0.108:  22%|██▏       | 11/50 [01:08<03:39,  5.64s/it]\u001b[A\n",
      "Current training Loss 0.108:  24%|██▍       | 12/50 [01:08<03:33,  5.61s/it]\u001b[A\n",
      "Current training Loss 0.14:  24%|██▍       | 12/50 [01:13<03:33,  5.61s/it] \u001b[A\n",
      "Current training Loss 0.14:  26%|██▌       | 13/50 [01:13<03:27,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.463:  26%|██▌       | 13/50 [01:19<03:27,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.463:  28%|██▊       | 14/50 [01:19<03:21,  5.61s/it]\u001b[A\n",
      "Current training Loss 0.058:  28%|██▊       | 14/50 [01:24<03:21,  5.61s/it]\u001b[A\n",
      "Current training Loss 0.058:  30%|███       | 15/50 [01:24<03:16,  5.61s/it]\u001b[A\n",
      "Current training Loss 0.19:  30%|███       | 15/50 [01:30<03:16,  5.61s/it] \u001b[A\n",
      "Current training Loss 0.19:  32%|███▏      | 16/50 [01:30<03:10,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.076:  32%|███▏      | 16/50 [01:36<03:10,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.076:  34%|███▍      | 17/50 [01:36<03:04,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.058:  34%|███▍      | 17/50 [01:43<03:04,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.058:  36%|███▌      | 18/50 [01:43<03:11,  5.98s/it]\u001b[A\n",
      "Current training Loss 0.049:  36%|███▌      | 18/50 [01:49<03:11,  5.98s/it]\u001b[A\n",
      "Current training Loss 0.049:  38%|███▊      | 19/50 [01:49<03:09,  6.10s/it]\u001b[A\n",
      "Current training Loss 0.317:  38%|███▊      | 19/50 [01:56<03:09,  6.10s/it]\u001b[A\n",
      "Current training Loss 0.317:  40%|████      | 20/50 [01:56<03:11,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.336:  40%|████      | 20/50 [02:03<03:11,  6.38s/it]\u001b[A\n",
      "Current training Loss 0.336:  42%|████▏     | 21/50 [02:03<03:12,  6.64s/it]\u001b[A\n",
      "Current training Loss 0.191:  42%|████▏     | 21/50 [02:10<03:12,  6.64s/it]\u001b[A\n",
      "Current training Loss 0.191:  44%|████▍     | 22/50 [02:10<03:10,  6.80s/it]\u001b[A\n",
      "Current training Loss 0.08:  44%|████▍     | 22/50 [02:17<03:10,  6.80s/it] \u001b[A\n",
      "Current training Loss 0.08:  46%|████▌     | 23/50 [02:17<03:00,  6.70s/it]\u001b[A\n",
      "Current training Loss 0.189:  46%|████▌     | 23/50 [02:22<03:00,  6.70s/it]\u001b[A\n",
      "Current training Loss 0.189:  48%|████▊     | 24/50 [02:22<02:45,  6.37s/it]\u001b[A\n",
      "Current training Loss 0.032:  48%|████▊     | 24/50 [02:28<02:45,  6.37s/it]\u001b[A\n",
      "Current training Loss 0.032:  50%|█████     | 25/50 [02:28<02:37,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.039:  50%|█████     | 25/50 [02:34<02:37,  6.28s/it]\u001b[A\n",
      "Current training Loss 0.039:  52%|█████▏    | 26/50 [02:34<02:25,  6.05s/it]\u001b[A\n",
      "Current training Loss 0.033:  52%|█████▏    | 26/50 [02:40<02:25,  6.05s/it]\u001b[A\n",
      "Current training Loss 0.033:  54%|█████▍    | 27/50 [02:40<02:15,  5.90s/it]\u001b[A\n",
      "Current training Loss 0.031:  54%|█████▍    | 27/50 [02:45<02:15,  5.90s/it]\u001b[A\n",
      "Current training Loss 0.031:  56%|█████▌    | 28/50 [02:45<02:07,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.116:  56%|█████▌    | 28/50 [02:50<02:07,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.116:  58%|█████▊    | 29/50 [02:50<01:59,  5.68s/it]\u001b[A\n",
      "Current training Loss 0.046:  58%|█████▊    | 29/50 [02:56<01:59,  5.68s/it]\u001b[A\n",
      "Current training Loss 0.046:  60%|██████    | 30/50 [02:56<01:52,  5.63s/it]\u001b[A\n",
      "Current training Loss 0.064:  60%|██████    | 30/50 [03:01<01:52,  5.63s/it]\u001b[A\n",
      "Current training Loss 0.064:  62%|██████▏   | 31/50 [03:01<01:46,  5.58s/it]\u001b[A\n",
      "Current training Loss 0.077:  62%|██████▏   | 31/50 [03:07<01:46,  5.58s/it]\u001b[A\n",
      "Current training Loss 0.077:  64%|██████▍   | 32/50 [03:07<01:40,  5.57s/it]\u001b[A\n",
      "Current training Loss 0.064:  64%|██████▍   | 32/50 [03:13<01:40,  5.57s/it]\u001b[A\n",
      "Current training Loss 0.064:  66%|██████▌   | 33/50 [03:13<01:34,  5.56s/it]\u001b[A\n",
      "Current training Loss 0.241:  66%|██████▌   | 33/50 [03:18<01:34,  5.56s/it]\u001b[A\n",
      "Current training Loss 0.241:  68%|██████▊   | 34/50 [03:18<01:28,  5.55s/it]\u001b[A\n",
      "Current training Loss 0.114:  68%|██████▊   | 34/50 [03:24<01:28,  5.55s/it]\u001b[A\n",
      "Current training Loss 0.114:  70%|███████   | 35/50 [03:24<01:23,  5.54s/it]\u001b[A\n",
      "Current training Loss 0.05:  70%|███████   | 35/50 [03:29<01:23,  5.54s/it] \u001b[A\n",
      "Current training Loss 0.05:  72%|███████▏  | 36/50 [03:29<01:17,  5.52s/it]\u001b[A\n",
      "Current training Loss 0.658:  72%|███████▏  | 36/50 [03:34<01:17,  5.52s/it]\u001b[A\n",
      "Current training Loss 0.658:  74%|███████▍  | 37/50 [03:34<01:11,  5.48s/it]\u001b[A\n",
      "Current training Loss 0.151:  74%|███████▍  | 37/50 [03:40<01:11,  5.48s/it]\u001b[A\n",
      "Current training Loss 0.151:  76%|███████▌  | 38/50 [03:40<01:05,  5.46s/it]\u001b[A\n",
      "Current training Loss 0.139:  76%|███████▌  | 38/50 [03:47<01:05,  5.46s/it]\u001b[A\n",
      "Current training Loss 0.139:  78%|███████▊  | 39/50 [03:47<01:04,  5.84s/it]\u001b[A\n",
      "Current training Loss 0.037:  78%|███████▊  | 39/50 [03:55<01:04,  5.84s/it]\u001b[A\n",
      "Current training Loss 0.037:  80%|████████  | 40/50 [03:55<01:06,  6.62s/it]\u001b[A\n",
      "Current training Loss 0.014:  80%|████████  | 40/50 [04:01<01:06,  6.62s/it]\u001b[A\n",
      "Current training Loss 0.014:  82%|████████▏ | 41/50 [04:01<00:58,  6.51s/it]\u001b[A\n",
      "Current training Loss 0.145:  82%|████████▏ | 41/50 [04:07<00:58,  6.51s/it]\u001b[A\n",
      "Current training Loss 0.145:  84%|████████▍ | 42/50 [04:07<00:51,  6.39s/it]\u001b[A\n",
      "Current training Loss 0.016:  84%|████████▍ | 42/50 [04:14<00:51,  6.39s/it]\u001b[A\n",
      "Current training Loss 0.016:  86%|████████▌ | 43/50 [04:14<00:44,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.015:  86%|████████▌ | 43/50 [04:19<00:44,  6.36s/it]\u001b[A\n",
      "Current training Loss 0.015:  88%|████████▊ | 44/50 [04:19<00:36,  6.10s/it]\u001b[A\n",
      "Current training Loss 0.019:  88%|████████▊ | 44/50 [04:25<00:36,  6.10s/it]\u001b[A\n",
      "Current training Loss 0.019:  90%|█████████ | 45/50 [04:25<00:29,  5.90s/it]\u001b[A\n",
      "Current training Loss 0.017:  90%|█████████ | 45/50 [04:30<00:29,  5.90s/it]\u001b[A\n",
      "Current training Loss 0.017:  92%|█████████▏| 46/50 [04:30<00:23,  5.78s/it]\u001b[A\n",
      "Current training Loss 0.02:  92%|█████████▏| 46/50 [04:36<00:23,  5.78s/it] \u001b[A\n",
      "Current training Loss 0.02:  94%|█████████▍| 47/50 [04:36<00:17,  5.68s/it]\u001b[A\n",
      "Current training Loss 0.019:  94%|█████████▍| 47/50 [04:41<00:17,  5.68s/it]\u001b[A\n",
      "Current training Loss 0.019:  96%|█████████▌| 48/50 [04:41<00:11,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.189:  96%|█████████▌| 48/50 [04:46<00:11,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.189:  98%|█████████▊| 49/50 [04:46<00:05,  5.57s/it]\u001b[A\n",
      "Current training Loss 0.339:  98%|█████████▊| 49/50 [04:52<00:05,  5.57s/it]\u001b[A\n",
      "Current training Loss 0.339: 100%|██████████| 50/50 [04:52<00:00,  5.85s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.284:   0%|          | 0/50 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.284:   2%|▏         | 1/50 [00:01<01:08,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.168:   2%|▏         | 1/50 [00:02<01:08,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.168:   4%|▍         | 2/50 [00:02<01:07,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.22:   4%|▍         | 2/50 [00:04<01:07,  1.41s/it] \u001b[A\n",
      "Current eval Loss 0.22:   6%|▌         | 3/50 [00:04<01:05,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.608:   6%|▌         | 3/50 [00:05<01:05,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.608:   8%|▊         | 4/50 [00:05<01:04,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.138:   8%|▊         | 4/50 [00:07<01:04,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.138:  10%|█         | 5/50 [00:07<01:02,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.021:  10%|█         | 5/50 [00:08<01:02,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.021:  12%|█▏        | 6/50 [00:08<01:01,  1.39s/it]\u001b[A\n",
      "Current eval Loss 0.135:  12%|█▏        | 6/50 [00:09<01:01,  1.39s/it]\u001b[A\n",
      "Current eval Loss 0.135:  14%|█▍        | 7/50 [00:09<00:59,  1.39s/it]\u001b[A\n",
      "Current eval Loss 0.096:  14%|█▍        | 7/50 [00:11<00:59,  1.39s/it]\u001b[A\n",
      "Current eval Loss 0.096:  16%|█▌        | 8/50 [00:11<00:58,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.131:  16%|█▌        | 8/50 [00:12<00:58,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.131:  18%|█▊        | 9/50 [00:12<00:57,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.176:  18%|█▊        | 9/50 [00:13<00:57,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.176:  20%|██        | 10/50 [00:13<00:56,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.111:  20%|██        | 10/50 [00:15<00:56,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.111:  22%|██▏       | 11/50 [00:15<00:55,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.099:  22%|██▏       | 11/50 [00:16<00:55,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.099:  24%|██▍       | 12/50 [00:16<00:53,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.099:  24%|██▍       | 12/50 [00:18<00:53,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.099:  26%|██▌       | 13/50 [00:18<00:52,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.657:  26%|██▌       | 13/50 [00:19<00:52,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.657:  28%|██▊       | 14/50 [00:19<00:51,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.04:  28%|██▊       | 14/50 [00:21<00:51,  1.42s/it] \u001b[A\n",
      "Current eval Loss 0.04:  30%|███       | 15/50 [00:21<00:49,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.021:  30%|███       | 15/50 [00:22<00:49,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.021:  32%|███▏      | 16/50 [00:22<00:48,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.075:  32%|███▏      | 16/50 [00:23<00:48,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.075:  34%|███▍      | 17/50 [00:23<00:46,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.1:  34%|███▍      | 17/50 [00:25<00:46,  1.42s/it]  \u001b[A\n",
      "Current eval Loss 0.1:  36%|███▌      | 18/50 [00:25<00:45,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.017:  36%|███▌      | 18/50 [00:26<00:45,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.017:  38%|███▊      | 19/50 [00:26<00:43,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.104:  38%|███▊      | 19/50 [00:28<00:43,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.104:  40%|████      | 20/50 [00:28<00:42,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.236:  40%|████      | 20/50 [00:29<00:42,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.236:  42%|████▏     | 21/50 [00:29<00:40,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.194:  42%|████▏     | 21/50 [00:30<00:40,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.194:  44%|████▍     | 22/50 [00:30<00:39,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.178:  44%|████▍     | 22/50 [00:32<00:39,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.178:  46%|████▌     | 23/50 [00:32<00:38,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.238:  46%|████▌     | 23/50 [00:33<00:38,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.238:  48%|████▊     | 24/50 [00:33<00:36,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.15:  48%|████▊     | 24/50 [00:35<00:36,  1.41s/it] \u001b[A\n",
      "Current eval Loss 0.15:  50%|█████     | 25/50 [00:35<00:35,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.052:  50%|█████     | 25/50 [00:36<00:35,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.052:  52%|█████▏    | 26/50 [00:36<00:33,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.012:  52%|█████▏    | 26/50 [00:38<00:33,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.012:  54%|█████▍    | 27/50 [00:38<00:32,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.384:  54%|█████▍    | 27/50 [00:39<00:32,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.384:  56%|█████▌    | 28/50 [00:39<00:31,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.156:  56%|█████▌    | 28/50 [00:40<00:31,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.156:  58%|█████▊    | 29/50 [00:40<00:29,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.121:  58%|█████▊    | 29/50 [00:42<00:29,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.121:  60%|██████    | 30/50 [00:42<00:28,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.194:  60%|██████    | 30/50 [00:43<00:28,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.194:  62%|██████▏   | 31/50 [00:43<00:26,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.097:  62%|██████▏   | 31/50 [00:45<00:26,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.097:  64%|██████▍   | 32/50 [00:45<00:25,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.17:  64%|██████▍   | 32/50 [00:46<00:25,  1.41s/it] \u001b[A\n",
      "Current eval Loss 0.17:  66%|██████▌   | 33/50 [00:46<00:23,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.113:  66%|██████▌   | 33/50 [00:47<00:23,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.113:  68%|██████▊   | 34/50 [00:47<00:22,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.037:  68%|██████▊   | 34/50 [00:49<00:22,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.037:  70%|███████   | 35/50 [00:49<00:21,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.04:  70%|███████   | 35/50 [00:50<00:21,  1.41s/it] \u001b[A\n",
      "Current eval Loss 0.04:  72%|███████▏  | 36/50 [00:50<00:19,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.463:  72%|███████▏  | 36/50 [00:52<00:19,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.463:  74%|███████▍  | 37/50 [00:52<00:18,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.015:  74%|███████▍  | 37/50 [00:53<00:18,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.015:  76%|███████▌  | 38/50 [00:53<00:16,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.031:  76%|███████▌  | 38/50 [00:54<00:16,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.031:  78%|███████▊  | 39/50 [00:54<00:15,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.077:  78%|███████▊  | 39/50 [00:56<00:15,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.077:  80%|████████  | 40/50 [00:56<00:14,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.011:  80%|████████  | 40/50 [00:57<00:14,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.011:  82%|████████▏ | 41/50 [00:57<00:12,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.025:  82%|████████▏ | 41/50 [00:59<00:12,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.025:  84%|████████▍ | 42/50 [00:59<00:11,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.015:  84%|████████▍ | 42/50 [01:00<00:11,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.015:  86%|████████▌ | 43/50 [01:00<00:09,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.011:  86%|████████▌ | 43/50 [01:02<00:09,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.011:  88%|████████▊ | 44/50 [01:02<00:08,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.018:  88%|████████▊ | 44/50 [01:03<00:08,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.018:  90%|█████████ | 45/50 [01:03<00:07,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.013:  90%|█████████ | 45/50 [01:04<00:07,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.013:  92%|█████████▏| 46/50 [01:04<00:05,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.017:  92%|█████████▏| 46/50 [01:06<00:05,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.017:  94%|█████████▍| 47/50 [01:06<00:04,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.015:  94%|█████████▍| 47/50 [01:07<00:04,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.015:  96%|█████████▌| 48/50 [01:07<00:02,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.106:  96%|█████████▌| 48/50 [01:09<00:02,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.106:  98%|█████████▊| 49/50 [01:09<00:01,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.316:  98%|█████████▊| 49/50 [01:10<00:01,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.316: 100%|██████████| 50/50 [01:10<00:00,  1.41s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.675:   0%|          | 0/13 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.675:   8%|▊         | 1/13 [00:01<00:16,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.411:   8%|▊         | 1/13 [00:02<00:16,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.411:  15%|█▌        | 2/13 [00:02<00:15,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.422:  15%|█▌        | 2/13 [00:04<00:15,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.422:  23%|██▎       | 3/13 [00:04<00:14,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.726:  23%|██▎       | 3/13 [00:05<00:14,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.726:  31%|███       | 4/13 [00:05<00:12,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.683:  31%|███       | 4/13 [00:07<00:12,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.683:  38%|███▊      | 5/13 [00:07<00:11,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.965:  38%|███▊      | 5/13 [00:08<00:11,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.965:  46%|████▌     | 6/13 [00:08<00:09,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.358:  46%|████▌     | 6/13 [00:09<00:09,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.358:  54%|█████▍    | 7/13 [00:09<00:08,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.437:  54%|█████▍    | 7/13 [00:11<00:08,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.437:  62%|██████▏   | 8/13 [00:11<00:07,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.98:  62%|██████▏   | 8/13 [00:12<00:07,  1.42s/it] \u001b[A\n",
      "Current eval Loss 0.98:  69%|██████▉   | 9/13 [00:12<00:05,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.766:  69%|██████▉   | 9/13 [00:14<00:05,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.766:  77%|███████▋  | 10/13 [00:14<00:04,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.848:  77%|███████▋  | 10/13 [00:15<00:04,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.848:  85%|████████▍ | 11/13 [00:15<00:02,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.594:  85%|████████▍ | 11/13 [00:17<00:02,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.594:  92%|█████████▏| 12/13 [00:17<00:01,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.069:  92%|█████████▏| 12/13 [00:17<00:01,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.069: 100%|██████████| 13/13 [00:17<00:00,  1.37s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.136 Train metric = 0.949 Val loss = 0.61 Val metric = 0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current training Loss 0.284:   0%|          | 0/50 [00:05<?, ?it/s]\u001b[A\n",
      "Current training Loss 0.284:   2%|▏         | 1/50 [00:05<04:43,  5.80s/it]\u001b[A\n",
      "Current training Loss 0.252:   2%|▏         | 1/50 [00:11<04:43,  5.80s/it]\u001b[A\n",
      "Current training Loss 0.252:   4%|▍         | 2/50 [00:11<04:35,  5.74s/it]\u001b[A\n",
      "Current training Loss 0.128:   4%|▍         | 2/50 [00:18<04:35,  5.74s/it]\u001b[A\n",
      "Current training Loss 0.128:   6%|▌         | 3/50 [00:18<04:48,  6.13s/it]\u001b[A\n",
      "Current training Loss 0.416:   6%|▌         | 3/50 [00:25<04:48,  6.13s/it]\u001b[A\n",
      "Current training Loss 0.416:   8%|▊         | 4/50 [00:25<04:54,  6.40s/it]\u001b[A\n",
      "Current training Loss 0.084:   8%|▊         | 4/50 [00:31<04:54,  6.40s/it]\u001b[A\n",
      "Current training Loss 0.084:  10%|█         | 5/50 [00:31<04:43,  6.30s/it]\u001b[A\n",
      "Current training Loss 0.033:  10%|█         | 5/50 [00:37<04:43,  6.30s/it]\u001b[A\n",
      "Current training Loss 0.033:  12%|█▏        | 6/50 [00:37<04:26,  6.06s/it]\u001b[A\n",
      "Current training Loss 0.129:  12%|█▏        | 6/50 [00:42<04:26,  6.06s/it]\u001b[A\n",
      "Current training Loss 0.129:  14%|█▍        | 7/50 [00:42<04:13,  5.89s/it]\u001b[A\n",
      "Current training Loss 0.025:  14%|█▍        | 7/50 [00:48<04:13,  5.89s/it]\u001b[A\n",
      "Current training Loss 0.025:  16%|█▌        | 8/50 [00:48<04:02,  5.77s/it]\u001b[A\n",
      "Current training Loss 0.031:  16%|█▌        | 8/50 [00:53<04:02,  5.77s/it]\u001b[A\n",
      "Current training Loss 0.031:  18%|█▊        | 9/50 [00:53<03:53,  5.69s/it]\u001b[A\n",
      "Current training Loss 0.05:  18%|█▊        | 9/50 [00:59<03:53,  5.69s/it] \u001b[A\n",
      "Current training Loss 0.05:  20%|██        | 10/50 [00:59<03:46,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.061:  20%|██        | 10/50 [01:04<03:46,  5.65s/it]\u001b[A\n",
      "Current training Loss 0.061:  22%|██▏       | 11/50 [01:04<03:39,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.076:  22%|██▏       | 11/50 [01:10<03:39,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.076:  24%|██▍       | 12/50 [01:10<03:32,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.076:  24%|██▍       | 12/50 [01:15<03:32,  5.60s/it]\u001b[A\n",
      "Current training Loss 0.076:  26%|██▌       | 13/50 [01:15<03:25,  5.54s/it]\u001b[A\n",
      "Current training Loss 0.357:  26%|██▌       | 13/50 [01:21<03:25,  5.54s/it]\u001b[A\n",
      "Current training Loss 0.357:  28%|██▊       | 14/50 [01:21<03:22,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.056:  28%|██▊       | 14/50 [01:26<03:22,  5.62s/it]\u001b[A\n",
      "Current training Loss 0.056:  30%|███       | 15/50 [01:26<03:14,  5.54s/it]\u001b[A\n",
      "Current training Loss 0.114:  30%|███       | 15/50 [01:31<03:14,  5.54s/it]\u001b[A\n",
      "Current training Loss 0.114:  32%|███▏      | 16/50 [01:31<03:03,  5.40s/it]\u001b[A\n",
      "Current training Loss 0.033:  32%|███▏      | 16/50 [01:36<03:03,  5.40s/it]\u001b[A\n",
      "Current training Loss 0.033:  34%|███▍      | 17/50 [01:36<02:55,  5.32s/it]\u001b[A\n",
      "Current training Loss 0.204:  34%|███▍      | 17/50 [01:42<02:55,  5.32s/it]\u001b[A\n",
      "Current training Loss 0.204:  36%|███▌      | 18/50 [01:42<02:48,  5.28s/it]\u001b[A\n",
      "Current training Loss 0.153:  36%|███▌      | 18/50 [01:47<02:48,  5.28s/it]\u001b[A\n",
      "Current training Loss 0.153:  38%|███▊      | 19/50 [01:47<02:42,  5.23s/it]\u001b[A\n",
      "Current training Loss 0.031:  38%|███▊      | 19/50 [01:52<02:42,  5.23s/it]\u001b[A\n",
      "Current training Loss 0.031:  40%|████      | 20/50 [01:52<02:35,  5.18s/it]\u001b[A\n",
      "Current training Loss 0.038:  40%|████      | 20/50 [01:57<02:35,  5.18s/it]\u001b[A\n",
      "Current training Loss 0.038:  42%|████▏     | 21/50 [01:57<02:29,  5.17s/it]\u001b[A\n",
      "Current training Loss 0.062:  42%|████▏     | 21/50 [02:02<02:29,  5.17s/it]\u001b[A\n",
      "Current training Loss 0.062:  44%|████▍     | 22/50 [02:02<02:23,  5.14s/it]\u001b[A\n",
      "Current training Loss 0.09:  44%|████▍     | 22/50 [02:07<02:23,  5.14s/it] \u001b[A\n",
      "Current training Loss 0.09:  46%|████▌     | 23/50 [02:07<02:19,  5.15s/it]\u001b[A\n",
      "Current training Loss 0.027:  46%|████▌     | 23/50 [02:12<02:19,  5.15s/it]\u001b[A\n",
      "Current training Loss 0.027:  48%|████▊     | 24/50 [02:12<02:13,  5.14s/it]\u001b[A\n",
      "Current training Loss 0.02:  48%|████▊     | 24/50 [02:17<02:13,  5.14s/it] \u001b[A\n",
      "Current training Loss 0.02:  50%|█████     | 25/50 [02:17<02:07,  5.11s/it]\u001b[A\n",
      "Current training Loss 0.027:  50%|█████     | 25/50 [02:22<02:07,  5.11s/it]\u001b[A\n",
      "Current training Loss 0.027:  52%|█████▏    | 26/50 [02:22<02:02,  5.09s/it]\u001b[A\n",
      "Current training Loss 0.024:  52%|█████▏    | 26/50 [02:27<02:02,  5.09s/it]\u001b[A\n",
      "Current training Loss 0.024:  54%|█████▍    | 27/50 [02:27<01:56,  5.06s/it]\u001b[A\n",
      "Current training Loss 0.029:  54%|█████▍    | 27/50 [02:32<01:56,  5.06s/it]\u001b[A\n",
      "Current training Loss 0.029:  56%|█████▌    | 28/50 [02:32<01:51,  5.05s/it]\u001b[A\n",
      "Current training Loss 0.053:  56%|█████▌    | 28/50 [02:37<01:51,  5.05s/it]\u001b[A\n",
      "Current training Loss 0.053:  58%|█████▊    | 29/50 [02:37<01:45,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.033:  58%|█████▊    | 29/50 [02:42<01:45,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.033:  60%|██████    | 30/50 [02:42<01:40,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.032:  60%|██████    | 30/50 [02:47<01:40,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.032:  62%|██████▏   | 31/50 [02:47<01:35,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.032:  62%|██████▏   | 31/50 [02:53<01:35,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.032:  64%|██████▍   | 32/50 [02:53<01:30,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.035:  64%|██████▍   | 32/50 [02:58<01:30,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.035:  66%|██████▌   | 33/50 [02:58<01:25,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.097:  66%|██████▌   | 33/50 [03:03<01:25,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.097:  68%|██████▊   | 34/50 [03:03<01:20,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.068:  68%|██████▊   | 34/50 [03:08<01:20,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.068:  70%|███████   | 35/50 [03:08<01:15,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.033:  70%|███████   | 35/50 [03:13<01:15,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.033:  72%|███████▏  | 36/50 [03:13<01:10,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.457:  72%|███████▏  | 36/50 [03:18<01:10,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.457:  74%|███████▍  | 37/50 [03:18<01:05,  5.02s/it]\u001b[A\n",
      "Current training Loss 0.068:  74%|███████▍  | 37/50 [03:23<01:05,  5.02s/it]\u001b[A\n",
      "Current training Loss 0.068:  76%|███████▌  | 38/50 [03:23<01:00,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.051:  76%|███████▌  | 38/50 [03:28<01:00,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.051:  78%|███████▊  | 39/50 [03:28<00:55,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.038:  78%|███████▊  | 39/50 [03:33<00:55,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.038:  80%|████████  | 40/50 [03:33<00:50,  5.05s/it]\u001b[A\n",
      "Current training Loss 0.014:  80%|████████  | 40/50 [03:38<00:50,  5.05s/it]\u001b[A\n",
      "Current training Loss 0.014:  82%|████████▏ | 41/50 [03:38<00:45,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.136:  82%|████████▏ | 41/50 [03:43<00:45,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.136:  84%|████████▍ | 42/50 [03:43<00:40,  5.06s/it]\u001b[A\n",
      "Current training Loss 0.024:  84%|████████▍ | 42/50 [03:48<00:40,  5.06s/it]\u001b[A\n",
      "Current training Loss 0.024:  86%|████████▌ | 43/50 [03:48<00:35,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.013:  86%|████████▌ | 43/50 [03:53<00:35,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.013:  88%|████████▊ | 44/50 [03:53<00:30,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.042:  88%|████████▊ | 44/50 [03:58<00:30,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.042:  90%|█████████ | 45/50 [03:58<00:25,  5.05s/it]\u001b[A\n",
      "Current training Loss 0.048:  90%|█████████ | 45/50 [04:03<00:25,  5.05s/it]\u001b[A\n",
      "Current training Loss 0.048:  92%|█████████▏| 46/50 [04:03<00:20,  5.05s/it]\u001b[A\n",
      "Current training Loss 0.054:  92%|█████████▏| 46/50 [04:08<00:20,  5.05s/it]\u001b[A\n",
      "Current training Loss 0.054:  94%|█████████▍| 47/50 [04:08<00:15,  5.04s/it]\u001b[A\n",
      "Current training Loss 0.02:  94%|█████████▍| 47/50 [04:13<00:15,  5.04s/it] \u001b[A\n",
      "Current training Loss 0.02:  96%|█████████▌| 48/50 [04:13<00:10,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.017:  96%|█████████▌| 48/50 [04:18<00:10,  5.03s/it]\u001b[A\n",
      "Current training Loss 0.017:  98%|█████████▊| 49/50 [04:18<00:05,  5.06s/it]\u001b[A\n",
      "Current training Loss 0.023:  98%|█████████▊| 49/50 [04:23<00:05,  5.06s/it]\u001b[A\n",
      "Current training Loss 0.023: 100%|██████████| 50/50 [04:23<00:00,  5.28s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on whole training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.256:   0%|          | 0/50 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.256:   2%|▏         | 1/50 [00:01<01:02,  1.27s/it]\u001b[A\n",
      "Current eval Loss 0.042:   2%|▏         | 1/50 [00:02<01:02,  1.27s/it]\u001b[A\n",
      "Current eval Loss 0.042:   4%|▍         | 2/50 [00:02<01:01,  1.28s/it]\u001b[A\n",
      "Current eval Loss 0.014:   4%|▍         | 2/50 [00:03<01:01,  1.28s/it]\u001b[A\n",
      "Current eval Loss 0.014:   6%|▌         | 3/50 [00:03<01:00,  1.28s/it]\u001b[A\n",
      "Current eval Loss 0.034:   6%|▌         | 3/50 [00:05<01:00,  1.28s/it]\u001b[A\n",
      "Current eval Loss 0.034:   8%|▊         | 4/50 [00:05<00:59,  1.29s/it]\u001b[A\n",
      "Current eval Loss 0.057:   8%|▊         | 4/50 [00:06<00:59,  1.29s/it]\u001b[A\n",
      "Current eval Loss 0.057:  10%|█         | 5/50 [00:06<00:58,  1.29s/it]\u001b[A\n",
      "Current eval Loss 0.032:  10%|█         | 5/50 [00:07<00:58,  1.29s/it]\u001b[A\n",
      "Current eval Loss 0.032:  12%|█▏        | 6/50 [00:07<00:56,  1.29s/it]\u001b[A\n",
      "Current eval Loss 0.047:  12%|█▏        | 6/50 [00:09<00:56,  1.29s/it]\u001b[A\n",
      "Current eval Loss 0.047:  14%|█▍        | 7/50 [00:09<00:55,  1.29s/it]\u001b[A\n",
      "Current eval Loss 0.022:  14%|█▍        | 7/50 [00:10<00:55,  1.29s/it]\u001b[A\n",
      "Current eval Loss 0.022:  16%|█▌        | 8/50 [00:10<00:55,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.038:  16%|█▌        | 8/50 [00:11<00:55,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.038:  18%|█▊        | 9/50 [00:11<00:55,  1.34s/it]\u001b[A\n",
      "Current eval Loss 0.06:  18%|█▊        | 9/50 [00:13<00:55,  1.34s/it] \u001b[A\n",
      "Current eval Loss 0.06:  20%|██        | 10/50 [00:13<00:53,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.058:  20%|██        | 10/50 [00:14<00:53,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.058:  22%|██▏       | 11/50 [00:14<00:51,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.023:  22%|██▏       | 11/50 [00:15<00:51,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.023:  24%|██▍       | 12/50 [00:15<00:50,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.038:  24%|██▍       | 12/50 [00:17<00:50,  1.32s/it]\u001b[A\n",
      "Current eval Loss 0.038:  26%|██▌       | 13/50 [00:17<00:48,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.431:  26%|██▌       | 13/50 [00:18<00:48,  1.31s/it]\u001b[A\n",
      "Current eval Loss 0.431:  28%|██▊       | 14/50 [00:18<00:47,  1.33s/it]\u001b[A\n",
      "Current eval Loss 0.02:  28%|██▊       | 14/50 [00:20<00:47,  1.33s/it] \u001b[A\n",
      "Current eval Loss 0.02:  30%|███       | 15/50 [00:20<00:52,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.012:  30%|███       | 15/50 [00:21<00:52,  1.49s/it]\u001b[A\n",
      "Current eval Loss 0.012:  32%|███▏      | 16/50 [00:22<00:52,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.023:  32%|███▏      | 16/50 [00:23<00:52,  1.56s/it]\u001b[A\n",
      "Current eval Loss 0.023:  34%|███▍      | 17/50 [00:23<00:52,  1.59s/it]\u001b[A\n",
      "Current eval Loss 0.024:  34%|███▍      | 17/50 [00:25<00:52,  1.59s/it]\u001b[A\n",
      "Current eval Loss 0.024:  36%|███▌      | 18/50 [00:25<00:50,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.023:  36%|███▌      | 18/50 [00:26<00:50,  1.57s/it]\u001b[A\n",
      "Current eval Loss 0.023:  38%|███▊      | 19/50 [00:26<00:47,  1.52s/it]\u001b[A\n",
      "Current eval Loss 0.03:  38%|███▊      | 19/50 [00:28<00:47,  1.52s/it] \u001b[A\n",
      "Current eval Loss 0.03:  40%|████      | 20/50 [00:28<00:44,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.029:  40%|████      | 20/50 [00:29<00:44,  1.50s/it]\u001b[A\n",
      "Current eval Loss 0.029:  42%|████▏     | 21/50 [00:29<00:42,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.026:  42%|████▏     | 21/50 [00:30<00:42,  1.48s/it]\u001b[A\n",
      "Current eval Loss 0.026:  44%|████▍     | 22/50 [00:30<00:40,  1.46s/it]\u001b[A\n",
      "Current eval Loss 0.04:  44%|████▍     | 22/50 [00:32<00:40,  1.46s/it] \u001b[A\n",
      "Current eval Loss 0.04:  46%|████▌     | 23/50 [00:32<00:39,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.017:  46%|████▌     | 23/50 [00:33<00:39,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.017:  48%|████▊     | 24/50 [00:33<00:37,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.018:  48%|████▊     | 24/50 [00:35<00:37,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.018:  50%|█████     | 25/50 [00:35<00:36,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.017:  50%|█████     | 25/50 [00:36<00:36,  1.47s/it]\u001b[A\n",
      "Current eval Loss 0.017:  52%|█████▏    | 26/50 [00:36<00:34,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.013:  52%|█████▏    | 26/50 [00:38<00:34,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.013:  54%|█████▍    | 27/50 [00:38<00:33,  1.44s/it]\u001b[A\n",
      "Current eval Loss 0.031:  54%|█████▍    | 27/50 [00:39<00:33,  1.44s/it]\u001b[A\n",
      "Current eval Loss 0.031:  56%|█████▌    | 28/50 [00:39<00:31,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.032:  56%|█████▌    | 28/50 [00:40<00:31,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.032:  58%|█████▊    | 29/50 [00:40<00:30,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.022:  58%|█████▊    | 29/50 [00:42<00:30,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.022:  60%|██████    | 30/50 [00:42<00:28,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.034:  60%|██████    | 30/50 [00:43<00:28,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.034:  62%|██████▏   | 31/50 [00:43<00:27,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.037:  62%|██████▏   | 31/50 [00:45<00:27,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.037:  64%|██████▍   | 32/50 [00:45<00:25,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.026:  64%|██████▍   | 32/50 [00:46<00:25,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.026:  66%|██████▌   | 33/50 [00:46<00:24,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.045:  66%|██████▌   | 33/50 [00:48<00:24,  1.45s/it]\u001b[A\n",
      "Current eval Loss 0.045:  68%|██████▊   | 34/50 [00:48<00:22,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.03:  68%|██████▊   | 34/50 [00:49<00:22,  1.43s/it] \u001b[A\n",
      "Current eval Loss 0.03:  70%|███████   | 35/50 [00:49<00:21,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.019:  70%|███████   | 35/50 [00:50<00:21,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.019:  72%|███████▏  | 36/50 [00:51<00:20,  1.44s/it]\u001b[A\n",
      "Current eval Loss 0.39:  72%|███████▏  | 36/50 [00:52<00:20,  1.44s/it] \u001b[A\n",
      "Current eval Loss 0.39:  74%|███████▍  | 37/50 [00:52<00:18,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.032:  74%|███████▍  | 37/50 [00:53<00:18,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.032:  76%|███████▌  | 38/50 [00:53<00:16,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.016:  76%|███████▌  | 38/50 [00:55<00:16,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.016:  78%|███████▊  | 39/50 [00:55<00:15,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.035:  78%|███████▊  | 39/50 [00:56<00:15,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.035:  80%|████████  | 40/50 [00:56<00:14,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.011:  80%|████████  | 40/50 [00:58<00:14,  1.43s/it]\u001b[A\n",
      "Current eval Loss 0.011:  82%|████████▏ | 41/50 [00:58<00:12,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.046:  82%|████████▏ | 41/50 [00:59<00:12,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.046:  84%|████████▍ | 42/50 [00:59<00:11,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.013:  84%|████████▍ | 42/50 [01:00<00:11,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.013:  86%|████████▌ | 43/50 [01:00<00:09,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.01:  86%|████████▌ | 43/50 [01:02<00:09,  1.42s/it] \u001b[A\n",
      "Current eval Loss 0.01:  88%|████████▊ | 44/50 [01:02<00:08,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.021:  88%|████████▊ | 44/50 [01:03<00:08,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.021:  90%|█████████ | 45/50 [01:03<00:07,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.017:  90%|█████████ | 45/50 [01:05<00:07,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.017:  92%|█████████▏| 46/50 [01:05<00:05,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.034:  92%|█████████▏| 46/50 [01:06<00:05,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.034:  94%|█████████▍| 47/50 [01:06<00:04,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.012:  94%|█████████▍| 47/50 [01:07<00:04,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.012:  96%|█████████▌| 48/50 [01:07<00:02,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.011:  96%|█████████▌| 48/50 [01:09<00:02,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.011:  98%|█████████▊| 49/50 [01:09<00:01,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.019:  98%|█████████▊| 49/50 [01:10<00:01,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.019: 100%|██████████| 50/50 [01:10<00:00,  1.42s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Current eval Loss 0.251:   0%|          | 0/13 [00:01<?, ?it/s]\u001b[A\n",
      "Current eval Loss 0.251:   8%|▊         | 1/13 [00:01<00:16,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.557:   8%|▊         | 1/13 [00:02<00:16,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.557:  15%|█▌        | 2/13 [00:02<00:15,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.088:  15%|█▌        | 2/13 [00:04<00:15,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.088:  23%|██▎       | 3/13 [00:04<00:14,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.307:  23%|██▎       | 3/13 [00:05<00:14,  1.40s/it]\u001b[A\n",
      "Current eval Loss 0.307:  31%|███       | 4/13 [00:05<00:12,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.588:  31%|███       | 4/13 [00:07<00:12,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.588:  38%|███▊      | 5/13 [00:07<00:11,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.604:  38%|███▊      | 5/13 [00:08<00:11,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.604:  46%|████▌     | 6/13 [00:08<00:09,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.44:  46%|████▌     | 6/13 [00:09<00:09,  1.42s/it] \u001b[A\n",
      "Current eval Loss 0.44:  54%|█████▍    | 7/13 [00:09<00:08,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.649:  54%|█████▍    | 7/13 [00:11<00:08,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.649:  62%|██████▏   | 8/13 [00:11<00:07,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.718:  62%|██████▏   | 8/13 [00:12<00:07,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.718:  69%|██████▉   | 9/13 [00:12<00:05,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.426:  69%|██████▉   | 9/13 [00:14<00:05,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.426:  77%|███████▋  | 10/13 [00:14<00:04,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.573:  77%|███████▋  | 10/13 [00:15<00:04,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.573:  85%|████████▍ | 11/13 [00:15<00:02,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.525:  85%|████████▍ | 11/13 [00:16<00:02,  1.41s/it]\u001b[A\n",
      "Current eval Loss 0.525:  92%|█████████▏| 12/13 [00:16<00:01,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.166:  92%|█████████▏| 12/13 [00:17<00:01,  1.42s/it]\u001b[A\n",
      "Current eval Loss 0.166: 100%|██████████| 13/13 [00:17<00:00,  1.36s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.048 Train metric = 0.992 Val loss = 0.453 Val metric = 0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 1/13 [00:01<00:16,  1.38s/it]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:02<00:15,  1.40s/it]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:04<00:14,  1.41s/it]\u001b[A\n",
      " 31%|███       | 4/13 [00:05<00:12,  1.43s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:07<00:11,  1.43s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:08<00:09,  1.42s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:09<00:08,  1.42s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:11<00:07,  1.41s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:12<00:05,  1.41s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:14<00:04,  1.41s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:15<00:02,  1.41s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:17<00:01,  1.42s/it]\u001b[A\n",
      "100%|██████████| 13/13 [00:17<00:00,  1.36s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "if args.use_torch_trainer:\n",
    "    device = torch.device(\"cuda\" if _torch_gpu_available and args.use_gpu else \"cpu\")\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        device=xm.xla_device()\n",
    "\n",
    "    print (\"Device: {}\".format(device))\n",
    "    \n",
    "    if args.use_TPU and _torch_tpu_available and args.num_tpus > 1:\n",
    "        train_data_loader = torch_xla.distributed.parallel_loader.ParallelLoader(train_data_loader, [device])\n",
    "        train_data_loader = train_data_loader.per_device_loader(device)\n",
    "\n",
    "\n",
    "    trainer = BasicTrainer(model, train_data_loader, val_data_loader, device, args.transformer_model_pretrained_path, \\\n",
    "                               final_activation=final_activation, \\\n",
    "                               test_data_loader=val_data_loader)\n",
    "\n",
    "    param_optimizer = list(trainer.model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr*xm.xrt_world_size())\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr)\n",
    "\n",
    "    if args.use_apex and _has_apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape) \n",
    "    \n",
    "    def _mp_fn(rank, flags, trainer, epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        a = trainer.train(epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed)\n",
    "\n",
    "    FLAGS = {}\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS, trainer, args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus, \\\n",
    "                 1, 3, False, args.use_apex, False, args.seed), nprocs=8, start_method='fork')\n",
    "    else:\n",
    "        use_wandb = _has_wandb and args.wandb_logging\n",
    "        trainer.train(args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus,  \\\n",
    "                max_grad_norm=1, early_stopping_rounds=3, snapshot_ensemble=False, is_amp=args.use_apex, use_wandb=use_wandb, seed=args.seed)\n",
    "\n",
    "elif args.use_lightning_trainer and _torch_lightning_available:\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.PLMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "    \n",
    "    log_args = {'description': args.transformer_model_pretrained_path, 'loss': loss.__class__.__name__, 'epochs': args.epochs, 'learning_rate': args.lr}\n",
    "\n",
    "    if _has_wandb and not _torch_tpu_available and args.wandb_logging:\n",
    "        wandb.init(project=\"Project\",config=log_args)\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filepath=args.model_save_path,\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "               verbose=False,\n",
    "               mode='min'\n",
    "               )\n",
    "\n",
    "    if args.use_gpu and _torch_gpu_available:\n",
    "        print (\"using GPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    elif args.use_TPU and _torch_tpu_available:\n",
    "        print (\"using TPU\")\n",
    "        if _has_apex:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    else:\n",
    "        print (\"using CPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    pltrainer = PLTrainer(num_train_steps, model, scorer, loss, args.lr, \\\n",
    "                          final_activation=final_activation, seed=42)\n",
    "\n",
    "    #try:\n",
    "    #    print (\"Loaded model from previous checkpoint\")\n",
    "    #    pltrainer = PLTrainer.load_from_checkpoint(args.model_save_path)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    trainer.fit(pltrainer, train_data_loader, val_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output1 = trainer.test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Pytorch Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Logging: False, GPU: False, Pytorch Lightning: True, TPU: False, Apex: False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog='Torch trainer function',conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/IMDB Dataset.csv', required=False,\n",
    "                    help='train data')\n",
    "parser.add_argument('--val_data', type=str, default='', required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "parser.add_argument('--bpe_vocab_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding vocab file path')\n",
    "parser.add_argument('--bpe_merges_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding merges file path')\n",
    "parser.add_argument('--berttweettokenizer_path', type=str, default='', required=False,\n",
    "                    help='BERTweet tokenizer path')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=100, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--epochs', type=int, default=5, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.00003, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--loss_function', type=str, default='bcelogit', required=False,\n",
    "                    help='loss function')\n",
    "parser.add_argument('--metric', type=str, default='f1', required=False,\n",
    "                    help='scorer metric')\n",
    "\n",
    "parser.add_argument('--use_lightning_trainer', type=bool, default=True, required=False,\n",
    "                    help='if lightning trainer needs to be used')\n",
    "parser.add_argument('--use_torch_trainer', type=bool, default=False, required=False,\n",
    "                    help='if custom torch trainer needs to be used')\n",
    "parser.add_argument('--use_apex', type=bool, default=False, required=False,\n",
    "                    help='if apex needs to be used')\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, required=False,\n",
    "                    help='GPU mode')\n",
    "parser.add_argument('--use_TPU', type=bool, default=False, required=False,\n",
    "                    help='TPU mode')\n",
    "parser.add_argument('--num_gpus', type=int, default=0, required=False,\n",
    "                    help='Number of GPUs')\n",
    "parser.add_argument('--num_tpus', type=int, default=0, required=False,\n",
    "                    help='Number of TPUs')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=16, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/sentiment_classification/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=False, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print (\"Wandb Logging: {}, GPU: {}, Pytorch Lightning: {}, TPU: {}, Apex: {}\".format(\\\n",
    "            _has_wandb and args.wandb_logging, _torch_gpu_available,\\\n",
    "            _torch_lightning_available and args.use_lightning_trainer, _torch_tpu_available, _has_apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "I0806 15:52:24.844974 4539198912 distributed.py:29] GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0806 15:52:24.846642 4539198912 distributed.py:29] TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU\n",
      "[LOG] Total number of parameters to learn 124646401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type               | Params\n",
      "----------------------------------------------\n",
      "0 | model  | TransformerWithCLS | 124 M \n",
      "1 | metric | PLMetric           | 0     \n",
      "I0806 15:52:25.195801 4539198912 lightning.py:1495] \n",
      "  | Name   | Type               | Params\n",
      "----------------------------------------------\n",
      "0 | model  | TransformerWithCLS | 124 M \n",
      "1 | metric | PLMetric           | 0     \n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.404 val metric = 0.856 \n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7a817d732b48c3b625c3dd90a24246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: val_loss reached 0.46321 (best 0.46321), saving model to ../models/sentiment_classification/epoch=0.ckpt as top 1\n",
      "I0806 15:57:33.435837 4539198912 model_checkpoint.py:346] \n",
      "Epoch 00000: val_loss reached 0.46321 (best 0.46321), saving model to ../models/sentiment_classification/epoch=0.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.463 val metric = 0.859 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.145 Train metric = 0.948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss reached 0.45796 (best 0.45796), saving model to ../models/sentiment_classification/epoch=1.ckpt as top 1\n",
      "I0806 16:02:44.166603 4539198912 model_checkpoint.py:346] \n",
      "Epoch 00001: val_loss reached 0.45796 (best 0.45796), saving model to ../models/sentiment_classification/epoch=1.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.458 val metric = 0.837 \n",
      "Train loss = 0.072 Train metric = 0.975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss  was not in top 1\n",
      "I0806 16:07:46.498960 4539198912 model_checkpoint.py:314] \n",
      "Epoch 00002: val_loss  was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.539 val metric = 0.778 \n",
      "Train loss = 0.108 Train metric = 0.961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss  was not in top 1\n",
      "I0806 16:12:26.587539 4539198912 model_checkpoint.py:314] \n",
      "Epoch 00003: val_loss  was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.675 val metric = 0.836 \n",
      "Train loss = 0.036 Train metric = 0.987\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss  was not in top 1\n",
      "I0806 16:17:00.354878 4539198912 model_checkpoint.py:314] \n",
      "Epoch 00004: val_loss  was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.551 val metric = 0.847 \n",
      "Train loss = 0.054 Train metric = 0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.use_torch_trainer:\n",
    "    device = torch.device(\"cuda\" if _torch_gpu_available and args.use_gpu else \"cpu\")\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        device=xm.xla_device()\n",
    "\n",
    "    print (\"Device: {}\".format(device))\n",
    "    \n",
    "    if args.use_TPU and _torch_tpu_available and args.num_tpus > 1:\n",
    "        train_data_loader = torch_xla.distributed.parallel_loader.ParallelLoader(train_data_loader, [device])\n",
    "        train_data_loader = train_data_loader.per_device_loader(device)\n",
    "\n",
    "\n",
    "    trainer = BasicTrainer(model, train_data_loader, val_data_loader, device, args.transformer_model_pretrained_path, \\\n",
    "                               final_activation=final_activation, \\\n",
    "                               test_data_loader=val_data_loader)\n",
    "\n",
    "    param_optimizer = list(trainer.model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr*xm.xrt_world_size())\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr)\n",
    "\n",
    "    if args.use_apex and _has_apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape) \n",
    "    \n",
    "    def _mp_fn(rank, flags, trainer, epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        a = trainer.train(epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed)\n",
    "\n",
    "    FLAGS = {}\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS, trainer, args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus, \\\n",
    "                 1, 3, False, args.use_apex, False, args.seed), nprocs=8, start_method='fork')\n",
    "    else:\n",
    "        use_wandb = _has_wandb and args.wandb_logging\n",
    "        trainer.train(args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus,  \\\n",
    "                max_grad_norm=1, early_stopping_rounds=3, snapshot_ensemble=False, is_amp=args.use_apex, use_wandb=use_wandb, seed=args.seed)\n",
    "\n",
    "elif args.use_lightning_trainer and _torch_lightning_available:\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.PLMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "    \n",
    "    log_args = {'description': args.transformer_model_pretrained_path, 'loss': loss.__class__.__name__, 'epochs': args.epochs, 'learning_rate': args.lr}\n",
    "\n",
    "    if _has_wandb and not _torch_tpu_available and args.wandb_logging:\n",
    "        wandb.init(project=\"Project\",config=log_args)\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filepath=args.model_save_path,\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min'\n",
    "                )\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "               verbose=False,\n",
    "               mode='min'\n",
    "               )\n",
    "\n",
    "    if args.use_gpu and _torch_gpu_available:\n",
    "        print (\"using GPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    elif args.use_TPU and _torch_tpu_available:\n",
    "        print (\"using TPU\")\n",
    "        if _has_apex:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    else:\n",
    "        print (\"using CPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    pltrainer = PLTrainer(num_train_steps, model, scorer, loss, args.lr, \\\n",
    "                          final_activation=final_activation, seed=42)\n",
    "\n",
    "    #try:\n",
    "    #    print (\"Loaded model from previous checkpoint\")\n",
    "    #    pltrainer = PLTrainer.load_from_checkpoint(args.model_save_path)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    trainer.fit(pltrainer, train_data_loader, val_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/13 [00:02<00:27,  2.33s/it]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:04<00:23,  2.17s/it]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:05<00:20,  2.08s/it]\u001b[A\n",
      " 31%|███       | 4/13 [00:08<00:18,  2.07s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:10<00:16,  2.06s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:12<00:14,  2.03s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:13<00:11,  1.97s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:15<00:09,  1.92s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:17<00:07,  1.91s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:19<00:05,  1.88s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:21<00:03,  1.86s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:23<00:01,  1.85s/it]\u001b[A\n",
      "100%|██████████| 13/13 [00:23<00:00,  1.84s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_output2 = []\n",
    "\n",
    "for val_batch in tqdm(val_data_loader):\n",
    "    out = torch.sigmoid(pltrainer(val_batch)).detach().cpu().numpy()\n",
    "    test_output2.extend(out[:,0].tolist())\n",
    "    \n",
    "#test_output2 = np.concatenate(test_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.90560145],\n",
       "       [0.90560145, 1.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output1 = np.array(test_output1)[:,0]\n",
    "test_output2 = np.array(test_output2)\n",
    "np.corrcoef(test_output1,test_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
